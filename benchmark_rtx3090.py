#!/usr/bin/env python3
"""
RTX 3090 ML Benchmark - Nova Training System
Tests: GPU compute, memory, PyTorch performance, inference speed
"""

import time
import torch
import platform
import subprocess
import psutil
from datetime import datetime

print("=" * 70)
print("üöÄ RTX 3090 ML BENCHMARK - NOVA TRAINING SYSTEM")
print("=" * 70)
print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Platform: {platform.system()} {platform.release()}")
print(f"Python: {platform.python_version()}")
print(f"PyTorch: {torch.__version__}")
print()

# ========== SYSTEM INFO ==========
print("=" * 70)
print("üìä SYSTEM INFORMATION")
print("=" * 70)

# CPU Info
cpu_info = subprocess.run(['lscpu'], capture_output=True, text=True)
cpu_lines = [l for l in cpu_info.stdout.split('\n') if 'Model name' in l]
if cpu_lines:
    print(f"CPU: {cpu_lines[0].split(':')[1].strip()}")
print(f"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical")

# RAM
ram_gb = psutil.virtual_memory().total / (1024**3)
print(f"RAM: {ram_gb:.1f} GB")

# GPU Info
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    gpu_props = torch.cuda.get_device_properties(0)
    vram_gb = gpu_props.total_memory / (1024**3)
    print(f"VRAM: {vram_gb:.1f} GB")
    print(f"CUDA Cores: {gpu_props.multi_processor_count * 128}")  # RTX 3090 has 128 cores/SM
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"Compute Capability: {gpu_props.major}.{gpu_props.minor}")
else:
    print("‚ö†Ô∏è  GPU: CUDA NOT AVAILABLE!")
    exit(1)

print()

# ========== BENCHMARK 1: GPU COMPUTE (MATRIX MULTIPLICATION) ==========
print("=" * 70)
print("üî• BENCHMARK 1: GPU COMPUTE - Matrix Multiplication")
print("=" * 70)

sizes = [1024, 2048, 4096, 8192]
device = torch.device('cuda:0')

for size in sizes:
    # Allocate matrices
    a = torch.randn(size, size, device=device, dtype=torch.float32)
    b = torch.randn(size, size, device=device, dtype=torch.float32)
    
    # Warmup
    torch.cuda.synchronize()
    _ = torch.matmul(a, b)
    torch.cuda.synchronize()
    
    # Benchmark
    iterations = 10
    start = time.time()
    for _ in range(iterations):
        c = torch.matmul(a, b)
        torch.cuda.synchronize()
    elapsed = time.time() - start
    
    # Calculate TFLOPS
    flops = 2 * size**3 * iterations  # Matrix multiply is 2*N^3 operations
    tflops = (flops / elapsed) / 1e12
    
    print(f"Matrix {size}x{size}: {elapsed/iterations*1000:.2f} ms/iter | {tflops:.2f} TFLOPS")
    
    del a, b, c
    torch.cuda.empty_cache()

print()

# ========== BENCHMARK 2: MEMORY BANDWIDTH ==========
print("=" * 70)
print("üíæ BENCHMARK 2: GPU MEMORY BANDWIDTH")
print("=" * 70)

# Test different data sizes
mem_sizes_gb = [1, 2, 4, 8]

for size_gb in mem_sizes_gb:
    if size_gb > vram_gb * 0.8:  # Don't exceed 80% VRAM
        break
    
    elements = int((size_gb * 1024**3) / 4)  # 4 bytes per float32
    
    # Allocate tensor
    data = torch.randn(elements, device=device, dtype=torch.float32)
    
    # Copy benchmark
    torch.cuda.synchronize()
    start = time.time()
    data_copy = data.clone()
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    bandwidth_gb_s = (size_gb * 2) / elapsed  # Read + Write
    
    print(f"Copy {size_gb}GB: {elapsed*1000:.2f} ms | Bandwidth: {bandwidth_gb_s:.1f} GB/s")
    
    del data, data_copy
    torch.cuda.empty_cache()

print()

# ========== BENCHMARK 3: SIMULATED MODEL INFERENCE ==========
print("=" * 70)
print("üß† BENCHMARK 3: Simulated Model Inference (Llama-style)")
print("=" * 70)

# Simulate 7B parameter model structure
batch_sizes = [1, 4, 8]
seq_length = 512
hidden_size = 4096
num_layers = 32

for batch_size in batch_sizes:
    # Simulate forward pass through transformer layers
    x = torch.randn(batch_size, seq_length, hidden_size, device=device)
    
    torch.cuda.synchronize()
    start = time.time()
    
    # Simulate attention + FFN for each layer
    for _ in range(num_layers):
        # Self-attention (simplified)
        q = torch.matmul(x, torch.randn(hidden_size, hidden_size, device=device))
        k = torch.matmul(x, torch.randn(hidden_size, hidden_size, device=device))
        v = torch.matmul(x, torch.randn(hidden_size, hidden_size, device=device))
        attn = torch.matmul(q, k.transpose(-2, -1)) / (hidden_size ** 0.5)
        attn = torch.softmax(attn, dim=-1)
        out = torch.matmul(attn, v)
        
        # FFN
        x = torch.matmul(out, torch.randn(hidden_size, hidden_size * 4, device=device))
        x = torch.nn.functional.gelu(x)
        x = torch.matmul(x, torch.randn(hidden_size * 4, hidden_size, device=device))
    
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    tokens_per_sec = (batch_size * seq_length) / elapsed
    
    print(f"Batch {batch_size}, Seq {seq_length}: {elapsed:.3f}s | {tokens_per_sec:.1f} tokens/sec")
    
    del x
    torch.cuda.empty_cache()

print()

# ========== BENCHMARK 4: MEMORY CAPACITY ==========
print("=" * 70)
print("üì¶ BENCHMARK 4: Maximum Model Size (QLoRA simulation)")
print("=" * 70)

# Test how large a model we can load in 4-bit
from torch.cuda import max_memory_allocated, reset_peak_memory_stats

reset_peak_memory_stats()

# Simulate loading progressively larger models
model_sizes_b = [1, 3, 7, 13, 20]

for size_b in model_sizes_b:
    # Approximate: 4-bit quantized model uses ~0.5 bytes/param
    # + ~10% overhead for QLoRA adapters
    params = size_b * 1e9
    memory_needed_gb = (params * 0.5 * 1.1) / 1e9
    
    if memory_needed_gb > vram_gb * 0.9:  # Leave 10% headroom
        print(f"{size_b}B params: {memory_needed_gb:.1f}GB VRAM needed ‚ùå (exceeds capacity)")
        break
    else:
        # Allocate equivalent tensor
        elements = int((memory_needed_gb * 1e9) / 4)
        try:
            test_model = torch.randn(elements, device=device, dtype=torch.float32)
            print(f"{size_b}B params: {memory_needed_gb:.1f}GB VRAM ‚úÖ (fits comfortably)")
            del test_model
            torch.cuda.empty_cache()
        except RuntimeError:
            print(f"{size_b}B params: {memory_needed_gb:.1f}GB VRAM ‚ùå (OOM)")
            break

print()

# ========== SUMMARY ==========
print("=" * 70)
print("üìä BENCHMARK SUMMARY")
print("=" * 70)
print(f"‚úÖ System: {platform.node()}")
print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)} ({vram_gb:.0f}GB)")
print(f"‚úÖ Peak compute: ~15-20 TFLOPS (matrix ops)")
print(f"‚úÖ Memory bandwidth: ~900 GB/s (theoretical)")
print(f"‚úÖ Inference: ~50-100 tokens/sec (7B model estimate)")
print(f"‚úÖ QLoRA capacity: 13B params comfortable, 20B possible")
print()
print("üöÄ READY FOR NOVA TRAINING!")
print("=" * 70)
