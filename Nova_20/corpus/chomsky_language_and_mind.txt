
--- Page 2 ---

Language and Mind
This is the long-awaited third edition of Chomsky’s outstanding collection
of essays on language and mind. The ﬁrst six chapters, originally publishedin the 1960s, made a groundbreaking contribution to linguistic theory. Thisnew edition complements them with an additional chapter and a new pref-ace, bringing Chomsky’s inﬂuential approach into the twenty-ﬁrst century.Chapters 1–6 present Chomsky’s early work on the nature and acquisition oflanguage as a genetically endowed, biological system (Universal Grammar),through the rules and principles of which we acquire an internalized knowl-edge (I-language). Over the past ﬁfty years, this framework has sparked anexplosion of inquiry into a wide range of languages, and has yielded some
major theoretical questions. The ﬁnal chapter revisits the key issues, reviewingthe “biolinguistic” approach that has guided Chomsky’s work from its originsto the present day, and raising some novel and exciting challenges for the studyof language and mind.
noam chomsky is Professor of Linguistics at Massachusetts Institute of
Technology. His many books include New Horizons in the Study of Language
and Mind (Cambridge University Press, 2000) and On Nature and Language
(Cambridge University Press, 2002).

--- Page 4 ---

Language and Mind
Third Edition
Noam Chomsky
Massachusetts Institute of Technology


--- Page 5 ---

  
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge  ,U K
First published in print format
- ----
- ----- ----© Noam Chomsky 2006
2005Information on this title: www.cambri dg e.org /9780521858199
This publication is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take placewithout the written permission of Cambridge University Press.
- ---
- ---
- ---
Cambridge University Press has no responsibility for the persistence or accuracy of s
for external or third-party internet websites referred to in this publication, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
hardback
paperback
paperbackeBook (NetLibrary)
eBook (NetLibrary)
hardback

--- Page 6 ---

Contents
Preface to the third edition pagevii
Preface to the second edition xiii
Preface to the ﬁrst edition xvii
1Linguistic contributions to the study of mind: past 1
2Linguistic contributions to the study of mind: present 21
3Linguistic contributions to the study of mind: future 57
4Form and meaning in natural languages 88
5The formal nature of language 102
6Linguistics and philosophy 143
7Biolinguistics and the human capacity 173
Index 186
v

--- Page 8 ---

Preface to the third edition
The ﬁrst six chapters that follow are from the late 1960s, mostly based on talks
for general university audiences, hence relatively informal. The ﬁnal chapter is
from 2004, based on a talk for a general audience. This recent essay reviewsthe “biolinguistic approach” that has guided this work from its origins half acentury ago, some of the important developments of recent decades, and howthe general approach looks today – to me at least.
The dominant approach to questions of language and mind in the 1950s was
that of the behavioral sciences. As the term indicates, the object of inquiry wastaken to be behavior, or, for linguistics, the products of behavior: perhaps acorpus obtained from informants by the elicitation techniques taught in ﬁeldmethods courses. Linguistic theory consisted of procedures of analysis, primar-ily segmentation and classiﬁcation, designed to organize a body of linguisticmaterial, guided by limited assumptions about structural properties and theirarrangement. The prominent linguist Martin Joos hardly exaggerated in a 1955exposition when he identiﬁed the “decisive direction” of contemporary struc-
tural linguistics as the decision that language can be “described without anypreexistent scheme of what a language must be.”
1Prevailing approaches in
the behavioral sciences generally were not very different. Of course, no oneaccepted the incoherent notion of a “blank slate.” But it was common to sup-pose that beyond some initial delimitation of properties detected in the environ-ment (a “quality space,” in the framework of the highly inﬂuential philosopherW.V.O. Quine), general learning mechanisms of some kind should sufﬁce to
account for what organisms, including humans, know and do. Genetic endow-ment in these domains would not be expected to reach much beyond somethinglike that.
The emerging biolinguistic approach adopted a different stance. It took the
object of inquiry to be, not behavior and its products, but the internal cognitive
1Chapter 3, note 12. Joos was referring explicitly to the “Boasian tradition” of American struc-
turalism, and had only a few – rather disparaging – remarks about European structuralism. Butthe observations carry over without too much change.
vii

--- Page 9 ---

viii Preface to the third edition
systems that enter into action and interpretation, and, beyond that, the basis in
our ﬁxed biological nature for the growth and development of these internal sys-tems. From this point of view, the central topic of concern is what Juan Huarte, inthe sixteenth century, regarded as the essential property of human intelligence:the capacity of the human mind to “engender within itself, by its own power, theprinciples on which knowledge rests,”
2ideas that were developed in important
ways in the philosophical–scientiﬁc traditions of later years. For language, “the
principles on which knowledge rests” are those of the internalized language ( I-
language )that the person has acquired. Having acquired these principles, Jones
has a wide range of knowledge, for example that glink butnotglnik is a possible
lexical item of English; that John is too angry to talk to (Mary) means that John
is to be talked to (if Mary is missing) but John is to do the talking (if Mary is
present); that himcan be used to refer to John in the sentence Iwonder who John
expects to see him ,b u t not if Iwonder who is omitted; that if John painted the
house brown then he put the paint on the exterior surface though he could paintthe house brown on the inside; that when John climbed the mountain he went upalthough he can climb down the mountain; that books are in some sense simul-taneously abstract and concrete as in John memorized and then burned the book ;
and so on over an unbounded range. “The power to engender” the I-languageprinciples on which such particular cases of knowledge rest is understood tobe the component of the genetic endowment that accounts for their growth anddevelopment.
Linguistics, so conceived, seeks to discover true theories of particular I-
languages ( grammars ), and, at a deeper level, the theory of the genetic basis
for language acquisition ( universal grammar ,UG,adapting a traditional term
to a new usage). Other cognitive systems, it was assumed, should be conceivedalong similar lines, each with its own principles, and powers of engenderingthem.
Within this framework, cognitive systems are understood to be, in effect,
organs of the body, primarily the brain, to be investigated in much the mannerof other subcomponents with distinctive properties that interact in the life of theorganism: the systems of vision, motor planning, circulation of the blood, etc.Along with their role in behavior, the “cognitive organs” enter into activitiestraditionally regarded as mental: thought, planning, interpretation, evaluation,and so on. The term “mental” here is informal and descriptive, pretty much onapar with such loose descriptive terms as “chemical,” “electrical,” “optical,”
and others that are used to focus attention on particular aspects of the worldthat seem to have an integrated character and to be worth abstracting for spe-cial investigation, but without any illusion that they carve nature at the joints.Behavior and its products – such as texts – provide data that may be useful as
2Chapter 1, pp. 8–9.

--- Page 10 ---

Preface to the third edition ix
evidence to determine the nature and origins of cognitive systems, but have no
privileged status for such inquiries, just as in the case of other organs of the
body.
The general shift of perspective is sometimes called the “cognitive revolu-
tion” of the 1950s. However, for reasons discussed in the early essays that follow,Ithink it might more properly be considered a renewal and further development
of the cognitive revolution of the seventeenth century. From the 1950s, manytraditional questions were revived – regrettably, without acquaintance with thetradition, which had been largely forgotten or misrepresented. Also revived wasthe view that had been crystallizing through the eighteenth century that proper-ties “termed mental” are the result of “such an organical structure as that of thebrain” (chemist–philosopher Joseph Priestley). This development of “Locke’ssuggestion,” as it is called in the scholarly literature, was a natural, virtuallyinevitable, concomitant of the Newtonian revolution, which effectively disman-tled the only signiﬁcant notion of “body” or “physical.” The basic conclusionwaswell understood by the nineteenth-century. Darwin asked rhetorically why
“thought, being a secretion of the brain,” should be considered “more wonder-ful than gravity, a property of matter.” In his classic nineteenth-century historyof materialism, Friedrich Lange observes that scientists have “accustomed our-selves to the abstract notion of forces, or rather to a notion hovering in a mysticobscurity between abstraction and concrete comprehension,” a “turning-point”in the history of materialism that removes the surviving remnants of the doctrinefarfrom the ideas and concerns of the “genuine Materialists” of the seventeenth
century, and deprives them of signiﬁcance. They need be of no special concernin the study of aspects of the world “termed mental.”
It is perhaps worth noting that this traditional understanding is still regarded
as highly contentious, and repetition of it, almost in virtually the same words,is regularly proposed as a “bold hypothesis” or “radical new idea” in the studyof the domains “termed mental.”
3
Another signiﬁcant feature of the original cognitive revolution was the recog-
nition that properties of the world termed mental may involve unbounded capac-ities of a limited ﬁnite organ, the “inﬁnite use of ﬁnite means,” in Wilhelm vonHumboldt’s phrase. The doctrine was at the heart of the Cartesian conceptof mind. It provided the basic criterion to deal with the problem of “otherminds” – to determine whether some creature has a mind like ours. Descartesand his followers focused on use of language as the clearest illustration. Inarather similar vein, Hume later recognized that our moral judgments are
unbounded in scope, and must be founded on general principles that are part ofour nature – genetically determined, in modern terms. That observation poses
3Forexamples and discussion, see my New Horizons in the Study of Language and Mind
(Cambridge, 2000).

--- Page 11 ---

x Preface to the third edition
Huarte’s problem in a different domain, and is, by now, the topic of intriguing
empirical research and conceptual analysis.
By the mid twentieth century, it had become possible to face such problems
as these in a more substantive way than in earlier periods. There was, by then, aclear general understanding of ﬁnite generative systems with unbounded scope,which could be readily adapted to the reframing and investigation of traditionalquestions that had necessarily been left obscure. Another inﬂuential factor in therenewal of the cognitive revolution was the work of ethologists and comparativepsychologists, then just coming to be more readily accessible, with its concernfor “the innate working hypotheses present in subhuman organisms,” and the“human a priori,” which should have much the same character.
4That framework
too could be adapted to the study of human cognitive organs and their geneticallydetermined nature, which constructs experience – the organism’s Umwelt ,i n
ethological terminology – and guides the general path of development, just asin all other aspects of growth of organisms.
Meanwhile, efforts to sharpen and reﬁne procedural approaches ran into
serious difﬁculties, revealing what appear to be intrinsic inadequacies. A basicproblem is that even the most simple elements of discourse are not detectableby procedures of segmentation and classiﬁcation. They do not have the required“beads on a string” property for such procedures to operate, and often cannotbe located in some identiﬁable part of the physical event that corresponds to themind-internal expression in which these elements function. It became increas-ingly clear that even the simplest units – morphemes, elementary lexical items,for that matter even phonological segments – can be identiﬁed only by their rolein generative procedures that form linguistic expressions. These expressions,in turn, can be regarded as “instructions” to other systems of the mind/bodythat are used for mental operations, as well as for production of utterances andinterpretation of external signals. More generally, study of the postulated mech-anisms of learning and control of behavior in the behavioral sciences revealedfundamental inadequacies, and even at the core of the disciplines serious doubtswere arising as to whether the entire enterprise was viable, apart from its utilityfor design of experiments that might be useful for some other purpose.
Forthe study of language, a natural conclusion seemed to be that the I-
language attained has roughly the character of a scientiﬁc theory: an integratedsystem of rules and principles from which the expressions of the language canbe derived, each of them a collection of instructions for thought and action.The child must somehow select the I-language from the ﬂux of experience.The problem appeared to be similar to what Charles Sanders Peirce had calledabduction ,inconsidering the problem of scientiﬁc discovery.
5And as in the
4Konrad Lorenz; chapter 3, pp. 83–84, below.
5See chapter 3, pp. 79–81,below.

--- Page 12 ---

Preface to the third edition xi
case of the sciences, the task is impossible without what Peirce called a “limit
on admissible hypotheses” that permits only certain theories to be entertained,butnot inﬁnitely many others compatible with relevant data. In the language
case, it appeared that the genetic endowment of the language faculty mustimpose a format for rule systems that is sufﬁciently restrictive so that candidateI-languages are “scattered,” and only a small number can even be considered inthe course of language acquisition. In later work in the cognitive sciences, suchapproaches are often called “theory theory” conceptions.
6Like abduction, and
for that matter every aspect of growth and development, language acquisitionfaces a problem of poverty of stimulus .The general observation is transparent,
so much so that outside of the cognitive sciences the ubiquitous phenomenonis not even digniﬁed with a name: no one speaks of the problem of poverty ofstimulus for an embryo that has somehow to become a worm or a cat, giventhe nutritional environment, or in any aspect of post-natal development, sayundergoing puberty.
In the essays reprinted below from the 1960s, the nature and acquisition of
language presented and discussed adopts the general framework just outlined.“The most challenging theoretical problem in linguistics” was therefore taken tobe “that of discovering the principles of universal grammar,” which “determinethe choice of hypotheses” – that is, restrict the accessible I-languages. It wasalso recognized, however, that for language, as for other biological organisms,astill more challenging problem lies on the horizon: to discover “the laws that
determine possible successful mutation and the nature of complex organisms,”quite apart from the particular cognitive organs or other organic systems underinvestigation.
7As the same point was made a few years earlier: “there is surely
no reason today for taking seriously a position that attributes a complex humanachievement entirely to months (or at most years) of experience [as in thebehavioral sciences], rather than to millions of years of evolution [as in the studyof the speciﬁc biological endowment, UG in the language case], or to principlesof neural organization that may be more deeply grounded in physical law”
8–
a“third factor” in growth and development, organ- and possibly organism-
independent. Investigation of the third factor seemed too remote from inquiryto merit much attention, and was therefore barely mentioned, though, in fact,even some of the earliest work – for example, on elimination of redundancy in
rule systems – was implicitly guided by such concerns.
In the years that followed, the topics under investigation were substantially
extended, not only in language-related areas but in the cognitive sciences gen-
erally. By the early 1980s, a substantial shift of perspective within linguistics
6Advocates of these approaches disagree, but mistakenly, I believe. See L. Antony and
N. Hornstein, Chomsky and his Critics (Blackwell, 2003), chapter 10, and reply.
7Pp. 47, 85f., below.
8Chomsky, Aspects of the Theory of Syntax (Cambridge, Mass: MIT Press, 1965), p. 59.

--- Page 13 ---

xii Preface to the third edition
reframed the basic questions considerably, abandoning entirely the format con-
ception of linguistic theory in favor of an approach that sought to limit attain-able I-languages to a ﬁnite set, aside from lexical choices (these too highlyrestricted). This Principles and Parameters approach may or may not turn out
to be justiﬁed; one can never know. But as a research program, it has beenhighly successful, yielding an explosion of empirical inquiry into a very widerange of typologically varied languages, posing new theoretical questions thatcould scarcely have been formulated before, often providing at least partialanswers as well, while also revitalizing related areas of language acquisitionand processing. Another consequence is that it removed some basic conceptualbarriers to the serious inquiry into the deeper “third factor” issues. These topicsare reviewed in the lecture that closes this collection. They raise possibilitiesthat, in my personal view at least, suggest novel and exciting challenges for thestudy of language in particular and problems of mind more generally.

--- Page 14 ---

Preface to the second edition
The six chapters that follow fall into two groups. The ﬁrst three constitute the
monograph Language and Mind ,published in 1968. As the preface to Lan-
guage and Mind ,reprinted below, explains, the three essays on linguistic con-
tributions to the study of mind (past, present, and future) are based on theBeckman lectures, delivered before a university-wide audience at the Univer-sity of California, Berkeley, in January 1967. These essays constitute a unitdistinct from the three chapters that follow them.
Chapter 4,“Form and meaning in natural languages,” is the approximate
text of a rather informal lecture given in January 1969 at Gustavus AdolphusCollege in Minnesota to an audience consisting largely of high school andcollege students and teachers. It reviews some of the basic notions presentedinLanguage and Mind and other works, and in addition presents some later
work on semantic interpretation of syntactic structures. This material, I believe,
reveals some of the limitations and inadequacies of earlier theory and suggests adirection in which this theory should be revised. More technical investigations ofthis and related matters appear in forthcoming monographs of mine, Semantics
in Generative Grammar andConditions on Rules ,tobepublished by Mouton
and Co., The Hague, in 1972.
Chapter 5is a considerably more technical study, exploring in some detail
material that is presupposed or only informally developed in Language and
Mind. The intended audience in this case consisted primarily of psychologists
and psycholinguists. This chapter, which originally appeared as an appendix toEric Lenneberg’s Biological Foundations of Language ,isanattempt to give a
concise and systematic presentation of the theory of transformational-generativegrammar and to explore its potential signiﬁcance for human psychology. Themonographs just cited carry the technical investigations further, in part, in direc-tions that are brieﬂy indicated in this chapter, which was actually written in 1965and is therefore the earliest of the essays collected here.
Chapter 6wasdirected to a rather different audience, namely, professional
philosophers. This was a contribution to a symposium on linguistics and phi-losophy held at New York University in April 1968. The purpose of this lecturewastoexplore the points of contact between contemporary linguistics and
xiii

--- Page 15 ---

xiv Preface to the second edition
philosophy – in particular, epistemology and philosophy of mind. The sugges-
tion has been made that current work in linguistics has interesting insights tooffer into the nature of human knowledge, the basis for its acquisition, and theways it characteristically is used. In part, this essay is concerned with the debate
that has arisen over these issues; in part, with the issues themselves.
There is a certain degree of redundancy in these essays. Chapters 4,5,and6
are each more or less self-contained. Each presupposes very little, and thereforesome of the expository sections overlap, and overlap further with the chaptersthat constitute Language and Mind. Ihope that the somewhat varying formula-
tions of basic points may prove helpful. In fact, even the simplest and most basicpoints discussed in these essays have been widely misconstrued. For example,there has been a tendency in popular discussion to confuse “deep structure”with “generative grammar” or with “universal grammar.” And a number of pro-fessional linguists have repeatedly confused what I refer to here as “the creativeaspect of language use” with the recursive property of generative grammars, avery different matter. In the hope that such questions as these will be clariﬁed,
Ihavenot eliminated redundancies in collecting these essays.
Chapters 4–6extend and enlarge upon the ideas and material discussed in
the Beckman lectures. All of these essays are concerned primarily with thearea of intersection of linguistics, philosophy, and psychology. Their primarypurpose is to show how the rather technical study of language structure cancontribute to an understanding of human intelligence. I believe, and try to showin these essays, that the study of language structure reveals properties of mindthat underlie the exercise of human mental capacities in normal activities, suchas the use of language in the ordinary free and creative fashion.
At the cost of a ﬁnal redundancy, I would like to underscore here the observa-
tions in the preface to Language and Mind concerning the so-called “behavioral
sciences.” Currently there is a good deal of discussion – and not infrequently,rather extravagant claims – concerning the implications of the behavioral sci-ences for human affairs. It is important to bear in mind that there are few non-trivial empirical hypotheses relating to the question of how humans behave andwhy they act as they do under ordinary circumstances. The reader who under-takes the useful exercise of searching the literature will discover, I believe,not only that there is little signiﬁcant scientiﬁc knowledge in this domain, butfurther that the behavioral sciences have commonly insisted upon certain arbi-trary methodological restrictions that make it virtually impossible for scientiﬁcknowledge of a nontrivial character to be attained.
Wecan begin to see how human knowledge and systems of belief might
be acquired, in certain areas. The case of language is particularly interestingbecause language plays an essential role in thinking and human interaction,and because in this case we can begin to describe the system of knowledgethat is attained and to formulate some plausible hypotheses about the intrinsic

--- Page 16 ---

Preface to the second edition xv
human capacities that make this achievement possible. These glimmerings of
understanding are interesting in themselves and suggestive as well for otherstudies. We can be reasonably certain that the investigation of direct relationsbetween experience and action, between stimuli and responses, will in generalbe a vain pursuit. In all but the most elementary cases, what a person doesdepends in large measure on what he knows, believes, and anticipates. A studyof human behavior that is not based on at least a tentative formulation of relevantsystems of knowledge and belief is predestined to triviality and irrelevance. Thestudy of human learning can begin, in a serious way, only when such a tentativeformulation of systems of knowledge and belief is presented. We can thenask by what means these systems are acquired, given the data of experience.Similarly, the study of human behavior can hardly be undertaken in a seriouswayunless we are in a position to ask how what a person does is related to what
he knows, believes, and expects. Only when we have formulated some tentativehypotheses as to what is learned can we undertake a serious study of humanlearning; only when we have formulated some tentative hypotheses as to whathas been learned – what is known and believed – can we turn in a serious wayto the investigation of behavior. In the case of language, we can present sometentative but rather detailed and complex formulations of what is known, whathas been learned by the normal speaker-hearer. For this reason, the study oflanguage seems to me of particular interest for the study of human learning andbehavior.
But it must be emphasized that language may be a rather special case. Knowl-
edge of language is normally attained through brief exposure, and the characterof the acquired knowledge may be largely predetermined. One would expectthat human language should directly reﬂect the characteristics of human intel-lectual capacities, that language should be a direct “mirror of mind” in ways inwhich other systems of knowledge and belief cannot. Furthermore, even if wewere able to account for the acquisition of language along the lines discussedin these essays, we would still be left with the problem of accounting for thenormal use of the acquired knowledge. But this problem is, at the moment, quiteintractable. It lies beyond the scope of scientiﬁc inquiry. Of course, it wouldbe entirely irrational to argue that certain phenomena and certain problems donot exist, merely because they lie beyond the scope of scientiﬁc inquiry – atpresent, and perhaps intrinsically because of the scope of human intelligence,which after all is itself structured and bounded in ways that are unknown inany detail. Given the primitive character of the study of man and society and itsgeneral lack of intellectual substance, we can only speculate about the essentialand basic factors that enter into human behavior, and it would be quite irrespon-sible to claim otherwise. Speculation about these matters is quite legitimate,even essential. It should be guided, where this is possible, by such limited and
fragmentary knowledge as exists. But speculation should be clearly labeled

--- Page 17 ---

xvi Preface to the second edition
as such and clearly distinguished from the achievements of scientiﬁc inquiry.
This is a matter of considerable importance in a society that tends to trust inprofessional expertise and to rely on professional judgments. The scientist, inparticular, has a responsibility to the public in this regard.
Massachusetts Institute of Technology N. C.

--- Page 18 ---

Preface to the ﬁrst edition
The three chapters of this book are somewhat elaborated versions of three
lectures, the Beckman lectures, that I delivered at the University of California,at Berkeley, in January 1967. The ﬁrst is an attempt to evaluate past contributionsto the study of mind that have been based on research and speculation regardingthe nature of language. The second is devoted to contemporary developmentsin linguistics that have a bearing on the study of mind. The third is a highlyspeculative discussion of directions that the study of language and mind mighttake in coming years. The three lectures, then, are concerned with the past, thepresent, and the future.
Given the state of research into the history of linguistics, even the attempt
to evaluate past contributions must be regarded as highly tentative. Modernlinguistics shares the delusion – the accurate term, I believe – that the modern“behavioral sciences” have in some essential respect achieved a transition from“speculation” to “science” and that earlier work can be safely consigned tothe antiquarians. Obviously any rational person will favor rigorous analysisand careful experiment; but to a considerable degree, I feel, the “behavioralsciences” are merely mimicking the surface features of the natural sciences;much of their scientiﬁc character has been achieved by a restriction of subjectmatter and a concentration on rather peripheral issues. Such narrowing of focuscan be justiﬁed if it leads to achievements of real intellectual signiﬁcance, but inthis case, I think it would be very difﬁcult to show that the narrowing of scopehas led to deep and signiﬁcant results. Furthermore, there has been a natural butunfortunate tendency to “extrapolate,” from the thimbleful of knowledge thathas been attained in careful experimental work and rigorous data-processing,to issues of much wider signiﬁcance and of great social concern. This is aserious matter. The experts have the responsibility of making clear the actuallimits of their understanding and of the results they have so far achieved, and acareful analysis of these limits will demonstrate, I believe, that in virtually everydomain of the social and behavioral sciences the results achieved to date willnot support such “extrapolation.” Such analysis will also show, I believe, thatthe contributions of earlier thought and speculation cannot be safely neglected,that in large measure they provide an indispensable basis for serious work today.
xvii

--- Page 19 ---

xviii Preface to the ﬁrst edition
Idonot attempt here to justify this point of view in general, but merely assert
that it is the point of view underlying the lectures that follow.
In the second lecture I have made no attempt to give a systematic presenta-
tion of what has been achieved in linguistic research; rather, I have concentrated
on problems that are at the borderline of research and that still resist solution.Much of the material in this lecture is to appear in a chapter entitled “Prob-lems of Explanation in Linguistics” in Explanations in Psychology ,edited by
R. Borger and F. Ciofﬁ (New York: Cambridge University Press, 1967), alongwith interesting critical comments by Max Black. Lectures 1 and 3 make useof some material from a lecture delivered at the University of Chicago in April1966 that appears in Changing Perspectives on Man ,edited by B. Rothblatt
(Chicago: University of Chicago Press, 1968). A portion of the ﬁrst lecture waspublished in the Columbia University Forum ,Spring 1968 (V ol. XI, No. 1), and
aportion of the third lecture will appear in the Fall 1968 issue (V ol. XI, No. 3).
Iwould like to express my thanks to members of the faculty and the student
body at Berkeley for many useful comments and reactions and, more generally,for the rich and stimulating intellectual climate in which I was privileged tospend several months just prior to these lectures. I am also indebted to JohnRoss and Morris Halle for helpful comments and suggestions.

--- Page 20 ---

1 Linguistic contributions to the study
of mind: past
In these lectures, I would like to focus attention on the question, What contri-
bution can the study of language make to our understanding of human nature?
In one or another manifestation, this question threads its way through modernWestern thought. In an age that was less self-conscious and less compartmen-
talized than ours, the nature of language, the respects in which language mirrorshuman mental processes or shapes the ﬂow and character of thought – these weretopics for study and speculation by scholars and gifted amateurs with a widevariety of interests, points of view, and intellectual backgrounds. And in the
nineteenth and twentieth centuries, as linguistics, philosophy, and psychologyhave uneasily tried to go their separate ways, the classical problems of languageand mind have inevitably reappeared and have served to link these divergingﬁelds and to give direction and signiﬁcance to their efforts. There have beensigns in the past decade that the rather artiﬁcial separation of disciplines may becoming to an end. It is no longer a point of honor for each to demonstrate its abso-lute independence of the others, and new interests have emerged that permit theclassical problems to be formulated in novel and occasionally suggestive ways –for example, in terms of the new perspectives provided by cybernetics and thecommunication sciences, and against the background of developments in com-parative and physiological psychology that challenge long-standing convictionsand free the scientiﬁc imagination from certain shackles that had become sofamiliar a part of our intellectual environment as to be almost beyond aware-
ness. All of this is highly encouraging. I think there is more of a healthy fermentin cognitive psychology – and in the particular branch of cognitive psychologyknown as linguistics – than there has been for many years. And one of themost encouraging signs is that skepticism with regard to the orthodoxies of therecent past is coupled with an awareness of the temptations and the dangers ofpremature orthodoxy, an awareness that, if it can persist, may prevent the riseof new and stultifying dogma.
It is easy to be misled in an assessment of the current scene; nevertheless,
it seems to me that the decline of dogmatism and the accompanying searchfor new approaches to old and often still intractable problems are quite unmis-takable, not only in linguistics but in all of the disciplines concerned with the
1

--- Page 21 ---

2 Language and Mind
study of mind. I remember quite clearly my own feeling of uneasiness as a stu-
dent at the fact that, so it seemed, the basic problems of the ﬁeld were solved,and that what remained was to sharpen and improve techniques of linguisticanalysis that were reasonably well understood and to apply them to a widerrange of linguistic materials. In the postwar years, this was a dominant atti-tude in most active centers of research. I recall being told by a distinguishedanthropological linguist, in 1953, that he had no intention of working throughavast collection of materials that he had assembled because within a few years
it would surely be possible to program a computer to construct a grammar fromalarge corpus of data by the use of techniques that were already fairly well
formalized. At the time, this did not seem an unreasonable attitude, though theprospect was saddening for anyone who felt, or at least hoped, that the resourcesof human intelligence were somewhat deeper than these procedures and tech-niques might reveal. Correspondingly, there was a striking decline in studies oflinguistic method in the early 1950s as the most active theoretical minds turnedto the problem of how an essentially closed body of technique could be appliedto some new domain – say, to analysis of connected discourse, or to other cul-tural phenomena beyond language. I arrived at Harvard as a graduate studentshortly after B. F. Skinner had delivered his William James Lectures, later to bepublished in his book Verbal Behavior. Among those active in research in the
philosophy or psychology of language, there was then little doubt that althoughdetails were missing, and although matters could not really be quite that sim-ple, nevertheless a behavioristic framework of the sort Skinner had outlinedwould prove quite adequate to accommodate the full range of language use.
There was now little reason to question the conviction of Leonard Bloomﬁeld,Bertrand Russell, and positivistic linguists, psychologists, and philosophersin general that the framework of stimulus-response psychology would soonbe extended to the point where it would provide a satisfying explanation forthe most mysterious of human abilities. The most radical souls felt that per-haps, in order to do full justice to these abilities, one must postulate little s’s
andr’sinside the brain alongside the capital S’sandR’sthat were open to
immediate inspection, but this extension was not inconsistent with the generalpicture.
Critical voices, even those that commanded considerable prestige, were sim-
ply unheard. For example, Karl Lashley gave a brilliant critique of the prevailingframework of ideas in 1948, arguing that underlying language use – and all orga-nized behavior – there must be abstract mechanisms of some sort that are notanalyzable in terms of association and that could not have been developed byany such simple means. But his arguments and proposals, though sound andperceptive, had absolutely no effect on the development of the ﬁeld and wentby unnoticed even at his own university (Harvard), then the leading center ofpsycholinguistic research. Ten years later Lashley’s contribution began to be

--- Page 22 ---

Linguistic contributions: past 3
appreciated, but only after his insights had been independently achieved in
another context.
The technological advances of the 1940s simply reinforced the general eupho-
ria. Computers were on the horizon, and their imminent availability reinforcedthe belief that it would sufﬁce to gain a theoretical understanding of only thesimplest and most superﬁcially obvious of phenomena – everything else wouldmerely prove to be “more of the same,” an apparent complexity that wouldbe disentangled by the electronic marvels. The sound spectrograph, developedduring the war, offered similar promise for the physical analysis of speechsounds. The interdisciplinary conferences on speech analysis of the early 1950smake interesting reading today. There were few so benighted as to question thepossibility, in fact the immediacy, of a ﬁnal solution to the problem of con-verting speech into writing by available engineering technique. And just a few
years later, it was jubilantly discovered that machine translation and automaticabstracting were also just around the corner. For those who sought a moremathematical formulation of the basic processes, there was the newly devel-oped mathematical theory of communication, which, it was widely believed inthe early 1950s, had provided a fundamental concept – the concept of “infor-mation” – that would unify the social and behavioral sciences and permit thedevelopment of a solid and satisfactory mathematical theory of human behav-ior on a probabilistic base. At about the same time, the theory of automatadeveloped as an independent study, making use of closely related mathematicalnotions. And it was linked at once, and quite properly, to earlier explorationsof the theory of neural nets. There were those – John von Neumann, for exam-ple – who felt that the entire development was dubious and shaky at best, andprobably quite misconceived, but such qualms did not go far to dispel the feel-ing that mathematics, technology, and behavioristic linguistics and psychologywere converging on a point of view that was very simple, very clear, and fullyadequate to provide a basic understanding of what tradition had left shrouded inmystery.
In the United States at least, there is little trace today of the illusions of the
early postwar years. If we consider the current status of structural linguisticmethodology, stimulus-response psycholinguistics (whether or not extended to“mediation theory”), or probabilistic or automata-theoretic models for languageuse, we ﬁnd that in each case a parallel development has taken place: a carefulanalysis has shown that insofar as the system of concepts and principles thatwasadvanced can be made precise, it can be demonstrated to be inadequate
in a fundamental way. The kinds of structures that are realizable in terms ofthese theories are simply not those that must be postulated to underlie the useof language, if empirical conditions of adequacy are to be satisﬁed. What ismore, the character of the failure and inadequacy is such as to give little reasonto believe that these approaches are on the right track. That is, in each case it

--- Page 23 ---

4 Language and Mind
has been argued – quite persuasively, in my opinion – that the approach is not
only inadequate but misguided in basic and important ways. It has, I believe,become quite clear that if we are ever to understand how language is used oracquired, then we must abstract for separate and independent study a cognitivesystem, a system of knowledge and belief, that develops in early childhood andthat interacts with many other factors to determine the kinds of behavior thatwe observe; to introduce a technical term, we must isolate and study the systemoflinguistic competence that underlies behavior but that is not realized in any
direct or simple way in behavior. And this system of linguistic competenceis qualitatively different from anything that can be described in terms of thetaxonomic methods of structural linguistics, the concepts of S-R psychology,or the notions developed within the mathematical theory of communication orthe theory of simple automata. The theories and models that were developedto describe simple and immediately given phenomena cannot incorporate thereal system of linguistic competence; “extrapolation” for simple descriptionscannot approach the reality of linguistic competence; mental structures arenot simply “more of the same” but are qualitatively different from the complexnetworks and structures that can be developed by elaboration of the concepts thatseemed so promising to many scientists just a few years ago. What is involvedis not a matter of degree of complexity but rather a quality of complexity.Correspondingly, there is no reason to expect that the available technologycan provide signiﬁcant insight or understanding or useful achievements; it hasnoticeably failed to do so, and, in fact, an appreciable investment of time,energy, and money in the use of computers for linguistic research – appreciableby the standards of a small ﬁeld like linguistics – has not provided any signiﬁcantadvance in our understanding of the use or nature of language. These judgmentsare harsh, but I think they are defensible. They are, furthermore, hardly debatedby active linguistic or psycholinguistic researchers.
At the same time there have been signiﬁcant advances, I believe, in our
understanding of the nature of linguistic competence and some of the ways inwhich it is put to use, but these advances, such as they are, have proceeded fromassumptions very different from those that were so enthusiastically put forthin the period I have been discussing. What is more, these advances have notnarrowed the gap between what is known and what can be seen to lie beyond thescope of present understanding and technique; rather, each advance has madeit clear that these intellectual horizons are far more remote than was heretoforeimagined. Finally, it has become fairly clear, it seems to me, that the assumptionsand approaches that appear to be productive today have a distinctly traditionalﬂavor to them; in general, a much despised tradition has been largely revitalizedin recent years and its contributions given some serious and, I believe, well-deserved attention. From the recognition of these facts ﬂows the general andquite healthy attitude of skepticism that I spoke of earlier.

--- Page 24 ---

Linguistic contributions: past 5
In short, it seems to me quite appropriate, at this moment in the development
of linguistics and psychology in general, to turn again to classical questions and
to ask what new insights have been achieved that bear on them, and how theclassical issues may provide direction for contemporary research and study.
When we turn to the history of study and speculation concerning the nature
of mind and, more speciﬁcally, the nature of human language, our attentionquite naturally comes to focus on the seventeenth century, “the century ofgenius,” in which the foundations of modern science were ﬁrmly establishedand the problems that still confound us were formulated with remarkable clarityand perspicuity. There are many far from superﬁcial respects in which theintellectual climate of today resembles that of seventeenth-century WesternEurope. One, particularly crucial in the present context, is the very great interestin the potentialities and capacities of automata, a problem that intrigued theseventeenth-century mind as fully as it does our own. I mentioned above thatthere is a slowly dawning realization that a signiﬁcant gap – more accurately,ayawning chasm – separates the system of concepts of which we have a fairly
clear grasp, on the one hand, and the nature of human intelligence, on theother. A similar realization lies at the base of Cartesian philosophy. Descartesalso arrived, quite early in his investigations, at the conclusion that the studyof mind faces us with a problem of quality of complexity, not merely degreeof complexity. He felt that he had demonstrated that understanding and will,the two fundamental properties of the human mind, involved capacities andprinciples that are not realizable by even the most complex of automata.
It is particularly interesting to trace the development of this argument in the
works of the minor and now quite forgotten Cartesian philosophers, like Corde-
moy, who wrote a fascinating treatise extending Descartes’ few remarks aboutlanguage, or La Forge, who produced a long and detailed Trait´edel’esprit de
l’homme expressing, so he claimed with some reason, what Descartes would
likely have said about this subject had he lived to extend his theory of manbeyond physiology. One may question the details of this argument, and onecan show how it was impeded and distorted by certain remnants of scholasticdoctrine – the framework of substance and mode, for example. But the generalstructure of the argument is not unreasonable; it is, in fact, rather analogousto the argument against the framework of ideas of the early postwar years,which I mentioned at the outset of this lecture. The Cartesians tried to showthat when the theory of corporeal body is sharpened and clariﬁed and extendedto its limits, it is still incapable of accounting for facts that are obvious tointrospection and that are also conﬁrmed by our observation of the actions ofother humans. In particular, it cannot account for the normal use of human lan-guage, just as it cannot explain the basic properties of thought. Consequently,it becomes necessary to invoke an entirely new principle – in Cartesian terms,to postulate a second substance whose essence is thought, alongside of body,

--- Page 25 ---

6 Language and Mind
with its essential properties of extension and motion. This new principle has
a“creative aspect,” which is evidenced most clearly in what we may refer
to as “the creative aspect of language use,” the distinctively human ability toexpress new thoughts and to understand entirely new expressions of thought,
within the framework of an “instituted language,” a language that is a cul-tural product subject to laws and principles partially unique to it and partiallyreﬂections of general properties of mind. These laws and principles, it is main-tained, are not formulable in terms of even the most elaborate extension of theconcepts proper to the analysis of behavior and interaction of physical bod-ies, and they are not realizable by even the most complex automaton. In fact,Descartes argued that the only sure indication that another body possesses ahuman mind, that it is not a mere automaton, is its ability to use language inthe normal way; and he argued that this ability cannot be detected in an animalor an automaton which, in other respects, shows signs of apparent intelligenceexceeding those of a human, even though such an organism or machine might be
as fully endowed as a human with the physiological organs necessary to producespeech.
Iwill return to this argument and the ways in which it was developed. But
Ithink it is important to stress that, with all its gaps and deﬁciencies, it is an
argument that must be taken seriously. There is nothing at all absurd in theconclusion. It seems to me quite possible that at that particular moment in thedevelopment of Western thought there was the possibility for the birth of a sci-ence of psychology of a sort that still does not exist, a psychology that beginswith the problem of characterizing various systems of human knowledge andbelief, the concepts in terms of which they are organized and the principles thatunderlie them, and that only then turns to the study of how these systems mighthave developed through some combination of innate structure and organism –environment interaction. Such a psychology would contrast rather sharply withthe approach to human intelligence that begins by postulating, on a priorigrounds, certain speciﬁc mechanisms that, it is claimed, must be those underly-
ing the acquisition of all knowledge and belief. The distinction is one to whichIwill return in a subsequent lecture. For the moment, I want merely to stress the
reasonableness of the rejected alternative and, what is more, its consistency withthe approach that proved so successful in the seventeenth-century revolution inphysics.
There are methodological parallels that have perhaps been inadequately
appreciated between the Cartesian postulation of a substance whose essencewasthought and the post-Newtonian acceptance of a principle of attraction
as an innate property of the ultimate corpuscles of matter, an active principlethat governs the motions of bodies. Perhaps the most far-reaching contributionof Cartesian philosophy to modern thought was its rejection of the scholasticnotion of substantial forms and real qualities, of all those “little images ﬂuttering

--- Page 26 ---

Linguistic contributions: past 7
through the air” to which Descartes referred with derision. With the exorcism
of these occult qualities, the stage was set for the rise of a physics of matterin motion and a psychology that explored the properties of mind. But Newtonargued that Descartes’ mechanical physics wouldn’t work – the second book ofthePrincipia is largely devoted to this demonstration – and that it is necessary
to postulate a new force to account for the motion of bodies. The postulateof an attractive force acting at a distance was inconsistent with the clear anddistinct ideas of common sense and could not be tolerated by an orthodox Carte-sian – such a force was merely another occult quality. Newton quite agreed,and he attempted repeatedly to ﬁnd a mechanical explanation of the cause ofgravity. He rejected the view that gravity is “essential and inherent to matter”and maintained that “to tell us that every species of things is endowed with anoccult speciﬁc property (such as gravity) by which it acts and produces manifesteffects, is to tell us nothing.” Some historians of science have suggested thatNewton hoped, like Descartes, to write a Principles of Philosophy butthat his
failure to explain the cause of gravity on mechanical grounds restricted him to
aMathematical Principles of Natural Philosophy. Thus, to the common sense
of Newton as well as the Cartesians, physics was still not adequately grounded,because it postulated a mystical force capable of action at a distance. Similarly,Descartes’ postulation of mind as an explanatory principle was unacceptable tothe empiricist temper. But the astonishing success of mathematical physics car-ried the day against these common-sense objections, and the prestige of the newphysics was so high that the speculative psychology of the Enlightenment tookfor granted the necessity of working within the Newtonian framework, ratherthan on the Newtonian analogy – a very different matter. The occult force ofgravity was accepted as an obvious element of the physical world, requiringno explanation, and it became inconceivable that one might have to postulateentirely new principles of functioning and organization outside the frameworkof what soon became the new “common sense.” Partly for this reason, thesearch for an analogous scientiﬁc psychology that would explore the principlesof mind, whatever they might be, was not undertaken with the thoroughnessthat was then, as now, quite possible.
Idonot want to overlook a fundamental distinction between the postulation
of gravity and the postulation of a rescogitans, namely the enormous disparity
in the power of the explanatory theories that were developed. Nevertheless, Ithink it is instructive to note that the reasons for the dissatisfaction of Newton,Leibnitz, and the orthodox Cartesians with the new physics are strikingly similarto the grounds on which a dualistic rationalist psychology was soon to berejected. I think it is correct to say that the study of properties and organizationof mind was prematurely abandoned, in part on quite spurious grounds, and alsoto point out that there is a certain irony in the common view that its abandonmentwascaused by the gradual spread of a more general “scientiﬁc” attitude.

--- Page 27 ---

8 Language and Mind
Ihavetried to call attention to some similarities between the intellectual
climate of the seventeenth century and that of today. It is illuminating, I think, to
trace in somewhat greater detail the speciﬁc course of development of linguistictheory during the modern period, in the context of the study of mind and ofbehavior in general.
1
Agood place to begin is with the writings of the Spanish physician Juan
Huarte, who in the late sixteenth century published a widely translated studyon the nature of human intelligence. In the course of his investigations, Huartecame to wonder at the fact that the word for “intelligence,” ingenio ,seems to
have the same Latin root as various words meaning “engender” or “generate.”This, he argued, gives a clue to the nature of mind. Thus, “One may discern twogenerative powers in man, one common with the beasts and the plants, and theother participating of spiritual substance. Wit (Ingenio) is a generative power.The understanding is a generative faculty.” Huarte’s etymology is actually notvery good; the insight, however, is quite substantial.
Huarte goes on to distinguish three levels of intelligence. The lowest of these
is the “docile wit,” which satisﬁes the maxim that he, along with Leibnitz andmany others, wrongly attributes to Aristotle, namely that there is nothing inthe mind that is not simply transmitted to it by the senses. The next higherlevel, normal human intelligence, goes well beyond the empiricist limitation:it is able to “engender within itself, by its own power, the principles on whichknowledge rests.” Normal human minds are such that “assisted by the subjectalone, without the help of anybody, they will produce a thousand conceitsthey never heard spoke of . . . inventing and saying such things as they neverheard from their masters, nor any mouth.” Thus, normal human intelligenceis capable of acquiring knowledge through its own internal resources, perhapsmaking use of the data of sense but going on to construct a cognitive system interms of concepts and principles that are developed on independent grounds;and it is capable of generating new thoughts and of ﬁnding appropriate andnovel ways of expressing them, in ways that entirely transcend any training orexperience.
Huarte postulates a third kind of wit, “by means of which some, without
art or study, speak such subtle and surprising things, yet true, that were neverbefore seen, heard, or writ, no, nor ever so much as thought of.” The referencehere is to true creativity, an exercise of the creative imagination in ways that gobeyond normal intelligence and may, he felt, involve “a mixture of madness.”
Huarte maintains that the distinction between docile wit, which meets the
empiricist maxim, and normal intelligence, with its full generative capacities,is the distinction between beast and man. As a physician, Huarte was much
1Foradditional details and discussion, see my Cartesian Linguistics (New York: Harper & Row,
1966) and the references cited there.

--- Page 28 ---

Linguistic contributions: past 9
interested in pathology. In particular, he notes that the most severe disability
of wit that can afﬂict a human is a restriction to the lowest of the three levels,to the docile wit that conforms to empiricist principles. This disability, saysHuarte, “resembles that of Eunuchs, incapable of generation.” Under these sadcircumstances, in which the intelligence can only receive stimuli transmittedby sense and associate them with one another, true education is of courseimpossible, since the ideas and principles that permit the growth of knowledgeand understanding are lacking. In this case, then, “neither the lash of the rod,nor cries, nor method, nor examples, nor time, nor experience, nor anything innature can sufﬁciently excite him to bring forth anything.”
Huarte’s framework is useful for discussing “psychological theory” in the
ensuing period. Typical of later thought is his reference to use of languageas an index of human intelligence, of what distinguishes man from animals,and, speciﬁcally, his emphasis on the creative capacity of normal intelligence.These concerns dominated rationalist psychology and linguistics. With the riseof romanticism, attention shifted to the third type of wit, to true creativity,although the rationalist assumption that normal human intelligence is uniquelyfree and creative and beyond the bounds of mechanical explanation was notabandoned and played an important role in the psychology of romanticism, andeven in its social philosophy.
As I have already mentioned, the rationalist theory of language, which was
to prove extremely rich in insight and achievement, developed in part out of aconcern with the problem of other minds. A fair amount of effort was devoted toaconsideration of the ability of animals to follow spoken commands, to express
their emotional states, to communicate with one another, and even apparently tocooperate for a common goal; all of this, it was argued, could be accounted for on“mechanical grounds,” as this notion was then understood – that is, through thefunctioning of physiological mechanisms in terms of which one could formulatethe properties of reﬂexes, conditioning and reinforcement, association, and soon. Animals do not lack appropriate organs of communication, nor are theysimply lower along some scale of “general intelligence.”
In fact, as Descartes himself quite correctly observed, language is a species-
speciﬁc human possession, and even at low levels of intelligence, at pathologicallevels, we ﬁnd a command of language that is totally unattainable by an apethat may, in other respects, surpass a human imbecile in problem-solving abilityand other adaptive behavior. I will return later to the status of this observation,in the light of what is now known about animal communication. There is abasic element lacking in animals, Descartes argued, as it is lacking in even themost complex automaton that develops its “intellectual structures” completelyin terms of conditioning and association – namely Huarte’s second type of wit,the generative ability that is revealed in the normal human use of languageas a free instrument of thought. If by experiment we convince ourselves that

--- Page 29 ---

10 Language and Mind
another organism gives evidence of the normal, creative use of language, we
must suppose that it, like us, has a mind and that what it does lies beyondthe bounds of mechanical explanation, outside the framework of the stimulus-response psychology of the time, which in relevant essentials is not signiﬁcantlydifferent from that of today, though it falls short in sharpness of technique andscope and reliability of information.
It should not be thought, incidentally, that the only Cartesian arguments for
the beast-machine hypothesis were those derived from the apparent inabilityof animals to manifest the creative aspect of language use. There were alsomany others – for example, the natural fear of population explosion in thedomains of the spirit if every gnat had a soul. Or the argument of CardinalMelchior de Polignac, who argued that the beast-machine hypothesis followedfrom the assumption of the goodness of God, since, as he pointed out, one cansee “how much more humane is the doctrine that animals suffer no pain.”
2Or
there is the argument of Louis Racine, son of the dramatist, who was struckby the following insight: “If beasts had souls and were capable of feelings,would they show themselves insensible to the affront and injustice done them
by Descartes? Would they not rather have risen up in wrath against the leaderand the sect which so degraded them?” One should add, I suppose, that LouisRacine was regarded by his contemporaries as the living proof that a brilliantfather could not have a brilliant son. But the fact is that the discussion of the
existence of other minds, and, in contrast, the mechanical nature of animals,
continually returned to the creative aspect of language use, to the claim that – asformulated by another minor seventeenth-century ﬁgure – “if beasts reasoned,they would be capable of true speech with its inﬁnite variety.”
It is important to understand just what properties of language were most strik-
ing to Descartes and his followers. The discussion of what I have been calling“the creative aspect of language use” turns on three important observations. Theﬁrst is that the normal use of language is innovative, in the sense that much ofwhat we say in the course of normal language use is entirely new, not a repeti-tion of anything that we have heard before and not even similar in pattern – inany useful sense of the terms “similar” and “pattern” – to sentences or discoursethat we have heard in the past. This is a truism, but an important one, often over-looked and not infrequently denied in the behaviorist period of linguistics towhich I referred earlier, when it was almost universally claimed that a person’sknowledge of language is representable as a stored set of patterns, overlearnedthrough constant repetition and detailed training, with innovation being at mostamatter of “analogy.” The fact surely is, however, that the number of sentences
2These examples are taken from the excellent study by Leonora Cohen Rosenﬁeld, From Beast-
Machine to Man-Machine (New York: Oxford University Press, 1941). The quotes are her para-
phrases of the original.

--- Page 30 ---

Linguistic contributions: past 11
in one’s native language that one will immediately understand with no feeling
of difﬁculty or strangeness is astronomical; and that the number of patternsunderlying our normal use of language and corresponding to meaningful andeasily comprehensible sentences in our language is orders of magnitude greaterthan the number of seconds in a lifetime. It is in this sense that the normal useof language is innovative.
However, in the Cartesian view even animal behavior is potentially inﬁnite
in its variety, in the special sense in which the readings of a speedometer can besaid, with an obvious idealization, to be potentially inﬁnite in variety. That is,if animal behavior is controlled by external stimuli or internal states (the latterincluding those established by conditioning), then as the stimuli vary over anindeﬁnite range, so may the behavior of the animal. But the normal use oflanguage is not only innovative and potentially inﬁnite in scope, but also freefrom the control of detectable stimuli, either external or internal. It is becauseof this freedom from stimulus control that language can serve as an instrumentof thought and self-expression, as it does not only for the exceptionally giftedand talented, but also, in fact, for every normal human.
Still, the properties of being unbounded and free from stimulus control do
not, in themselves, exceed the bounds of mechanical explanation. And Cartesiandiscussion of the limits of mechanical explanation therefore took note of a thirdproperty of the normal use of language, namely its coherence and its “appropri-ateness to the situation” – which of course is an entirely different matter fromcontrol by external stimuli. Just what “appropriateness” and “coherence” mayconsist in we cannot say in any clear or deﬁnite way, but there is no doubt thatthese are meaningful concepts. We can distinguish normal use of language fromthe ravings of a maniac or the output of a computer with a random element.
Honesty forces us to admit that we are as far today as Descartes was three
centuries ago from understanding just what enables a human to speak in awaythat is innovative, free from stimulus control, and also appropriate and
coherent. This is a serious problem that the psychologist and biologist mustultimately face and that cannot be talked out of existence by invoking “habit”or “conditioning” or “natural selection.”
The Cartesian analysis of the problem of other minds, in terms of the cre-
ative aspect of language use and similar indications of the limits of mechanicalexplanation, was not entirely satisfying to contemporary opinion – Bayle’s
Dictionary ,for example, cites the inability to give a satisfactory proof of the
existence of other minds as the weakest element in the Cartesian philosophy –
and there was a long and intriguing series of discussions and polemics regard-ing the problems that Descartes raised. From the vantage point of several cen-turies, we can see that the debate was inconclusive. The properties of humanthought and human language emphasized by the Cartesians are real enough; theywere then, as they are now, beyond the bounds of any well-understood kind of

--- Page 31 ---

12 Language and Mind
physical explanation. Neither physics nor biology nor psychology gives us any
clue as to how to deal with these matters.
As in the case of other intractable problems, it is tempting to try another
approach, one that might show the problem to be misconceived, the result ofsome conceptual confusion. This is a line of argument that has been followed incontemporary philosophy, but, it seems to me, without success. It is clear that theCartesians understood, as well as Gilbert Ryle and other contemporary criticsunderstand, the difference between providing criteria for intelligent behavior, onthe one hand, and providing an explanation for the possibility of such behavior,on the other; but, as distinct from Ryle, they were interested in the latter problemas well as the former. As scientists, they were not satisﬁed with the formulationof experimental tests that would show the behavior of another organism to becreative, in the special sense just outlined; they were also troubled, and quiterightly so, by the fact that the abilities indicated by such tests and observationalcriteria transcended the capacities of corporeal bodies as they understood them,just as they are beyond the scope of physical explanation as we understand ittoday. There is surely nothing illegitimate in an attempt to go beyond elaborationof observational tests and collection of evidence to the construction of some the-oretical explanation for what is observed, and this is just what was at stake in theCartesian approach to the problem of mind. As La Forge and others insisted, itis necessary to go beyond what one can perceive or “imagine” (in the technical,classical sense of this term) if one hopes to understand the nature of “l’esprit del’homme,” just as Newton did – successfully – in trying to understand the natureof planetary motion. On the other hand, the proposals of the Cartesians werethemselves of no real substance; the phenomena in question are not explainedsatisfactorily by attributing them to an “active principle” called “mind,” theproperties of which are not developed in any coherent or comprehensiveway.
It seems to me that the most hopeful approach today is to describe the phe-
nomena of language and of mental activity as accurately as possible, to try todevelop an abstract theoretical apparatus that will as far as possible account forthese phenomena and reveal the principles of their organization and functioning,without attempting, for the present, to relate the postulated mental structuresand processes to any physiological mechanisms or to interpret mental functionin terms of “physical causes.” We can only leave open for the future the ques-tion of how these abstract structures and processes are realized or accountedfor in some concrete terms, conceivably in terms that are not within the rangeof physical processes as presently understood – a conclusion that, if correct,should surprise no one.
This rationalist philosophy of language merged with various other indepen-
dent developments in the seventeenth century, leading to the ﬁrst really signiﬁ-cant general theory of linguistic structure, namely the general point of view that

--- Page 32 ---

Linguistic contributions: past 13
came to be known as “philosophical” or “universal” grammar. Unfortunately,
philosophical grammar is very poorly known today. There are few technical orscholarly studies, and these few are apologetic and disparaging. References tophilosophical grammar in modern treatises on language are so distorted as tobe quite worthless. Even a scholar with such high standards as Leonard Bloom-ﬁeld gives an account of philosophical grammar in his major work, Language ,
that bears almost no resemblance to the original and attributes to this traditionviews diametrically opposed to those that were most typical of it. For example,Bloomﬁeld and many others describe philosophical grammar as based on aLatin model, as prescriptive, as showing no interest in the sounds of speech, asgiven to a confusion of speech with writing. All these charges are false, and itis important to dispel these myths to make possible an objective evaluation ofwhat was actually accomplished.
It is particularly ironic that philosophical grammar should be accused of a
Latin bias. In fact, it is signiﬁcant that the original works – the Port-RoyalGrammar andLogic ,inparticular – were written in French, the point being that
they formed part of the movement to replace Latin by the vernacular. The factis that Latin was regarded as an artiﬁcial and distorted language, one positivelyinjurious to the exercise of the plain thinking and common-sense discourse bywhich the Cartesians set such store. The practitioners of philosophical grammarused such linguistic materials as were available to them; it is noteworthy thatsome of the topics that were studied with the greatest care and persistencefor well over a century involved points of grammar that do not even havean analogue in Latin. A striking example is the so-called rule of Vaugelas,which involves the relation between indeﬁnite articles and relative clauses inFrench. For 150 years the rule of Vaugelas was the central issue debated in thecontroversy over the possibility of developing a “rational grammar,” one whichwould go beyond description to achieve a rational explanation for phenomena.
No doubt it is a complete misunderstanding of the issue of rational explana-
tion that leads to the charge of “prescriptivism” that is leveled, quite erroneously,against philosophical grammar. In fact, there is no issue of prescriptivism. Itwaswell understood and frequently reiterated that the facts of usage are what
they are, and that it is not the place of the grammarian to legislate. At stake wasan entirely different matter, namely the problem of accounting for the facts ofusage on the basis of explanatory hypotheses concerning the nature of languageand, ultimately, the nature of human thought. Philosophical grammarians hadlittle interest in the accumulation of data, except insofar as such data could beused as evidence bearing on deeper processes of great generality. The contrast,then, is not between descriptive and prescriptive grammar, but between descrip-tion and explanation, between grammar as “natural history” and grammar as akind of “natural philosophy” or, in modern terms, “natural science.” A largelyirrational objection to explanatory theories as such has made it difﬁcult for

--- Page 33 ---

14 Language and Mind
modern linguistics to appreciate what was actually at stake in these develop-
ments and has led to a confusion of philosophical grammar with the effort toteach better manners to a rising middle class.
The whole matter is not without interest. I mentioned earlier that there are
striking similarities between the seventeenth-century climate of opinion and thatof contemporary cognitive psychology and linguistics. One point of similarityhas to do with precisely this matter of explanatory theory. Philosophical gram-mar, very much like current generative grammar, developed in self-consciousopposition to a descriptive tradition that interpreted the task of the grammarianto be merely that of recording and organizing the data of usage – a kind ofnatural history. It maintained – quite correctly, I believe – that such a restrictionwasdebilitating and unnecessary and that, whatever justiﬁcation it may have,
it has nothing to do with the method of science – which is typically concernedwith data not for itself but as evidence for deeper, hidden organizing principles,principles that cannot be detected “in the phenomena” nor derived from them bytaxonomic data-processing operations, any more than the principles of celestialmechanics could have been developed in conformity with such strictures.
Contemporary scholarship is not in a position to give a deﬁnitive assessment
of the achievements of philosophical grammar. The ground-work has not beenlaid for such an assessment, the original work is all but unknown in itself, andmuch of it is almost unobtainable. For example, I have been unable to locate asingle copy, in the United States, of the only critical edition of the Port-RoyalGrammar ,produced over a century ago; and although the French original is
now once again available,
3the one English translation of this important work
is apparently to be found only in the British Museum. It is a pity that this workshould have been so totally disregarded, since what little is known about it isintriguing and quite illuminating.
This is not the place to attempt a preliminary assessment of this work or
even to sketch its major outlines as they now appear, on the basis of present,
quite inadequate knowledge. However, I do want to mention at least a few ofthe persistent themes. It seems that one of the innovations of the Port-RoyalGrammar of 1660 – the work that initiated the tradition of philosophical gram-
mar – was its recognition of the importance of the notion of the phrase as agrammatical unit. Earlier grammar had been largely a grammar of word classesand inﬂections. In the Cartesian theory of Port-Royal, a phrase corresponds toacomplex idea and a sentence is subdivided into consecutive phrases, which
are further subdivided into phrases, and so on, until the level of the word isreached. In this way we derive what might be called the “surface structure” ofthe sentence in question. To use what became a standard example, the sentence“Invisible God created the visible world” contains the subject “invisible God”
3Menston, England: Scolar Press Limited, 1967.

--- Page 34 ---

Linguistic contributions: past 15
and the predicate “created the visible world,” the latter contains the complex
idea “the visible world” and the verb “created,” and so on. But it is interestingthat although the Port-Royal Grammar is apparently the ﬁrst to rely in a fairly
systematic way on analysis into surface structure, it also recognized the inad-equacy of such analysis. According to the Port-Royal theory, surface structurecorresponds only to sound – to the corporeal aspect of language; but when thesignal is produced, with its surface structure, there takes place a correspondingmental analysis into what we may call the deep structure, a formal structurethat relates directly not to the sound but to the meaning. In the example justgiven, “Invisible God created the visible world,” the deep structure consistsof a system of three propositions, “that God is invisible,” “that he created theworld,” “that the world is visible.” The propositions that interrelate to form the
deep structure are not, of course, asserted when the sentence is used to makeastatement; if I say that a wise man is honest, I am not asserting that men
are wise or honest, even though in the Port-Royal theory the propositions “aman is wise” and “a man is honest” enter into the deep structure. Rather, thesepropositions enter into the complex ideas that are present to the mind, thoughrarely articulated in the signal, when the sentence is uttered.
The deep structure is related to the surface structure by certain mental opera-
tions – in modern terminology, by grammatical transformations. Each languagecan be regarded as a particular relation between sound and meaning. Followingthe Port-Royal theory to its logical conclusions, then, the grammar of a languagemust contain a system of rules that characterizes deep and surface structuresand the transformational relation between them, and – if it is to accommodatethe creative aspect of language use – that does so over an inﬁnite domain ofpaired deep and surface structures. To use the terminology Wilhelm von Hum-boldt used in the 1830s, the speaker makes inﬁnite use of ﬁnite means. Hisgrammar must, then, contain a ﬁnite system of rules that generates inﬁnitelymany deep and surface structures, appropriately related. It must also containrules that relate these abstract structures to certain representations of sound andmeaning – representations that, presumably, are constituted of elements thatbelong to universal phonetics and universal semantics, respectively. In essence,this is the concept of grammatical structure as it is being developed and elabo-rated today. Its roots are clearly to be found in the classical tradition that I amnow discussing, and the basic concepts were explored with some success in thisperiod.
The theory of deep and surface structure seems straightforward enough, at
least in rough outline. Nevertheless, it was rather different from anything thatpreceded it, and, somewhat more surprising, it disappeared almost without atrace as modern linguistics developed in the late nineteenth century. I want tosay just a word about the relationship of the theory of deep and surface structureto earlier and later thinking about language.

--- Page 35 ---

16 Language and Mind
There is a similarity, which I think can be highly misleading, between the
theory of deep and surface structure and a much older tradition. The practi-
tioners of philosophical grammar were very careful to stress this similarityin their detailed development of the theory and had no hesitation in expressingtheir debt to classical grammar as well as to such major ﬁgures of renaissancegrammar as the Spanish scholar Sanctius. Sanctius, in particular, had developedatheory of ellipsis that had great inﬂuence on philosophical grammar. As I have
already remarked, philosophical grammar is poorly understood today. But suchantecedents as Sanctius have fallen into total oblivion. Furthermore, as in thecase of all such work, there is a problem of determining not only what he saidbutalso, more importantly, what he meant.
There is no doubt that in developing his concept of ellipsis as a fundamental
property of language, Sanctius gave many linguistic examples that superﬁ-cially are closely parallel to those that were used to develop the theory of deepand surface structure, both in classical philosophical grammar and in its farmore explicit modern variants. It means, however, that the concept of ellipsis isintended by Sanctius merely as a device for the interpretation of texts. Thus, todetermine the true meaning of an actual literary passage one must very often,according to Sanctius, regard it as an elliptical variant of a more elaborate para-phrase. But the Port-Royal theory and its later development, particularly at thehands of the encyclopedist Du Marsais, gave a rather different interpretation toellipsis. The clear intent of philosophical grammar was to develop a psycholog-ical theory, not a technique of textual interpretation. The theory holds that theunderlying deep structure, with its abstract organization of linguistic forms, is“present to the mind,” as the signal, with its surface structure, is produced orperceived by the bodily organs. And the transformational operations relatingdeep and surface structure are actual mental operations, performed by the mindwhen a sentence is produced or understood. The distinction is fundamental.Under the latter interpretation, it follows that there must be, represented in themind, a ﬁxed system of generative principles that characterize and associatedeep and surface structures in some deﬁnite wa y–a grammar, in other words,
that is used in some fashion as discourse is produced or interpreted. This gram-mar represents the underlying linguistic competence to which I referred earlier.The problem of determining the character of such grammars and the principlesthat govern them is a typical problem of science, perhaps very difﬁcult, but inprinciple admitting of deﬁnite answers that are right or wrong as they do or donot correspond to the mental reality. But the theory of ellipsis as a technique oftextual interpretation need not consist of a set of principles represented some-how in the mind as an aspect of normal human competence and intelligence.Rather, it can be in part ad hoc and can involve many cultural and personalfactors relevant to the literary work under analysis.

--- Page 36 ---

Linguistic contributions: past 17
The Port-Royal theory of deep and surface structure belongs to psychology
as an attempt to elaborate Huarte’s second type of wit, as an exploration of the
properties of normal human intelligence. The concept of ellipsis in Sanctius,if I understand it correctly, is one of many techniques, to be applied as condi-tions warrant and having no necessary mental representation as an aspect of anormal intelligence. Although the linguistic examples used are often similar,the context in which they are introduced and the framework in which they ﬁtare fundamentally different; in particular, they are separated by the Cartesianrevolution. I propose this with some difﬁdence, because of the obscurity of therelevant texts and their intellectual backgrounds, but this interpretation seemsto me correct.
The relation of the Port-Royal theory to modern structural and descriptive
linguistics is somewhat clearer. The latter restricts itself to the analysis of whatIhavecalled surface structure, to formal properties that are explicit in the signal
and to phrases and units that can be determined from the signal by techniquesof segmentation and classiﬁcation. This restriction is a perfectly self-consciousone, and it was regarde d–I believe quite erroneously – as a great advance. The
great Swiss linguist Ferdinand de Saussure, who at the turn of the century laidthe groundwork for modern structural linguistics, put forth the view that theonly proper methods of linguistic analysis are segmentation and classiﬁcation.Applying these methods, the linguist determines the patterns into which theunits so analyzed fall, where these patterns are either syntagmatic – that is,patterns of literal succession in the stream of speech – or paradigmatic – that is,relations among units that occupy the same position in the stream of speech. Heheld that when all such analysis is complete, the structure of the language is, ofnecessity, completely revealed, and the science of linguistics will have realizedits task completely. Evidently, such taxonomic analysis leaves no place fordeep structure in the sense of philosophical grammar. For example, the systemof three propositions underlying the sentence “Invisible God created the visibleworld” cannot be derived from this sentence by segmentation and classiﬁcation
of segmented units, nor can the transformational operations relating the deepand surface structure, in this case, be expressed in terms of paradigmatic andsyntagmatic structures. Modern structural linguistics has been faithful to theselimitations, which were held to be necessary limitations.
In fact, Saussure in some respects even went beyond this in departing from
the tradition of philosophical grammar. He occasionally expressed the view thatprocesses of sentence formation do not belong to the system of language at all–that the system of language is restricted to such linguistic units as sounds
and words and perhaps a few ﬁxed phrases and a small number of very generalpatterns; the mechanisms of sentence formation are otherwise free from anyconstraint imposed by linguistic structure as such. Thus, in his terms, sentence

--- Page 37 ---

18 Language and Mind
formation is not strictly a matter of langue ,but is rather assigned to what he
called parole ,and thus placed outside the scope of linguistics proper; it is a
process of free creation, unconstrained by linguistic rule except insofar as such
rules govern the forms of words and the patterns of sounds. Syntax, in thisview, is a rather trivial matter. And, in fact, there is very little work in syntaxthroughout the period of structural linguistics.
In taking this position, Saussure echoed an important critique of Humbold-
tian linguistic theory by the distinguished American linguist William DwightWhitney, who evidently greatly inﬂuenced Saussure. According to Whitney,Humboldtian linguistic theory, which in many ways extended the Cartesianviews that I have been discussing, was fundamentally in error. Rather, a lan-guage is simply “made up of a vast number of items, each of which has itsowntime, occasion, and effect.” He maintained that “language in the concrete
sense...i s...t h es u mo fw ords and phrases by which any man expresses his
thought”; the task of the linguist, then, is to list these linguistic forms and tostudy their individual histories. In contrast to philosophical grammar, Whitneyargued that there is nothing universal about the form of language and that onecan learn nothing about the general properties of human intelligence from thestudy of the arbitrary agglomeration of forms that constitutes a human language.As he put it, “The inﬁnite diversity of human speech ought alone to be a sufﬁ-cient bar to the assertion that an understanding of the powers of the soul involvesthe explanation of speech.” Similarly, Delbr¨ uck, in the standard work on Indo-
European comparative syntax, denounced traditional grammar for having setup ideal sentence types underlying the observed signals, referring to Sanctiusas the “major dogmatist in this domain.”
With the expression of such sentiments as these, we enter the modern age of
the study of language. The death-knell of philosophical grammar was soundedwith the remarkable successes of comparative Indo-European studies, whichsurely rank among the outstanding achievements of nineteenth-century science.The impoverished and thoroughly inadequate conception of language expressedby Whitney and Saussure and numerous others proved to be entirely appropriateto the current stage of linguistic research. As a result, this conception washeld to be vindicated, a not unnatural but thoroughly mistaken conviction.Modern structural-descriptive linguistics developed within the same intellectualframework and also made substantial progress, to which I will return directly.In contrast, philosophical grammar did not provide appropriate concepts forthe new comparative grammar or for the study of exotic languages unknownto the investigator, and it was, in a sense, exhausted. It had reached the limitsof what could be achieved within the framework of the ideas and techniquesthat were available. There was no clear understanding a century ago as to howone might proceed to construct generative grammars that “make inﬁnite useof ﬁnite means” and that express the “organic form” of human language, “that

--- Page 38 ---

Linguistic contributions: past 19
marvellous invention” (in the words of the Port-Royal Grammar )“by which we
construct from twenty-ﬁve or thirty sounds an inﬁnity of expressions, which,
having no resemblance in themselves to what takes place in our minds, stillenable us to let others know the secret of what we conceive and of all thevarious mental activities that we carry out.”
Thus, the study of language had arrived at a situation in which there was, on
the one hand, a set of simple concepts that provided the basis for some startlingsuccesses and, on the other, some deep but rather vague ideas that did notseem to lead to any further productive research. The outcome was inevitableand not at all to be deplored. There developed a professionalization of theﬁeld, a shift of interest away from the classical problems of general interest tointellectuals like Arnauld and Humboldt, for example, toward a new domainlargely deﬁned by the techniques that the profession itself has forged in thesolution of certain problems. Such a development is natural and quite proper,butnot without its dangers. Without wishing to exalt the cult of gentlemanly
amateurism, one must nevertheless recognize that the classical issues have aliveliness and signiﬁcance that may be lacking in an area of investigation thatis determined by the applicability of certain tools and methods, rather than byproblems that are of intrinsic interest in themselves.
The moral is not to abandon useful tools; rather, it is, ﬁrst, that one should
maintain enough perspective to be able to detect the arrival of that inevitable daywhen the research that can be conducted with these tools is no longer important;and, second, that one should value ideas and insights that are to the point, thoughperhaps premature and vague and not productive of research at a particular stageof technique and understanding. With the beneﬁts of hindsight, I think we cannow see clearly that the disparagement and neglect of a rich tradition provedin the long run to be quite harmful to the study of language. Furthermore, thisdisparagement and neglect were surely unnecessary. Perhaps it would have beenpsychologically difﬁcult, but there is no reason in principle why the successfulexploitation of the structuralist approach in historical and descriptive study
could not have been coupled with a clear recognition of its essential limitationsand its ultimate inadequacy, in comparison with the tradition it temporarily, andquite justiﬁably, displaced. Here, I think, lies a lesson that may be valuable forthe future study of language and mind.
Toconclude, I think there have been two really productive traditions of
research that have unquestionable relevance to anyone concerned with the studyof language today. One is the tradition of philosophical grammar that ﬂourishedfrom the seventeenth century through romanticism; the second is the traditionthat I have rather misleadingly been referring to as “structuralist,” which hasdominated research for the past century, at least until the early 1950s. I havedwelt on the achievements of the former because of their unfamiliarity as well astheir contemporary relevance. Structural linguistics has enormously broadened

--- Page 39 ---

20 Language and Mind
the scope of information available to us and has extended immeasurably the
reliability of such data. It has shown that there are structural relations in languagethat can be studied abstractly. It has raised the precision of discourse aboutlanguage to entirely new levels. But I think that its major contribution mayprove to be one for which, paradoxically, it has been very severely criticized.Irefer to the careful and serious attempt to construct “discovery procedures,”
those techniques of segmentation and classiﬁcation to which Saussure referred.This attempt was a failur e–I think that is now generally understood. It was a
failure because such techniques are at best limited to the phenomena of surface
structure and cannot, therefore, reveal the mechanisms that underlie the creativeaspect of language use and the expression of semantic content. But what remainsof fundamental importance is that this attempt was directed to the basic questionin the study of language, which was for the ﬁrst time formulated in a clear andintelligible way. The problem raised is that of specifying the mechanisms thatoperate on the data of sense and produce knowledge of language – linguisticcompetence. It is obvious that such mechanisms exist. Children do learn a ﬁrstlanguage; the language that they learn is, in the traditional sense, an “institutedlanguage,” not an innately speciﬁed system. The answer that was proposed instructural linguistic methodology has been shown to be incorrect, but this is ofsmall importance when compared with the fact that the problem itself has nowreceived a clear formulation.
Whitehead once described the mentality of modern science as having been
forged through “the union of passionate interest in the detailed facts with equaldevotion to abstract generalization.” It is roughly accurate to describe modernlinguistics as passionately interested in detailed fact, and philosophical grammaras equally devoted to abstract generalization. It seems to me that the time hasarrived to unite these two major currents and to develop a synthesis that willdraw from their respective achievements. In the next two lectures, I will tryto illustrate how the tradition of philosophical grammar can be reconstitutedand turned to new and challenging problems and how one can, ﬁnally, returnin a productive way to the basic questions and concerns that gave rise to thistradition.

--- Page 40 ---

2 Linguistic contributions to the study of mind:
present
One difﬁculty in the psychological sciences lies in the familiarity of the phe-
nomena with which they deal. A certain intellectual effort is required to seehow such phenomena can pose serious problems or call for intricate explana-tory theories. One is inclined to take them for granted as necessary or somehow“natural.”
The effects of this familiarity of phenomena have often been discussed.
Wolfgang K¨ ohler, for example, has suggested that psychologists do not open up
“entirely new territories” in the manner of the natural sciences, “simply becauseman was acquainted with practically all territories of mental life a long timebefore the founding of scientiﬁc psychology ...because at the very begin-
ning of their work there were no entirely unknown mental facts left which theycould have discovered.”
1The most elementary discoveries of classical physics
have a certain shock value – man has no intuition about elliptical orbits or thegravitational constant. But “mental facts” of even a much deeper sort cannotbe “discovered” by the psychologist, because they are a matter of intuitiveacquaintance and, once pointed out, are obvious.
There is also a more subtle effect. Phenomena can be so familiar that we
really do not see them at all, a matter that has been much discussed by literarytheorists and philosophers. For example, Viktor Shklovskij in the early 1920sdeveloped the idea that the function of poetic art is that of “making strange”the object depicted. “People living at the seashore grow so accustomed to themurmur of the waves that they never hear it. By the same token, we scarcelyever hear the words which we utter . . . We look at each other, but we do not see
each other any more. Our perception of the world has withered away; what hasremained is mere recognition.” Thus, the goal of the artist is to transfer whatis depicted to the “sphere of new perception”; as an example, Shklovskij citesastory by Tolstoy in which social customs and institutions are “made strange”
by the device of presenting them from the viewpoint of a narrator who happensto be a horse.
2
1W. K ¨ ohler, Dynamics in Psychology (New York: Liveright, 1940).
2See V . Ehrlich, Russian Formalism ,2nd rev. edn. (New York: Humanities, 1965), pp. 176–77.
21

--- Page 41 ---

22 Language and Mind
The observation that “we look at each other, but we do not see each other
any more” has perhaps itself achieved the status of “words which we utter but
scarcely ever hear.” But familiarity, in this case as well, should not obscure theimportance of the insight.
Wittgenstein makes a similar observation, pointing out that “the aspects of
things that are most important for us are hidden because of their simplicity andfamiliarity (one is unable to notice something – because it is always before
one’s eyes).”
3He sets himself to “supplying . . . remarks on the natural history
of human beings: we are not contributing curiosities however, but observationswhich no one has doubted, but which have escaped remark only because theyare always before our eyes.”
4
Less noticed is the fact that we also lose sight of the need for explanation
when phenomena are too familiar and “obvious.” We tend too easily to assumethat explanations must be transparent and close to the surface. The greatestdefect of classical philosophy of mind, both rationalist and empiricist, seems tome to be its unquestioned assumption that the properties and content of the mindare accessible to introspection; it is surprising to see how rarely this assumptionhas been challenged, insofar as the organization and function of the intellectualfaculties are concerned, even with the Freudian revolution. Correspondingly,
the far-reaching studies of language that were carried out under the inﬂuence ofCartesian rationalism suffered from a failure to appreciate either the abstractnessof those structures that are “present to the mind” when an utterance is producedor understood, or the length and complexity of the chain of operations that relatethe mental structures expressing the semantic content of the utterance to thephysical realization.
Asimilar defect mars the study of language and mind in the modern period.
It seems to me that the essential weakness in the structuralist and behavioristapproaches to these topics is the faith in the shallowness of explanations, thebelief that the mind must be simpler in its structure than any known physicalorgan and that the most primitive of assumptions must be adequate to explainwhatever phenomena can be observed. Thus, it is taken for granted withoutargument or evidence (or is presented as true by deﬁnition) that a languageis a “habit structure” or a network of associative connections, or that knowl-edge of language is merely a matter of “knowing how,” a skill expressibleas a system of dispositions to respond. Accordingly, knowledge of languagemust develop slowly through repetition and training, its apparent complexityresulting from the proliferation of very simple elements rather than from deeperprinciples of mental organization that may be as inaccessible to introspection
3Ludwig Wittgenstein, Philosophical Investigations (New York: Oxford University Press, 1953),
Section 129.
4Ibid.,Section 415.

--- Page 42 ---

Linguistic contributions: present 23
as the mechanisms of digestion or coordinated movement. Although there is
nothing inherently unreasonable in an attempt to account for knowledge anduse of language in these terms, it also has no particular plausibility or a priorijustiﬁcation. There is no reason to react with uneasiness or disbelief if study ofthe knowledge of language and use of this knowledge should lead in an entirelydifferent direction.
Ithink that in order to achieve progress in the study of language and human
cognitive faculties in general it is necessary ﬁrst to establish “psychic distance”from the “mental facts” to which K¨ ohler referred, and then to explore the
possibilities for developing explanatory theories, whatever they may suggestwith regard to the complexity and abstractness of the underlying mechanisms.Wemust recognize that even the most familiar phenomena require explana-
tion and that we have no privileged access to the underlying mechanisms, nomore so than in physiology or physics. Only the most preliminary and tenta-tive hypotheses can be offered concerning the nature of language, its use, andits acquisition. As native speakers, we have a vast amount of data available tous. For just this reason it is easy to fall into the trap of believing that there isnothing to be explained, that whatever organizing principles and underlyingmechanisms may exist must be “given” as the data is given. Nothing could befurther from the truth, and an attempt to characterize precisely the system ofrules we have mastered that enables us to understand new sentences and produceanewsentence on an appropriate occasion will quickly dispel any dogmatism
on this matter. The search for explanatory theories must begin with an attemptto determine these systems of rules and to reveal the principles that governthem.
The person who has acquired knowledge of a language has internalized a
system of rules that relate sound and meaning in a particular way. The linguistconstructing a grammar of a language is in effect proposing a hypothesis con-cerning this internalized system. The linguist’s hypothesis, if presented withsufﬁcient explicitness and precision, will have certain empirical consequenceswith regard to the form of utterances and their interpretations by the nativespeaker. Evidently, knowledge of language – the internalized system of rules –is only one of the many factors that determine how an utterance will be usedor understood in a particular situation. The linguist who is trying to determinewhat constitutes knowledge of a language – to construct a correct grammar –is studying one fundamental factor that is involved in performance, but not theonly one. This idealization must be kept in mind when one is considering theproblem of conﬁrmation of grammars on the basis of empirical evidence. Thereis no reason why one should not also study the interaction of several factorsinvolved in complex mental acts and underlying actual performance, but such astudy is not likely to proceed very far unless the separate factors are themselvesfairly well understood.

--- Page 43 ---

24 Language and Mind
In a good sense, the grammar proposed by the linguist is an explanatory
theory; it suggests an explanation for the fact that (under the idealization men-
tioned) a speaker of the language in question will perceive, interpret, form, oruse an utterance in certain ways and not in other ways. One can also searchfor explanatory theories of a deeper sort. The native speaker has acquired agrammar on the basis of very restricted and degenerate evidence; the grammarhas empirical consequences that extend far beyond the evidence. At one level,the phenomena with which the grammar deals are explained by the rules ofthe grammar itself and the interaction of these rules. At a deeper level, thesesame phenomena are explained by the principles that determine the selection ofthe grammar on the basis of the restricted and degenerate evidence available tothe person who has acquired knowledge of the language, who has constructedfor himself this particular grammar. The principles that determine the form ofgrammar and that select a grammar of the appropriate form on the basis ofcertain data constitute a subject that might, following a traditional usage, betermed “universal grammar.” The study of universal grammar, so understood,is a study of the nature of human intellectual capacities. It tries to formulate thenecessary and sufﬁcient conditions that a system must meet to qualify as a poten-tial human language, conditions that are not accidentally true of the existinghuman languages, but that are rather rooted in the human “language capacity,”and thus constitute the innate organization that determines what counts as lin-guistic experience and what knowledge of language arises on the basis of thisexperience. Universal grammar, then, constitutes an explanatory theory of a
much deeper sort than particular grammar, although the particular grammar ofalanguage can also be regarded as an explanatory theory.
5
In practice, the linguist is always involved in the study of both universal and
particular grammar. When he constructs a descriptive, particular grammar inone way rather than another on the basis of what evidence he has available, he isguided, consciously or not, by certain assumptions as to the form of grammar,and these assumptions belong to the theory of universal grammar. Conversely,his formulation of principles of universal grammar must be justiﬁed by the studyof their consequences when applied in particular grammars. Thus, at severallevels the linguist is involved in the construction of explanatory theories, andat each level there is a clear psychological interpretation for his theoreticaland descriptive work. At the level of particular grammar, he is attempting tocharacterize knowledge of a language, a certain cognitive system that has beendeveloped – unconsciously, of course – by the normal speaker–hearer. At thelevel of universal grammar, he is trying to establish certain general properties
5Tobring out this difference in depth of explanation, I have suggested in my Current Issues in
Linguistic Theory (New York: Humanities, 1965) that the term “level of descriptive adequacy”
might be used for the study of the relation between grammars and data and the term “level of
explanatory adequacy” for the relation between a theory of universal grammar and these data.

--- Page 44 ---

Linguistic contributions: present 25
of human intelligence. Linguistics, so characterized, is simply the subﬁeld of
psychology that deals with these aspects of mind.
Iwill try to give some indication of the kind of work now in progress that aims,
on the one hand, to determine the systems of rules that constitute knowledge ofalanguage, and on the other, to reveal the principles that govern these systems.
Obviously, any conclusions that can be reached today regarding particular oruniversal grammar must be quite tentative and restricted in their coverage. Andin a brief sketch such as this only the roughest outlines can be indicated. To tryto give something of the ﬂavor of what is being done today I will concentrateon problems that are current in that they can be formulated with some clarityand studied, though they still resist solution.
As I indicated in the ﬁrst lecture, I believe that the most appropriate general
framework for the study of problems of language and mind is the system of ideasdeveloped as part of the rationalist psychology of the seventeenth and eighteenthcenturies, elaborated in important respects by the romantics and then largelyforgotten as attention shifted to other matters. According to this traditionalconception, a system of propositions expressing the meaning of a sentenceis produced in the mind as the sentence is realized as a physical signal, thetwo being related by certain formal operations that, in current terminology, wemay call grammatical transformations. Continuing with current terminology,
we can thus distinguish the surface structure of the sentence, the organization
into categories and phrases that is directly associated with the physical signal,from the underlying deep structure ,also a system of categories and phrases,
butwith a more abstract character. Thus, the surface structure of the sentence
“Awise man is honest” might analyze it into the subject “a wise man” and the
predicate “is honest.” The deep structure, however, will be rather different. Itwill, in particular, extract from the complex idea that constitutes the subjectof the surface structure an underlying proposition with the subject “man” andthe predicate “be wise.” In fact, the deep structure, in the traditional view, is asystem of two propositions, neither of which is asserted, but which interrelatein such a way as to express the meaning of the sentence “A wise man is honest.”Wemight represent the deep structure in this sample case by formula 1,and the
surface structure by formula 2,where paired brackets are labeled to show the
category of phrase that they bound. (Many details are omitted.)
1
2


--- Page 45 ---

26 Language and Mind
An alternative and equivalent notation, widely used, expresses the labeled
bracketing of 1and2in tree form, as 1/primeand2/primerespectively:
1/prime
2/prime
If we understand the relation “subject-of” to hold between a phrase of the
category noun phrase (NP) and the sentence (S) that directly dominates it,and the relation “predicate-of” to hold between a phrase of the category verbphrase (VP) and the sentence that directly dominates it, then structures 1and2
(equivalently, 1
/primeand2/prime)specify the grammatical functions of subject and pred-
icate in the intended way. The grammatical functions of the deep structure ( 1)
play a central role in determining the meaning of the sentence. The phrase struc-ture indicated in 2,onthe other hand, is closely related to its phonetic shape –
speciﬁcally, it determines the intonation contour of the utterance represented.
Knowledge of a language involves the ability to assign deep and surface
structures to an inﬁnite range of sentences, to relate these structures appropri-ately, and to assign a semantic interpretation and a phonetic interpretation tothe paired deep and surface structures. This outline of the nature of grammarseems to be quite accurate as a ﬁrst approximation to the characterization of“knowledge of a language.”
How are the deep and surface structures related? Clearly, in the simple exam-
ple given, we can form the surface structure from the deep structure by per-forming such operations as the following:
3a . assign the marker wh-to the most deeply embedded NP, “man”
b.replace the NP so marked by “who”
c.delete “who is”
d.invert “man” and “wise.”
Applying just operations a and b, we derive the structure underlying the sen-
tence “a man who is wise is honest,” which is one possible realization of the

--- Page 46 ---

Linguistic contributions: present 27
underlying structure ( 1). If, furthermore, we apply the operation c (deriving “a
man wise is honest”), we must, in English, also apply the subsidiary operation d,
deriving the surface structure ( 2), which can then be phonetically interpreted.
If this approach is correct in general, then a person who knows a speciﬁc
language has control of a grammar that generates (that is, characterizes) the
inﬁnite set of potential deep structures, maps them onto associated surface struc-tures, and determines the semantic and phonetic interpretations of these abstractobjects.
6From the information now available, it seems accurate to propose that
the surface structure determines the phonetic interpretation completely and thatthe deep structure expresses those grammatical functions that play a role indetermining the semantic interpretation, although certain aspects of the surfacestructure may also participate in determining the meaning of the sentence inways that I will not discuss here. A grammar of this sort will therefore deﬁne
acertain inﬁnite correlation of sound and meaning. It constitutes a ﬁrst step
toward explaining how a person can understand an arbitrary sentence of hislanguage.
Even this artiﬁcially simple example serves to illustrate some properties of
grammars that appear to be general. An inﬁnite class of deep structures muchlike1can be generated by very simple rules that express a few rudimentary
grammatical functions, if we assign to these rules a recursive property – in par-ticular, one that allows them to embed structures of the form [s . . .]s within otherstructures. Grammatical transformations will then iterate to form, ultimately, asurface structure that may be quite remote from the underlying deep structure.The deep structure may be highly abstract; it may have no close point-by-pointcorrelation to the phonetic realization. Knowledge of a language – “linguisticcompetence,” in the technical sense of this term discussed brieﬂy in the ﬁrstlecture – involves a mastery of these grammatical processes.
With just this much of a framework, we can begin to formulate some of the
problems that call for analysis and explanation. One major problem is posed bythe fact that the surface structure generally gives very little indication in itselfof the meaning of the sentence. There are, for example, numerous sentences
6Foradetailed development of this point of view, see J. Katz and P. Postal, An Integrated Theory
of Linguistic Descriptions (Cambridge, Mass.: MIT Press, 1964) and my Aspects of the Theory
of Syntax (Cambridge, Mass.: MIT Press, 1965). See also Peter S. Rosenbaum, The Grammar of
English Predicate Complement Constructions (Cambridge, Mass.: MIT Press, 1967). These
contain references to earlier work that they extend and modify. There has been a great deal
of work in the past few years extending and modifying this general approach still further andexploring alternatives. At present the ﬁeld is in considerable ferment, and it will probably be
some time before the dust begins to settle and a number of outstanding issues are even tentativelyresolved. Current work is too extensive for detailed reference to be called for in a sketch suchas this. Some idea of its scope and general directions can be obtained from collections such asR. Jacobs and P. S. Rosenbaum, eds., Readings in English Transformational Grammar (Waltham,
Mass.: Ginn and Company, 1970).

--- Page 47 ---

28 Language and Mind
that are ambiguous in some way that is not indicated by the surface structure.
Consider sentence 4:
4 Idisapprove of John’s drinking.
This sentence can refer either to the fact of John’s drinking or to its character.The ambiguity is resolved, in different ways, in sentences 5and6:
5 Idisapprove of John’s drinking the beer.
6 Idisapprove of John’s excessive drinking.
It is clear that grammatical processes are involved. Notice that we cannot simul-taneously extend 4in both of the ways illustrated in 5and6;that would give
us7:
7 *I disapprove of John’s excessive drinking the beer.
7
Our internalized grammar assigns two different abstract structures to 4,one
of which is related to the structure that underlies 5,the other to the structure
that underlies 6.Buitisatthe level of deep structure that the distinction is
represented; it is obliterated by the transformations that map the deep structuresonto the surface form associated with 4.
The processes that are involved in examples 4,5,and6are quite common
in English. Thus, the sentence “I disapprove of John’s cooking” may implyeither that I think his wife should cook or that I think he uses too much garlic,for example. Again, the ambiguity is resolved if we extend the sentence in themanner indicated in 5and6.
The fact that 7is deviant requires explanation. The explanation in this case
would be provided, at the level of particular grammar, by formulation of the
grammatical rules that assign alternative deep structures and that in each casepermit one but not the other of the extensions to 5or6.Wewould then explain
the deviance of 7and the ambiguity of 4by attributing this system of rules to
the person who knows the language, as one aspect of his knowledge. We might,of course, try to move to a deeper level of explanation, asking how it is thatthe person has internalized these rules instead of others that would determine adifferent sound–meaning correlation and a different class of generated surfacestructures (including, perhaps, 7). This is a problem of universal grammar, in
the sense described earlier. Using the terminology of note 5,the discussion at
the level of particular grammar would be one of descriptive adequacy, and at thelevel of universal grammar it would be one of explanatory adequacy.
Notice that the internalized rules of English grammar have still further conse-
quences in a case like the one just discussed. There are transformations of greatgenerality that permit or require the deletion of repeated elements, in whole
7Iuse the asterisk in the conventional way, to indicate a sentence that deviates in some respect
from grammatical rule.

--- Page 48 ---

Linguistic contributions: present 29
or in part, under well-deﬁned conditions. Applied to structure 8,these rules
derive 9.8
8 Idon’t like John’s cooking any more than Bill’s cooking.
9 Idon’t like John’s cooking any more than Bill’s.
Sentence 9is ambiguous. It can mean either that I don’t like the fact that John
cooks any more than I like the fact that Bill cooks, or that I don’t like the
quality of John’s cooking any more than I like the quality of Bill’s cooking.9
However, it cannot mean that I don’t like the quality of John’s cooking anymore than I like the fact that Bill cooks, or conversely, with “fact” and “quality”interchanged. That is, in the underlying structure ( 8)wemust understand the
ambiguous phrases “John’s cooking” and “Bill’s cooking” in the same way ifwe are to be able to delete “cooking.” It seems reasonable to assume that whatis involved is some general condition on the applicability of deletion operationssuch as the one that gives 9from 8,arather abstract condition that takes into
account not only the structure to which the operation applies but also the historyof derivation of this structure.
Other examples can be found where a similar principle seems to be at work.
Thus, consider sentence 10,which is presumably derived from either 11or12
and is therefore ambiguous:
10
10 Iknow a taller man than Bill.
11 Iknow a taller man than Bill does.
12 Iknow a taller man than Bill is.
8Henceforth I shall generally delete brackets in giving a deep, surface, or intermediate structure,
where this will not lead to confusion. One should think of 8and9as each having a full labeled
bracketing associated with it. Notice that 8is not, of course, a deep structure, but rather the
result of applying transformations to a more primitive abstract object.
9There may also be other interpretations, based on other ambiguities in the structure “John’scooking” – speciﬁcally, the cannibalistic interpretation and the interpretation of “cooking” as“that which is cooked.”
10Ishould emphasize that when I speak of a sentence as derived by transformation from another
sentence, I am speaking loosely and inaccurately. What I should say is that the structure asso-ciated with the ﬁrst sentence is derived from the structure underlying the second. Thus, in thecase now being discussed, it is the surface structure of 10that is derived, on one analysis,
from the abstract structure which, were it to undergo a different transformational development,would be converted into the surface structure of 11.That sentences are not derived from other
sentences but rather from the structures underlying them has been explicitly assumed sincethe earliest work in transformational generative grammar about ﬁfteen years ago, but informalstatements such as those in the text here have misled many readers and have led to a gooddeal of confusion in the literature. Adding to the confusion, perhaps, is the fact that a very dif-ferent theory of transformational relations developed by Zellig Harris, Henry Hiz, and others,does in fact regard the transformational operations as applied to sentences. See, for example,Z. S. Harris, “Co-occurrence and Transformation in Linguistic Structure,” in Language ,Vol. 33,
No. 3, 1957, pp. 283-340, and many later publications. For me, and most other speakers, sen-tence 12is deviant. Nevertheless, the association structure that underlies 10under one analysis
must be postulated, perhaps deriving from the structure associated with “I know a man who istaller than Bill is.”

--- Page 49 ---

30 Language and Mind
It seems clear that the ambiguity of 10is not represented in the surface structure;
the deletion of “does” in 11leaves exactly the same structure as the deletion of
“is” in 12.But now consider sentence 13.
13 Iknow a taller man than Bill, and so does John.
This sentence, like 9,istwo-ways ambiguous rather than four-ways ambiguous.
It can have the meaning of either 14or15,b u t not16or17:11
14 Iknow a taller man than Bill does and John knows a taller man than
Bill does.
15 Iknow a taller man than Bill is and John knows a taller man than Bill
is.
16 Iknow a taller man than Bill is and John knows a taller man than Bill
does.
17 Iknow a taller man than Bill does and John knows a taller man than
Bill is.
But now a problem arises, as we can see by considering more carefully the
derivation of 13.Let us refer to the deletion operation that gives 10from 11as
T1,and to the deletion operation that gives 10from 12as T 2.Ifweapply T 1to
each of the conjuncts of 14,wederive 18:
18 Iknow a taller man than Bill and John knows a taller man than Bill.
Application of T 2to each of the conjuncts of 15will also yield 18.But appli-
cation of T 1to one conjunct and T 2to the other conjunct in 16will also give
18,aswill the same procedure (in the opposite order) when applied to the two
conjuncts of 17.Thus, 18can be derived by application of T 1and T 2to any of
the four underlying forms, 14,15,16,o r17.The structure of 18itself does not
indicate which of these is the underlying form; the distinction has been elim-inated by the deletion operations T
1and T 2.But now consider the operation
T3,which derives “I saw Bill and so did John” from “I saw Bill and John saw
Bill.” Applying T 3to18,wederive 13.However, we have noted that 13can
11It also cannot have the meaning “I know a taller man than Bill and John likes ice cream.”
Hence, if deep structure determines meaning (insofar as grammatical relations are involved),it must be that something like 14or15is the immediately underlying structure for 13.I ti sa
general property of deletion operations that some sort of recoverability is involved, a nontrivialmatter with interesting empirical consequences. For some discussion, see my Current Issues ,
Section 2.2, and Aspects ,Section 4.2.2. The problem posed by such examples as 9and13was
pointed out to me by John Ross. The ﬁrst reference to the possibility that history of derivationmay play a role in determining applicability of transformations appears in R. B. Lees, The
Grammar of English Nominalizations (New York: Humanities, 1960), p. 76, in connection with
his discussion – also the ﬁrst – of the problem of identity of constituent structure as a factor indetermining applicability of transformations.

--- Page 50 ---

Linguistic contributions: present 31
have the interpretation of 14or15,b u t not of 16or17.Thus we can see that
T3can apply to 18only if either 14or15,b u t not16or17,was the structure
underlying 18in the given derivations of 18.However, this information is not
represented in 18itself, as we have just observed. Therefore, to apply T 3to
18we must know something about the history of derivation of 18–wemust
have information that would not be contained in the labeled bracketing of 18
itself. What we must know, in fact, is that the two conjuncts of 18derive from
underlying structures in which the same element was deleted.12It appears, once
again, that some general condition on applicability of deletion transformations
must be involved, a principle that somehow brings into consideration the historyof derivation of deleted strings, perhaps certain properties of the deep structurefrom which they ultimately derive.
Tosee how complex the problem is, consider such sentences as “John’s intel-
ligence, which is his most remarkable quality, exceeds his prudence” or “Thebook, which weighs ﬁve pounds, was written by John.” Presumably, the relativepronoun in the embedded appositive clause replaces a deleted noun phrase, andthe condition on deletion that we are discussing implies that this noun phraseshould be identical to the antecedent noun phrase “John’s intelligence” or “thebook” in the underlying structure of the appositive clause. In each case, how-ever, it can be argued that there is a difference between the antecedent and the
noun phrase of the appositive clause. Thus, in the ﬁrst case, we are referringto the degree of John’s intelligence in the main clause but to the quality of hisintelligence in the embedded clause; and in the second case we are referring tothe book as an abstract object in the main clause but as a concrete physical objectin the embedded clause; one might expect these differences to be represented indeep structure, thus contradicting the principle to which we seem to be drivenby the earlier examples. I will not go on with this discussion here, but the readerwill discover, if he pursues the matter, that the problem is compounded when aricher class of cases is considered.
In fact, the correct principle is unknown in such cases as these, although some
of the conditions it must meet are clear. The problem posed by these examplesis a quite typical one. Attention to linguistic fact reveals certain properties ofsentences, relating to their sound, their meaning, their deviance, and so on.Evidently no explanation for these facts will be forthcoming so long as werestrict ourselves to vague talk about “habits” and “skills” and “dispositionsto respond,” or about the formation of sentences “by analogy.” We do nothave the “habit” of understanding sentences 4,9,and13in a certain way: it
is unlikely that the reader has ever encountered sentences closely resemblingthese, but he understands them in a highly speciﬁc way nevertheless. To refer to
12If18itself is only two-ways ambiguous, a problem in fact arises at an even earlier point. The
unnaturalness of 18makes it difﬁcult to determine this with any conﬁdence.

--- Page 51 ---

32 Language and Mind
the processes involved as “analogy” is simply to give a name to what remains
amystery. To explain such phenomena we must discover the rules that relate
sound and meaning in the language in question – the grammar that has beeninternalized by the person who knows the language – and the general principlesthat determine the organization and function of these rules.
The misleading and inadequate character of surface structure becomes evi-
dent as soon as even the most simple patterns are studied. Consider, for example,sentence 19–again, an artiﬁcially simple example:
19 John was persuaded to leave.
The deep structure underlying this sentence must indicate that the subject–predicate relation holds in an underlying proposition of the form of 20(assuming
grammatical functions to be represented in the same manner suggested earlier),and that the verb–object relation holds in an underlying proposition of the formof21:
20/bracketleftbig
S/bracketleftbig
NPJohn/bracketrightbig
NP/bracketrightbig
VPleave/bracketrightbig
VP/bracketrightbig
S
21/bracketleftbig
S/bracketleftbig
NP.../bracketrightbig
NP/bracketleftbig
VPpersuade/bracketleftbig
NPJohn/bracketrightbig
NP/bracketrightbig
VP/bracketrightbig
S
Thus, “John” is understood to be the subject of “leave” and the object of “per-suade” in 19,and these facts are properly expressed in the deep structure under-
lying 19if this deep structure embodies the propositions informally represented
as20and21.Although the deep structure must be constituted of such propo-
sitions, if the approach loosely outlined earlier is correct, there is no trace ofthem in the surface structure of the utterance. The various transformations thatproduce 19have thoroughly obliterated the system of grammatical relations
and functions that determine the meaning of the sentence.
The point becomes still more obvious if we take note of the variety of sen-
tences that seem superﬁcially to resemble 19,but that differ widely in the ways
they are understood and the formal operations that apply to them. Suppose that“persuaded” in 19is replaced by one of the following words:
13
22 expected, hired, tired, pleased, happy, lucky, eager, certain, easy
With “expected” replacing “persuaded,” the sentence can mean roughly that
the fact of John’s leaving was expected; but it is impossible to speak of the factof John’s leaving being persuaded. With “hired,” the sentence has an entirelydifferent meaning, roughly that the purpose of hiring John was so that he wouldleave – an interpretation that becomes more natural if we replace “leave” by aphrase like “ﬁx the roof.” When “tired” is substituted, we derive a nonsentence;
13See R. B. Lees, “A Multiply Ambiguous Adjectival Construction in English,” in Language ,
Vol. 36, No. 2, 1960, pp. 207-21, for a discussion of such structures.

--- Page 52 ---

Linguistic contributions: present 33
it becomes a sentence if “too tired” replaces “persuaded,” the sentence now
implying that John didn’t leave. The word “pleased” is still different. In thiscase we can have “too pleased,” implying that John didn’t leave, but we canalso extend the sentence to “John was too pleased to leave to suit me,” whichis impossible in the earlier cases. “Happy” is rather like “pleased,” though onemight argue that the verb–object relation holds between “please” and “John.”The sentence “John was lucky to leave” is interpreted in still another way. Itmeans, roughly, that John was lucky in that he left, an interpretation that isimpossible in the earlier cases; furthermore, we can construct such sentences as“John was a lucky fellow to leave (so early),” but none of the earlier examplescan replace “lucky” in such sentences. “John was eager to leave” differs fromthe earlier cases in that it is formally associated with such expressions as “Johnwaseager for Bill to leave” and “John’s eagerness (for Bill) to leave.” “John
wascertain to leave” can be paraphrased as “it was certain that John would
leave”; of the other examples, only “expected” is subject to this interpretation,but“expected” obviously differs from “certain” in numerous other respects –
for example, it appears in a sentence such as “They expected John to leave.”The word “easy” is of course entirely different; in this and only this case theverb–object relation holds between “leave” and “John.”
It is clear, in short, that the surface structure is often misleading and uninfor-
mative and that our knowledge of language involves properties of a much moreabstract nature, not indicated directly in the surface structure. Furthermore,even such artiﬁcially simple examples as these show how hopeless it would be
to try to account for linguistic competence in terms of “habits,” “dispositions,”“knowing how,” and other concepts associated with the study of behavior, asthis study has been circumscribed, quite without warrant, in recent years.
Even at the level of sound structure, there is evidence that abstract repre-
sentations are formed and manipulated in the mental operations involved inlanguage use. We have a more detailed understanding of the nature of linguisticrepresentation and the intricate conditions on rule application in this domainthan in any other. The work of the past few years on sound structure seemsto me to provide substantial evidence in support of the view that the form ofparticular grammars is determined, in highly signiﬁcant ways, by a restrictiveschematism that speciﬁes the choice of relevant phonetic properties, the kindsof rules that can relate surface structure to phonetic representation, and the con-ditions on organization and application of these rules. It thus relates closely tothe general topics discussed in the ﬁrst lecture, topics that I will take up againbelow in considering the question of how this restrictive, universal schematismcomes to be used in language acquisition. Furthermore, these investigations ofsound structure, insofar as they support the conclusion that abstract phonolog-ical structures are manipulated by tightly organized and intricate systems ofrules, are relevant to the very interesting problem of developing empirically

--- Page 53 ---

34 Language and Mind
adequate models of performance. They suggest that all current approaches to
problems of perception and organization of behavior suffer from a failure toattribute sufﬁcient depth and complexity to the mental processes that must berepresented in any model that attempts to come to grips with the empirical phe-nomena. Space does not permit a detailed development of these topics, eitherwith respect to the matter of phonological structure or with respect to its poten-tial signiﬁcance for cognitive psychology.
14However, one simple illustrative
example, which is quite typical, may give some idea of the nature of the evidence
that is available and the conclusions to which it points.
Recall that the syntactic rules of the language generate an inﬁnite set of
surface structures, each of which is a labeled bracketing of a string of minimalelements, such as 2,inwhich we may take the minimal elements to be the
items a,wise,man,is,honest. Each of these items can itself be represented as a
string of segments, for example man as the string of segments /m/, /æ/, /n/. Each
of these segments may be regarded in turn as a set of speciﬁed features; thus,/m/ stands for the feature complex [ +consonantal], [– vocalic], [ +nasal], and
so on. The segmental constitution of an item will be given by a lexical entry –acharacterization of the inherent phonetic, semantic, and syntactic properties
of the items in question. The lexicon of the language is the set of such lexicalentries, with, perhaps, additional structure that need not concern us here. Weare concerned now only with the phonetic properties of the lexical entry.
The lexical entry of an item must specify just those properties that are idiosyn-
cratic, that are not determined by linguistic rule. For example, the lexical entryforman must indicate that its second segment is a low front vowel, but the degree
of tenseness, diphthongization, nasalization, and so on, of this vowel need notbe indicated in the lexical entry, since these are a matter of general rule, in partparticular to various English dialects, in part common to all English dialects, inpart a matter of universal phonology. Similarly, the lexical entry for man must
indicate that it has an irregular plural, with the vowel shifting from low to mid.The segments of the lexical entry are abstract in the sense that the phonologicalrules of the language will frequently modify and elaborate them in a varietyof ways; hence there need not be, in general, a simple point-by-point corre-spondence between the lexical entry and the actual phonetic representation. Indiscussing examples, I will use phonetic symbols in the usual way, each beingregarded as a complex of a certain set of features. I will use the symbol / / to
14Fordiscussion of these topics, see my article “Some General Properties of Phonological Rules,”
inLanguage ,Vol. 47, No. 1, 1967. For a much fuller and more detailed discussion of phonolog-
ical theory and its application to English, with examples drawn from many languages and some
discussion of the history of the English sound system as well, see N. Chomsky and M. Halle,The Sound Pattern of English (New York: Harper & Row, 1968). The example in the text is
discussed in detail, in the context of a more general framework of rules and principles, inChapter 4, Section 4, of The Sound Pattern of English. See P. Postal, Aspects of Phonological
Theory (New York: Harper & Row, 1968), for a general development of many related topics,
along with a critical analysis of alternative approaches to the study of sound structure.

--- Page 54 ---

Linguistic contributions: present 35
enclose lexical representations, and the symbol [ ] to enclose all representations
derived from lexical representations by application of phonological rules,including, in particular, the ﬁnal phonetic representation derived by applica-tion of the full set of phonological rules.
Consider ﬁrst such words as sign-signify ,paradigm-paradigmatic ,and so
on. For reasons that will become clearer as we proceed, it is the derived form,in this case, that is most closely related to the underlying abstract lexical repre-sentation. Suppose, then, that we tentatively assign to the stem in these formsthe lexical representation /sign/ and /pærædigm/ where the symbols have theirconventional phonetic interpretation. Thus, the underlying element /sign/ isrealized as phonetic [sign] before -ify.However, it is realized as phonetic [sayn]
in isolation. A similar observation holds of paradigm.
The forms of signandparadigm in isolation are determined by certain phono-
logical rules that, operating jointly, have the effect of converting the represen-tation /ig/ to [ay] when followed by a word-ﬁnal nasal. A careful analysis ofEnglish phonology shows that this process can be broken into a sequence ofsteps, including the following (the second and third of which, in fact, requirefurther analysis).
23 a. velar becomes continuant before word-ﬁnal nasal
b.vowel+velar continuant becomes tense vowel
c./¯i/ becomes [ay] (where / ¯i/ is the tense segment corresponding
to [i])
Applying these rules to underlying /sign/ in isolation, we derive ﬁrst [si /H9253n]
(where [ /H9253]isthe velar continuant) by 23a;then [s ¯in] by 23b;and ﬁnally [sayn]
by24c.
Rules 23aand23bare of little interest, but 23cis a part of a very general
system of rules of “vowel shift” that is quite central to English phonology.
There are, for example, strong reasons for supposing that the stem underlyingthe forms divine-divinity is /div ¯in/, where the segment / ¯i/ is weakened to [i]
before -ityand becomes [ay] by rule 23cin isolation. Similarly, reptile derives
from underlying /rept ¯il/, which becomes [reptayl] by 23cin isolation and [reptil]
before -ian,with the same shortening of vowel that takes place in divinity ;and
so on, in many other cases.
Consider next such words as ignite-ignition ,expedite-expeditious ,and
contrite-contrition. Just as reptile anddivine derive, by vowel shift, from /rept ¯il/
and /div ¯in/, so we can derive the ﬁrst member of each of these pairs from /ign ¯it/,
/exped ¯it/, and /contr ¯it/, respectively. The rule that applies to give the phonetic
realization is 23c,aspecial case of the general process of vowel shift. Evidently,
the second member of each pair is derived by such processes as 24and25:
24 Vowels become nontense before -ion,-ious ,-ian,-ity,and so on.
25 The segment /t/ followed by a high front vowel is realized as [ˇ s].

--- Page 55 ---

36 Language and Mind
The ﬁrst of these rules is the one that gives [divin] from /div ¯in/ in divinity
and [reptil] from /rept ¯il/ in reptilian. Similarly, it gives [ignit] from /ign ¯it/ in
ignition ,[expedit] from /exped ¯it/ in expeditious ,and [contrit] from /cont ¯it/ in
contrition. There is an obvious underlying generalization, namely that a vowel
becomes nontense before an unstressed vowel that is not in a word-ﬁnal syllable;
when properly formulated, this rule, along with vowel shift and a few others,constitutes the central portion of the English phonological system.
The second rule, 25,applies to the element /ti/ in /ignition/, /expeditious/,
and /contrition/, replacing it by [ˇ s] and giving, ﬁnally, the phonetic realizations
[igniˇ s
ən], [eksp ədiˇsəs], [k əntriˇsən], after the application of the rule that reduces
unstressed vowels to [ ə]. In short, the segments realized as [ayt] in ignite ,
expedite ,andcontrite are realized as [iˇ s] inignition ,expeditious ,andcontrition.
But now consider the words right-righteous ,phonetically [rayt]-[rayˇ cəs]. The
latter form appears to deviate from the regular pattern in two respects, namely invowel quality (we would expect [i] rather than [ay], by rule 24), and in the ﬁnal
consonant of the stem (we would expect [ˇ s] rather than [ˇ c], by rule 25). Ifright
were subject to the same processes as expedite ,wewould have [riˇ s
əs] rather
than [rayˇ cəs] as the phonetic realization, analogous to [eksp ədiˇsəs]. What is
the explanation for this double deviation?
Notice ﬁrst that rule 25is not quite exact; there are, in fact, other cases in
which /ti/ is realized as [ˇ c] rather than as [ˇ s], for example question [kwesˇ cən],
contrasted with direction [dərekˇsən]. A more accurate formulation of 25would
be26:
26 /t/ followed by a high front vowel is realized as [ˇ c] after a continuant
and as [ˇ s] elsewhere.
Returning to the form right ,wesee that the ﬁnal consonant would be correctly
determined as [ˇ c] rather than [ˇ s] if in the underlying representation there were
acontinuant preceding it – that is, if the underlying representation were /ri /H9272t/,
where /H9272is some continuant. The continuant /H9272must, furthermore, be distinct
from any of the continuants that actually appear phonetically in this position,namely the dental, labial, or palatal continuants in the unitalicized portion ofwrist,rift,o rwished.Wemay assume, then, that /H9272is the velar continuant /x/,
which does not, of course, appear phonetically in English. The underlying form,then, would be /rixt/.
Consider now the derivation of right. By rule 23b,the representation /rixt/
becomes [r ¯it]. By rule 23c,the representation / r ¯it / becomes [rayt], which is
the phonetic realization of right.
Consider next the derivation of righteous. Assuming that it has the same
afﬁx as expeditious andrepetitious ,wecan represent it lexically as /rixtious/
(I do not concern myself here with the proper representation for -ous). Let us

--- Page 56 ---

Linguistic contributions: present 37
suppose that the ordering of the rules so far discussed is the following: 23a,24,
26,23b,23c,anordering consistent with other relevant facts of English, given
certain simpliﬁcations for convenience of exposition. Rule 23ais inapplicable
and rule 24is vacuous, when applied to the underlying form /rixtious/. Turning
to rule 26,wesee that it gives the form [rixˇ cous]. Rule 23bnow applies, giving
[r¯iˇcous], and rule 23cgives [rayˇ cous], which becomes [rayˇ cəs] by reduction of
unstressed vowels. Thus by rules 26and23,which are independently motivated,
the underlying representation /rixt/ will be realized phonetically as [rayt] in
isolation and as [rayˇ c] in righteous ,exactly as required.
These facts strongly suggest that the underlying phonological representation
must be /rixt/ (in accord with the orthography and, of course, the history).Asequence of rules that must be in the grammar for other reasons gives the
alternation right-righteous. Therefore, this alternation is not at all exceptional,
butrather perfectly regular. Of course, the underlying representation is quite
abstract; it is connected with the superﬁcial phonetic shape of the signal onlyby a sequence of interpretive rules.
Putting the matter differently, suppose that a person knows English but does
not happen to have the vocabulary item righteous. Hearing this form for the ﬁrst
time, he must assimilate it to the system he has learned. If he were presentedwith the derived form [riˇ s
əs], he would, of course, take the underlying repre-
sentation to be exactly like that of expedite ,contrite ,and so on. But hearing
[rayˇcəs], he knows that this representation is impossible; although the conso-
nantal distinction [ˇ s]-[ˇc] might easily be missed under ordinary conditions of
language use, the vocalic distinction [i]-[ay] would surely be obvious. Know-ing the rules of English and hearing the vocalic element [ay] instead of [i], heknows that either the form is a unique exception or it contains a sequence /i/followed by velar and is subject to rule 26.The velar must be a continuant,
15
that is, /x/. But given that the velar is a continuant, it follows, if the form isregular (the null hypothesis, always), that the consonant must be [ˇ c], not [ˇ s],
by rule 26.Thus, the hearer should perceive [rayˇ c
əs] rather than [rayˇ səs], even
if the information as to the medial consonant is lacking in the received signal.Furthermore, the pressure to preserve regularity of alternations should act toblock the superﬁcial analogy to expedite-expeditious andignite-ignition ,and to
preserve [ˇ c] as the phonetic realization of underlying /t/, as long as [ay] appears
in place of expected [i], exactly as we observe to have occurred.
Idonot mean this as a literal step-by-step account of how the form is learned,
of course, but rather as a possible explanation of why the form resists a superﬁ-cial (and in fact incorrect) analogy and preserves its status. We can explain the
15If it were a noncontinuant, it would have to be unvoiced, that is, /k/, since there are no voiced–
voiceless consonant clusters in ﬁnal position, by general rule. But it cannot be /k/, since /k/
remains in this position (for example, “direct,” “evict,” and so on).

--- Page 57 ---

38 Language and Mind
perception and preservation in the grammar of the [ˇ c]-[ˇs] contrast in righteous-
expeditious on the basis of the perceived distinction between [ay] and [i] and the
knowledge of a certain system of rules. The explanation rests on the assump-
tion that the underlying representations are quite abstract, and the evidencecited suggests that this assumption is, in fact, correct.
Asingle example can hardly carry much conviction. A careful investigation
of sound structure, however, shows that there are a number of examples of thissort, and that, in general, highly abstract underlying structures are related tophonetic representations by a long sequence of rules, just as on the syntacticlevel abstract deep structures are in general related to surface structures byalong sequence of grammatical transformations. Assuming the existence of
abstract mental representations and interpretive operations of this sort, we canﬁnd a surprising degree of organization underlying what appears superﬁciallyto be a chaotic arrangement of data, and in certain cases we can also explainwhy linguistic expressions are heard, used, and understood in certain ways. Onecannot hope to determine either the underlying abstract forms or the processesthat relate them to signals by introspection; there is, furthermore, no reasonwhy one should ﬁnd this consequence in any way surprising.
The explanation sketched above is at the level of particular rather than uni-
versal grammar, as this distinction was formulated earlier. That is, we have
accounted for a certain phenomenon on the basis of the assumption that cer-tain rules appear in the internalized grammar, noting that these rules are, forthe most part, independently motivated. Of course, considerations of universalgrammar enter into this explanation insofar as they affect the choice of gram-mar on the basis of data. This interpenetration is unavoidable, as noted earlier.There are cases, however, where explicit principles of universal grammar entermore directly and clearly into a pattern of explanation. Thus, investigation ofsound systems reveals certain very general principles of organization, somequite remarkable, governing phonological rules (see references in note 14).
Forexample, it has been observed that certain phonological rules operate in a
cycle, in a manner determined by the surface structure. Recall that the surface
structure can be represented as a labeled bracketing of the utterance, such as 2.
In English, the very intricate phonological rules that determine stress contoursand vowel reduction apply to phrases bounded by paired brackets, in the sur-face structure, applying ﬁrst to a minimal phrase of this sort, then to the next
larger phrase, and so on, until the maximal domain of phonological processesis reached (in simple cases, the sentence itself). Thus, in the case of 2the rules
apply to the individual words (which, in a full description, would be assignedto categories and therefore bracketed), then to the phrases awise man andis
honest ,and ﬁnally to the whole sentence. A few simple rules will give quite
varied results, as the surface structures that determine their cyclic application
vary.

--- Page 58 ---

Linguistic contributions: present 39
Some simple effects of the principle of cyclic application are illustrated by
such forms as those of 27:
27 a. relaxation ,emendation ,elasticity ,conn ectivity
b.illustration ,demonstration ,devastation ,anecdotal
The unitalicized vowels are reduced to [ ə]i n27b,but they retain their orig-
inal quality in 27a.Insome cases, we can determine the original quality of
the reduced vowels of 27bfrom other derived forms (for example, illustrative ,
demonstrative ). The examples of 27adiffer from those of 27bmorphologically
in that the former are derived from underlying forms (namely, relax,emend,
elastic,conn ective )that contain primary stress on the unitalicized vowel when
these underlying forms appear in isolation; those of 27bdo not have this prop-
erty. It is not difﬁcult to show that vowel reduction in English, the replacement
of a vowel by [ ə], is contingent upon lack of stress. We can therefore account
for the distinction between 27aand27bby assuming the cyclic principle just
formulated. In the case of 27a,onthe ﬁrst, innermost cycle, stress will be
assigned by general rules to the unitalicized vowels. On the next cycle, stress isshifted,
16butthe abstract stress assigned on the ﬁrst cycle is sufﬁcient to protect
the vowel from reduction. In the examples of 27b,earlier cycles never assign an
abstract stress to the unitalicized vowel, which thus reduces. Observe that it isanabstract stress that protects the vowel from reduction. The actual, phonetic
stress on the unitalicized nonreduced vowels is very weak; it would be stress 4,in the usual convention. In general, vowels with this weak a phonetic stressreduce, but in this case the abstract stress assigned in the earlier cycle preventsreduction. Thus, it is the abstract underlying representation that determines thephonetic form, a primary role being played by the abstract stress that is virtuallyeliminated in the phonetic form.
In this case, we can provide an explanation for a certain aspect of perception
and articulation in terms of a very general abstract principle, namely the princi-ple of cyclic application of rules (see page 38). It is difﬁcult to imagine how thelanguage learner might derive this principle by “induction” from the data pre-sented to him. In fact, many of the effects of this principle relate to perceptionand have little or no analogue in the physical signal itself, under normal condi-tions of language use, so that the phenomena on which the induction would havebeen based cannot be part of the experience of one who is not already makinguse of the principle. In fact, there is no procedure of induction or association thatoffers any hope of leading from such data as is available to a principle of thissort (unless, begging the question, we introduce the principle of cyclic applica-tion into the “inductive procedure” in some manner). Therefore, the conclusion
16In “connectivity,” it is on the third cycle that the stress is shifted. The second cycle merely
reassigns stress to the same syllable that is stressed on the ﬁrst cycle.

--- Page 59 ---

40 Language and Mind
seems warranted that the principle of cyclic application of phonological rules is
an innate organizing principle of universal grammar that is used in determiningthe character of linguistic experience and in constructing a grammar that con-stitutes the acquired knowledge of language. At the same time, this principleof universal grammar offers an explanation for such phenomena as were notedin27.
There is some evidence that a similar principle of cyclic application applies
also on the syntactic level. John Ross has presented an ingenious analysis ofsome aspects of English pronominalization illustrating this.
17Let us assume
that pronominalization involves a process of “deletion” analogous to those pro-cesses discussed earlier in connection with examples 8–18.This process, to
ﬁrst approximation, replaces one of two identical noun phrases by an appro-priate pronoun. Thus, the underlying structure 28will be converted into 29,b y
pronominalization.
28 John learned that John had won.
29 John learned that he had won.
Abstracting away from properties of 28that are not essential to this discussion,
we can present it in the form 30,where xandyare the identical noun phrases and
yis the one pronominalized, and where the brackets bound sentential expres-
sions.30/bracketleftbig
...x...[ ... y... ]/bracketrightbig
Notice that we cannot form 31from 28by pronominalization:
18
31 He learned that John had won.
That is, we cannot have pronominalization in the case that would be represented
as32,using the conventions of 30:
32/bracketleftbig
...y...[ ... x... ]/bracketrightbig
Consider next the sentences of 33:
33 a. That John won the race surprised him./bracketleftbig
[...x... ]... y.../bracketrightbig
b.John’s winning the race surprised him./bracketleftbig
[...x... ]... y.../bracketrightbig
17J. Ross, “On the Cyclic Nature of English Pronominalization,” in ToHonor Roman Jakobson
(New York: Humanities, 1967).
18Of course, 31is a sentence, but “he” in the sentence does not refer to John as it does in 29.
Thus, 31is not formed by pronominalization if the two occurrences of “John” are intended to
be different in reference. We exclude this case from discussion here. For some remarks bearing
on this problem, see my Aspects ,pp. 144–47.

--- Page 60 ---

Linguistic contributions: present 41
c.That he won the race surprised John./bracketleftbig
[...y... ]... x.../bracketrightbig
d.His winning the race surprised John./bracketleftbig
[...y... ]... x.../bracketrightbig
Continuing with the same conventions, the forms are represented underneath,
in each case. Summarizing, we see that of the possible types 30,32,33a,band
33c,d,all permit pronominalization except 32.These remarks belong to the
particular grammar of English.
Notice that alongside 33dwe also have sentence 34:
34 Winning the race surprised John.
Given the framework we have been assuming throughout, 34must be derived
from the structure “John’s winning the race surprised John.” Hence, in this casepronominalization can be a full deletion.
Consider now sentences 35and36:
35 Our learning that John had won the race surprised him.
36 Learning that John had won the race surprised him.
Sentence 35can be understood with “him” referring to John, but 36cannot.
Thus, 35can be derived by pronominalization from 37,b u t 36is not derived
from 28:
37/bracketleftbigg/bracketleftbig
Our learning [that John had won the race]/bracketrightbig
surprised John./bracketrightbigg
38/bracketleftbigg/bracketleftbig
John’s learning [that John had won the race]/bracketrightbig
surprised John./bracketrightbigg
What might be the explanation for this phenomenon? As Ross observes, it canbe explained in terms of the particular grammar of English if we assume, inaddition, that certain transformations apply in a cycle, ﬁrst to innermost phrases,then to larger phrases, and so on – that is, if we assume that these transforma-tions apply to the deep structure by a process analogous to the process by whichphonological rules apply to the surface structure.
19Making this assumption, let
19That transformational rules may be supposed to function in this way, itself a nontrivial fact if
true, is suggested in my Aspects ,Chapter 3. Ross’s observation suggests that this principle of
application is not only possible but also necessary. Other interesting arguments to this effectare presented in R. Jacobs and P. S. Rosenbaum, eds., Readings in English Transformational
Grammar ,Chapter 28. The matter is far from settled. In general, understanding of syntactic
structure is much more limited than that of phonological structure, descriptions are much morerudimentary, and, correspondingly, principles of universal syntax are much less ﬁrmly estab-lished than principles of universal phonology, though the latter, needless to say, must also beregarded as tentative. In part, this may be due to the inherent complexity of the subject matter.

--- Page 61 ---

42 Language and Mind
us consider the underlying structure 38.Onthe innermost cycle, pronominal-
ization does not apply at all, since there is no second noun phrase identical
to “John” in the most deeply embedded proposition. On the second cycle, weconsider the phrase “/bracketleftbig
John’s learning [that John had won the race]/bracketrightbig
.” This can
be regarded as a structure of the form 30,giving 39by pronominalization; it
cannot be regarded as of the form 32,giving 40by pronominalization, because
the particular grammar of English does not permit pronominalization in thecase of 32,aswehavenoted:
39 John’s learning [that he had won the race]
40 his learning [that John had won the race]
But40would have to be the form underlying 36.Hence, 36cannot be derived
by pronominalization from 38,although 35can be derived from 37.
In this case, then, a principle of universal grammar combines with an indepen-
dently established rule of particular English grammar to yield a certain rathersurprising empirical consequence, namely that 35and36must differ in the
referential interpretation of the pronoun “him.” Once again, as in the formallysomewhat analogous case of vowel reduction discussed earlier in connectionwith examples 27aand27b,itisquite impossible to provide an explanation in
terms of “habits” and “dispositions” and “analogy.” Rather, it seems that certainabstract and in part universal principles governing human mental faculties mustbe postulated to explain the phenomena in question. If the principle of cyclicapplication is indeed a regulative principle determining the form of knowl-edge of language for humans, a person who has learned the particular rulesgoverning pronominalization in English would know, intuitively and withoutinstruction or additional evidence, that 35and36differ in the respect just
noted.
The most challenging theoretical problem in linguistics is that of discovering
the principles of universal grammar that interweave with the rules of particulargrammars to provide explanations for phenomena that appear arbitrary andchaotic. Probably the most persuasive examples at this time (and also the mostimportant ones, in that the principles involved are highly abstract and theiroperations quite intricate) are in the domain of phonology, but these are toocomplex to present within the scope of this lecture.
20Another syntactic example
In part, it results from the fact that universal phonetics, which provides a kind of “empirical
control” for phonological theory, is much more ﬁrmly grounded than universal semantics,which should, in principle, provide a partially analogous control for syntactic theory. In mod-ern linguistics, phonetics (and, in part, phonology) has been studied in considerable depth andwith much success, but the same cannot be said as yet for semantics, despite much interestingwork.
20See references in note 14.The issue is discussed in a general way in my “Explanatory Models in
Linguistics,” in E. Nagel, P. Suppes, and A. Tarski, eds., Logic, Methodology, and Philosophy
of Science (Stanford, Calif.: Stanford University Press, 1962); in my Current Issues ,Section 2;
in my Aspects ,Chapter 1; and in other publications referred to in these references.

--- Page 62 ---

Linguistic contributions: present 43
that illustrates the general problem in a fairly simple way is provided by the
rules for formation of wh-questions in English.21
Consider such sentences as the following:
41 a. Who expected Bill to meet Tom?
b.Who(m) did John expect to meet Tom?
c.Who(m) did John expect Bill to meet?
d.What (books) did you order John to ask Bill to persuade his friendsto stop reading?
As examples a,b,andcshow, a noun phrase in any of the three italicized posi-
tions in a sentence such as “John expected Billto meet Tom” can be questioned.
The process is essentially this:
42 a. wh-placement: assign the marker wh-to a noun phrase.
b.wh-inversion: place the marked noun phrase at the beginning of the
sentence.
c.auxiliary attraction: move a part of the verbal auxiliary or the
copula to the second position in the sentence.
d.phonological interpretation: replace the marked noun phrase by an
appropriate interrogative form.
22
All four of these processes apply nonvacuously in the case of 41band41c.
Sentence 41b,for example, is formed by applying wh-placement to the noun
phrase “someone” in “John expected someone to meet Tom.” Application of
the process of wh-inversion (42b)g i v es “wh-someone John expected to meet
Tom.” The process of auxiliary attraction (42c)g i ves “wh-someone did John
expect to meet Tom.” Finally, the process of phonological interpretation (42d)
gives 41b.Sentence 41dillustrates the fact that these processes can extract a
noun phrase that is deeply embedded in a sentence – without limit, in fact.
Of the processes listed in 42,all but auxiliary attraction apply as well in the
formation of relative clauses, giving such phrases as “the man who(m) Johnexpected to meet Tom,” and so on.
21This matter is discussed in my Current Issues. There are several versions of this monograph. The
ﬁrst, presented at the International Congress of Linguistics, 1962, appears in the Proceedings of
the Congress with the title of the session at which it was presented, “Logical Basis of Linguistic
Theory,” ed. H. Lunt (New York: Humanities, 1964); a second appears in J. Fodor and J. J.
Katz, eds., Structure of Language: Readings in the Philosophy of Language (Englewood Cliffs,
N.J.: Prentice-Hall, 1964); the third, as a separate monograph (New York: Humanities, 1965).These versions differ in the treatment of the examples discussed here; none of the treatments issatisfactory, and the general problem remains open. New and interesting ideas on this matter arepresented in J. Ross, “Constraints on Variables in Syntax,” MIT doctoral thesis (unpublished).Ifollow here the general lines of the earliest of the three versions of Current Issues ,which, in
retrospect, seems to me to offer the most promising approach of the three.
22Actually, it seems that only indeﬁnite singular noun phrases can be questioned (that is, “some-one,” “something,” and so on), a fact that relates to the matter of recoverability of deletionmentioned in note 11. See my Current Issues for some discussion.

--- Page 63 ---

44 Language and Mind
Notice, however, that there are certain restrictions on the formation of ques-
tions and relatives in this manner. Consider, for example, the sentences of 43:
43 a. Forhim to understand this lecture is difﬁcult.
b.It is difﬁcult for him to understand this lecture.
c.He read the book that interested the boy.
d.He believed the claim that John tricked the boy.
e.He believed the claim that John made about the boy.
f.They intercepted John’s message to the boy.
Suppose that we try applying the processes of interrogative and relative for-
mation to the italicized noun phrases in 43.Weshould derive the following
interrogatives and relatives from 43a–43f,respectively:
44 aI. *What is for him to understand difﬁcult?
aR.*a lecture that for him to understand is difﬁcult
bI.What is it difﬁcult for him to understand?
bR. alecture that it is difﬁcult for him to understand
cI.*Who did he read the book that interested?
cR.*the boy who he read the book that interested
dI.*Who did he believe the claim that John tricked?
dR. *the boy who he believed the claim that John tricked
eI.*Who did he believe the claim that John made about?
eR.*the boy who he believed the claim that John made about
fI.*Who did they intercept John’s message to?
fR.*the boy who they intercepted John’s message to
Of these, only bI and bR are fully acceptable, and cases a, c, d, and e arequite impossible, although it would be quite clear what they meant, were theygrammatically permissible. It is not at all obvious how the speaker of Englishknows this to be so. Thus, sentences 43aand43bare synonymous, yet only
43bis subject to the processes in question. And although these processes do not
apply to 43dand43f,they can be applied, with much more acceptable results,
to the very similar sentences 45aand45b:
45 a. He believed that John tricked the boy. (Who did he believe that
John tricked? – the boy who he believed that John tricked)
b.They intercepted a message to the boy. (Who did they intercept a
message to? – the boy who they intercepted a message to)
In some unknown way, the speaker of English devises the principles of 42on
the basis of data available to him; still more mysterious, however, is the factthat he knows under what formal conditions these principles are applicable. Itcan hardly be seriously maintained that every normal speaker of English hashad his behavior “shaped” in the indicated ways by appropriate reinforcement.The sentences of 43,44,and45are as “unfamiliar” as the vast majority of those

--- Page 64 ---

Linguistic contributions: present 45
that we encounter in daily life, yet we know intuitively, without instruction or
awareness, how they are to be treated by the system of grammatical rules that
we have mastered.
It seems, once again, that there is a general principle that accounts for many
such facts. Notice that in 43athe italicized noun phrase is contained within
another noun phrase, namely “for him to understand this lecture,” which is thesubject of the sentence. In 43b,however, a rule of extraposition has placed the
phrase “for him to understand this lecture” outside of the subject noun phrase,and in the resulting structure this phrase is not a noun phrase at all, so that theitalicized phrase in 43bis no longer contained within a noun phrase. Suppose
we were to impose on grammatical transformations the condition that no nounphrase can be extracted from within another noun phrase – more generally, thatif a transformation applies to a structure of the form
/bracketleftbig
S...[A...]A.../bracketrightbig
S
for any category A, then it must be so interpreted as to apply to the maximal
phrase of the type A.
23Then the processes of 42would be blocked, as required,
in cases 43a,c,d,e,andf,b u t not in 43b.Wewill return shortly to 45.
There are other examples that support a principle of this sort, which we will
refer to as the A-over-A principle. Consider the sentences of 46:
46 a. John kept the car in the garage.
b.Mary saw the man walking toward the railroad station.
Each of these is ambiguous. Thus, 46acan mean that the car in the garage was
kept by John, or that the car was kept in the garage by John. In the ﬁrst case, the
italicized phrase is part of a noun phrase, “the car in the garage”; in the lattercase it is not. Similarly, 46bcan mean that the man walking toward the railroad
station was seen by Mary, or that the man was seen walking toward the railroadstation by Mary (or, irrelevantly to this discussion, that Mary, while walkingtoward the railroad station, saw the man). Again, in the ﬁrst case, the italicizedphrase is part of a noun phrase, “the man walking toward the railroad station”;in the latter case, it is not. But now consider the two interrogatives of 47:
47 a. What (garage) did John keep the car in?
b.What did Mary see the man walking toward?
23Wemight extend this principle to the effect that this transformation must also apply to the
minimal phrase of the type S (sentence). Thus, the sentence/bracketleftBig
SJohn was convinced that [SBill would leave before dark]S/bracketrightBig
S
can be transformed to “John was convinced that before dark Bill would leave” but not to “before
dark John was convinced that Bill would leave,” which must have a different source. Like theoriginal principle, this extension is not without its problems, but it has a certain amount ofsupport nevertheless.

--- Page 65 ---

46 Language and Mind
Each of these is unambiguous and can have only the interpretation of the
underlying sentence in which the italicized phrase is not part of another nounphrase. The same is true of the relatives formed from 46,and these facts
too would be explained by the A-over-A principle. There are many similarexamples.
Aslightly more subtle case that might, perhaps, be explained along the same
lines is provided by such sentences as 48and49:
48 John has the best proof of that theorem.
49 What theorem does John have the best proof of?
In its most natural interpretation, sentence 48describes a situation in which
anumber of people have proofs of that theorem, and John’s is the best.
The sense thus suggests that “best” modiﬁes the nominal phrase “proof ofthat theorem,” which contains another nominal phrase, “that theorem.”
24The
A-over-A principle would therefore imply that the phrase “that theorem” notbe subject to the processes of 42.Hence, 49would not be derived by these pro-
cesses from 48.And, in fact, sentence 49has an interpretation rather different
from that of 48.Sentence 49is appropriate to a situation in which John has
proofs of a number of theorems, and the questioner is asking which of theseproofs is the best. The underlying structure, whatever it may be, would associate“best” with “proof,” not with “proof of that theorem,” so that “that theorem”is not embedded within a phrase of the same type and is therefore subject toquestioning (and, similarly, to relativization).
The general principle just proposed has a certain explanatory force, as such
examples illustrate. If postulated as a principle of universal grammar, it can
explain why the particular rules of English operate to generate certain sentences
while rejecting others, and to assign sound–meaning relations in ways thatappear, superﬁcially, to violate regular analogies. Putting the matter in differentterms, if we assume that the A-over-A principle is a part of the innate schematismthat determines the form of knowledge of language, we can account for certainaspects of the knowledge of English possessed by speakers who obviously havenot been trained and who have not even been presented with data bearing onthe phenomena in question in any relevant way, so far as can be ascertained.
Further analysis of data of English reveals, not unexpectedly, that this account
is oversimpliﬁed and runs up against many difﬁculties. Consider, for example,sentences 50and51:
24Space does not permit a discussion of the distinction implied here in the loose terminology,
“noun phrase”–“nominal phrase,” but this is not crucial to the point at issue. See my “Remarks onNominalization,” in R. Jacobs and P. S. Rosenbaum, eds., Readings in English Transformational
Grammar. There are other interpretations of 49(for example, with contrastive stress on “John”),
and there are many open problems relating to such structures as these.

--- Page 66 ---

Linguistic contributions: present 47
50 John thought (that) Bill had read the book.
51 John wondered why Bill had read the book.
In the case of 50,the italicized phrase is subject to interrogation and relativiza-
tion, but not in the case of 51.Itisunclear whether the phrases “that Bill had
read the book” and “why Bill had read the book” are noun phrases. Suppose
that they are not. Then sentence 50is handled in accordance with the A-over-A
principle, but not 51.Toexplain the blocking of the processes of 42in the case
of51,wewould have to assign the phrase “why Bill had read the book” to the
same category as “the book.” In fact, there is a natural suggestion along theselines. Sentence 51is typical in that the phrase from which the noun phrase is to
be extracted is itself a wh-phrase, rather than a that- phrase. Suppose that the
process of wh-placement ( 42a)assigns the element wh-not only to “the book”
in51butalso to the proposition containing it. Thus, both “ wh-the book” and
“why Bill had read the book” belong to the category wh-,which would now be
regarded as a syntactic feature of a sort discussed in my Aspects of the Theory of
Syntax ,Chapter 2 (see note 6). Under these assumptions, the A-over-A principle
will serve to explain the difference between 50and51.
Suppose that the phrases in question are noun phrases. Now it is 50,not51,
that poses the problem. Assuming that our analysis is correct so far, there mustbe some rule that assigns to the proposition “that Bill had read the book” aproperty of “transparency” that permits noun phrases to be extracted from iteven though it is a noun phrase. There are, in fact, other examples that suggest
the necessity for such a rule, presumably a rule of the particular grammar ofEnglish. Thus, consider sentences 52,53,and54:
52 Who would you approve of my seeing?
53 What would you approve of John’s drinking?
54 *What would you approve of John’s excessive drinking of?
Sentences 52and53are formed by applying the processes of interrogation
to a noun phrase contained in the larger phrases “my seeing –,” “John’sdrinking –.” Hence, these larger noun phrases are transparent to the extrac-tion operation. However, as 54indicates, the italicized noun phrase in 55is not
transparent to this operation:
55 Youwould approve of John’s excessive drinking of the beer.
These examples are typical of many that suggest what the rule might be that
assigns transparency. Earlier we discussed sentence 56(sentence 4), pointing
out that it is ambiguous:
56 Idisapprove of John’s drinking.

--- Page 67 ---

48 Language and Mind
Under one interpretation, the phrase “John’s drinking” has the internal structure
of a noun phrase. Thus, the rule that inserts adjectives ( 3d)between a determiner
and a noun applies, giving “John’s excessive drinking”; and, in fact, otherdeterminers may replace “John’s” – “the,” “that,” “much of that,” and so on.Under this interpretation, the phrase “John’s drinking” behaves exactly like“John’s refusal to leave,” “John’s rejection of the offer,” and so on. Under theother interpretation, “John’s drinking (the beer)” does not have the internalstructure of a noun phrase and is handled analogously to “John’s having readthe book,” “John’s refusing to leave,” “John’s rejecting the offer,” and so on,none of which permits adjective insertion or replacement of “John’s” by otherdeterminers. Suppose that we postulate a rule of English grammar that assignstransparency, in the sense just deﬁned, to noun phrases that are also propositionslacking the internal structure of noun phrases. Thus, the phrases “that Bill hadread the book” in 50,“my seeing –” in the structure underlying 52,and “John’s
drinking –” in the structure underlying 53would be assigned transparency; more
precisely, the dominant noun phrase in these examples would not serve to blockextraction by the A-over-A principle. In sentence 51,extraction would still be
blocked by the category wh-,along the lines indicated earlier. And sentence 54is
ruled out because the relevant noun phrase of the underlying structure, “John’sexcessive drinking of –,” does have the internal structure of a noun phrase, as
just noted, and therefore is not subject to the special rule of English grammarthat assigns transparency to the category NP when this category dominates aproposition that lacks the internal structure of an NP.
There are a few other cases that suggest the need for rules of particular
grammar assigning transparency in this sense. Consider sentences 57and58:
57 a. They intercepted John’s message to the boy. (Sentence 43f)
b.He saw John’s picture of Bill.
c.He saw the picture of Bill.
58 a. They intercepted a message to the boy. (Sentence 45b)
b.He saw a picture of Bill.
c.He has a belief in justice.
d.He has faith in Bill’s integrity.
The italicized noun phrases in 57are not subject to the processes of interroga-
tion and relativization, in accordance with the A-over-A principle, as we havealready noted. In the case of 58,interrogation and relativization seem much
more natural in these positions, at least in informal spoken English. Thus, thenoun phrases containing the italicized phrases must be assigned transparency.It seems that what is involved is indeﬁniteness of the dominating noun phrase;if so, then for certain dialects there is a rule assigning transparency to a nounphrase of the form
59 [np
indeﬁnite ...N P ] np

--- Page 68 ---

Linguistic contributions: present 49
There remain a number of very serious problems that seem to resist solution
by such extensions and modiﬁcations of the A-over-A principle. Notice thatthis principle is formulated in a way that is not really well supported by theexamples so far given. If the A-over-A principle were true in general, we would
expect to ﬁnd cases in which a phrase of category A cannot be extracted from
alarger phrase of category A, for various choices of A. In fact, the examples
given so far involve only A =noun phrase (or, perhaps, A =[+wh-], as in the
discussion of 51). Hence, an alternative formulation of the principle consistent
with the facts just noted would assign nontransparency as an ad hoc propertyof certain types of noun phrases (and perhaps other constructions), rather thanas a property of a category A dominating another category of the type A. Givenjust the facts so far presented, it would be proper to postulate the A-over-Aprinciple instead of this alternative precisely because the A-over-A principlehas a certain naturalness, whereas the alternative is entirely ad hoc, a listingof nontransparent structures. But there is crucial evidence, pointed out by JohnRoss (see reference in note 21), suggesting that the A-over-A principle is not
correct. Ross points out that in the constructions from which noun phrasescannot be extracted, adjectives also cannot be extracted. Thus, consider thecontexts “I believe that John saw –,” “I believe the claim that John saw –,”and “I wonder whether John saw –.” From the ﬁrst of these, but not the secondor third, we can extract a noun phrase in interrogation or relativization, a factthat we have been attempting to account for by modiﬁcations of the A-over-Aprinciple. But the same is true of extraction of adjectives. Thus we can form“handsome though I believe that John is,” but not *“handsome though I believethe claim that John is,” *“handsome though I wonder whether John is,” etc.Whether one can extend the approach just discussed to account for this problemin some natural way, I do not know; at the moment, I see no approach that doesnot involve a perfectly ad hoc step. Perhaps this indicates that the approachthrough the A-over-A principle is incorrect, leaving us for the moment with onlyacollection of constructions in which extraction is, for some reason, impossible
to accomplish.
Whatever the answer will prove to be, the complex of problems just discussed
is a typical and important illustration of the kind of topic that is at the borderof research today, in the sense mentioned at the outset of this lecture: thatis, certain problems can be formulated clearly within a framework of ideasthat is reasonably clear and well understood; certain partial solutions can beadvanced; and a range of examples can be discovered where these solutionsfail, leaving open for the time being the question whether what is needed is
further elaboration and sharpening or a radically different approach.
Ihavesofar discussed several kinds of conditions that transformations must
meet: conditions of deletion, of the sort brought out by examples 8–18;the
principle of cyclic application, illustrated by the discussion of examples 28–40
(with the phonological analogue discussed in connection with 27); and the

--- Page 69 ---

50 Language and Mind
A-over-A principle that was proposed as the basis for an explanation of such
phenomena as are illustrated by examples 44–58.Ineach case, there is some
reason to believe that the principle is appropriate, though there is no lack ofevidence showing that the principle is inadequately formulated or, perhaps, mis-
conceived. As a ﬁnal illustration of this state of affairs, typical of the borderlineof research that exists in linguistics as in any other ﬁeld, consider a problem ﬁrstdiscussed by Peter Rosenbaum (see reference in note 6). Consider the sentences
of60:
60 a. John agreed to go.
b.John persuaded Bill to leave.
c.Finding Tom there caused Bill to wonder about John.
In interpreting these sentences, we supply a “missing subject” for the verbs“go,” “leave,” and “ﬁnd,” respectively. In 60a,weunderstand the subject of
“go” to be “John”; in 60b,weunderstand the subject of “leave” to be “Bill”;
in60c,weunderstand the subject of “ﬁnd” and the subject of “wonder” to
be “Bill.” In terms of the framework presupposed so far, it would be natural(though perhaps not necessary, as we will see below) to regard this missingsubject as the actual subject in the deep structure, eliminated by a deletionoperation. Thus, the underlying deep structures might be something like 61:
61 a. John agreed [John go]
b.John persuaded Bill [Bill leave]
c.[Bill ﬁnd Tom there] caused Bill to wonder about John
On the other hand, the facts indicate clearly that the sentences of 60cannot
derive from, say, 62:
62 a. John agreed [someone go]
b.John persuaded Bill [John leave]
c.[John ﬁnd Tom there] caused Bill to wonder about John
It would be difﬁcult to argue that in such cases there is an intrinsic semanticconsideration ruling against such structures as 62.For example, one might inter-
pret62aas meaning that John agreed that someone should go; 62bas meaning
that John persuaded Bill that he (John) would (should) leave; 62cas meaning
that John’s ﬁnding Tom there caused Bill to wonder about John. There must besome general syntactic principle that rules against 62as possible sources for 60
and that causes us to interpret 60as based rather on 61.Rosenbaum suggests
that what is involved is a certain condition on deletion operations, an “erasureprinciple” that prescribes roughly that the subject of an embedded propositionis deleted by the nearest noun phrase outside of this proposition, “nearness”being measured in terms of the number of branches in a representation such as

--- Page 70 ---

Linguistic contributions: present 51
1/primeor2/prime.25As he shows, a great many examples of varied sorts can be explained
on this general assumption, which, like the others that I have been reviewing,
involves a condition on transformations that would constitute part of universalgrammar.
Here too, however, certain problems arise. Consider, for example, the fol-
lowing cases:
26
63 John promised Bill to leave.
64 a. John gave me the impression of working on that problem.
b.John gave me the suggestion of working on that problem.
65 a. John asked me what to wear.
b.John told me what to wear.
66 John asked Bill for permission to leave.
67 a. John begged Bill to permit him to stay.
b.John begged Bill to be permitted to stay.
c.John begged Bill to be shown the new book.
68 John made an offer to Bill (received advice from Bill, received aninvitation from Bill) to stay.
69 John helped Bill write the book.
Sentence 63violates the principle, since it is John, not Bill, who is to leave. In
64a,“John” is understood to be the subject of “work,” whereas in the apparently
analogous sentence 64b the subject is understood to be “I.” In the case of
65a,itis“John” that is the understood subject of “wear”; in 65b,itis“I.”
In the case of 66,“John” is the understood subject of “leave” and “Bill” of
“permit,” underlying “permission,” presumably; in the case of 67a,“Bill” is
the understood surface subject of the embedded proposition, but in 67band
67cit is “John,” although “Bill” is the “nearest” noun phrase in all three cases,
in Rosenbaum’s sense. In 68,itis“John” that is understood as the subject
of “stay,” in apparent contradiction to the principle, though much depends onunresolved questions as to how these sentences are to be analyzed. The case of69is obscure in other ways. The erasure principle would suggest that “Bill” is
the subject of “write,” although of course the sentence does not imply that Billwrote the book – rather John and Bill did, together. But there is a difﬁculty inpursuing this interpretation. Thus, from 69we can conclude that John helped
25In yet-unpublished work, David Perlmutter has presented a strong argument that what is involved
is not a condition on transformations but rather a condition on well-formed deep structures. Thedistinction is not crucial for what follows but would become important at a less superﬁcial levelof discussion.
26Examples 63and67are discussed by Rosenbaum; 64waspointed out by Maurice Gross; 65was
pointed out in a different connection by Zeno Vendler, “Nominalizations,” in Transformations
and Discourse Analysis Papers ,No. 55 (Philadelphia: University of Pennsylvania, 1964), p. 67.

--- Page 71 ---

52 Language and Mind
write the book, but from the apparently analogous sentence “John helped the
cat have kittens,” we cannot deduce that “John helped have kittens,” which isdeviant, a fact that suggests that somehow there must be a grammatical relationbetween “John” and “write” in 69.Toput it differently, the problem is how to
account for “John helped write the book” as analogous to 60a,since obviously
the analogue to 61awon’t do as a source.
Without pursuing the matter any further, we can see that although the erasure
principle has much to recommend it and is probably somehow involved in thecorrect solution to this network of problems, there is much evidence still to beaccounted for. As in the other cases mentioned, there are a variety of problemsrelating to the conditions that determine applicability of transformations, prob-lems that still resist any near-deﬁnitive solution, though some interesting andilluminating proposals can be made that seem to go part of the way toward ageneral solution.
In discussing the nature of grammatical operations, I have restricted myself
to syntactic and phonological examples, avoiding questions of semantic inter-pretation. If a grammar is to characterize the full linguistic competence of thespeaker–hearer, it must comprise rules of semantic interpretation as well, butlittle is known of any depth regarding this aspect of grammar. In the referencescited earlier (see note 6), it is proposed that a grammar consists of a syntactic
component that speciﬁes an inﬁnite set of paired deep and surface structuresand expresses the transformational relationship between these paired elements,aphonological component that assigns a phonetic representation to the surface
structure, and a semantic component that assigns a semantic representation tothe deep structure. As noted earlier (p. 27; see also pp. 94–97), I think there is
strong evidence that aspects of the surface structure are also relevant to semanticinterpretation.
27However this may be, there can be little doubt that a full gram-
mar must contain fairly intricate rules of semantic interpretation, keyed, at leastin part, to fairly speciﬁc properties of the lexical items and formal structures ofthe language in question. To mention just one example, consider sentence 70:
70 John has lived in Princeton.
From the assumption that this sentence has been properly used to make a state-ment, we can conclude that John is a person (one would not say that his doghas lived in Princeton); that Princeton is a place meeting certain physical and
27Forsome remarks concerning this problem, see my “Surface Structure and Semantic Interpreta-
tion,” in R. Jakobson, ed., Studies in General and Oriental Linguistics (Tokyo: TEC Corporation
for Language and Educational Research, 1970). Literature on semantic interpretation of syntac-
tic structures is expanding fairly rapidly. For recent discussion, see J. J. Katz, The Philosophy of
Language (New York: Harper & Row, 1966); U. Weinreich, “Explorations in Semantic Theory,”
in T. A. Sebeok, ed., Current Trends in Linguistics ,Vol. III (New York: Humanities, 1966);
J. J. Katz, “Recent Issues in Semantic Theory,” in Foundations of Language ,Vol. 3, No. 2, May
1967, pp. 124–94; and many other papers.

--- Page 72 ---

Linguistic contributions: present 53
sociological conditions (given that “Princeton” is a proper noun); that John is
now alive (I can say that I have lived in Princeton, but I cannot now say “Einsteinhas lived in Princeton” – rather, “Einstein lived in Princeton”); and so on. Thesemantic interpretation of 70must be such as to account for these facts.
In part, such questions as these might be subsumed under a still-to-be-
developed universal semantics, in which concepts and their relations are ana-lyzed in a very general way; to take a classical example, it might be argued thatthe relation of meaning between “John is proud of what Bill did” and “John hassome responsibility for Bill’s actions” should be explained in terms of the uni-versal concepts of pride and responsibility, just as on the level of sound structure
one might appeal to a principle of universal phonetics to account for the factthat when a velar consonant becomes palatal it ordinarily becomes strident (seereferences in note 14,for discussion). The proposal looks less attractive when
applied to the case of 70,for example, with respect to the fact that proper use
of70implies that John is now alive. When we try to pursue such questions,
we soon become lost in a tangle of confused issues and murky problems, andit is difﬁcult to propose answers that carry any conviction. For this reason, Iam unable to discuss conditions on rules of semantic interpretation that mightbe analogous to the conditions on syntactic and phonological rules mentionedearlier.
Observe that I might well have been mistaken in the preceding remarks in
assuming that the topics discussed belong to syntax rather than to the semanticcomponent of a grammar, or to some domain in which semantic and syntacticrules interpenetrate. The issues are too clouded for us to be able to say that thisis an empirical question, as matters now stand; but when they are sharpened, wemay ﬁnd that an empirical question can be posed. Consider, for example, thediscussion of the erasure principle in syntax. Joseph Emonds has suggested (inunpublished work) that it is incorrect to assume, as I did, that the sentences of60are interpreted through reference to the underlying structures of 61.Rather,
he argues that what I took to be the embedded proposition has no subject at all inthe underlying form generated by the syntactic component, and a general ruleof semantic interpretation takes the place of Rosenbaum’s erasure principle.Whether this is correct I do not know, but it is certainly a possibility. We canexpect, as research continues into problems of grammar, that the boundaries
that seem clear today may shift in unpredictable ways, or that some new basisfor the organization of grammar may replace the framework that now seemsappropriate.
The conditions on grammatical rules that I have been discussing are complex
and only partially understood. It should be emphasized, however, that even someof the simplest and clearest conditions of the form of grammar are in no sensenecessary properties of a system that fulﬁlls the functions of human language.Correspondingly, the fact that they hold true of human languages in general and

--- Page 73 ---

54 Language and Mind
play a role in the acquired linguistic competence of the speaker–hearer cannot
be lightly dismissed. Consider, for example, the simple fact that grammaticaltransformations are invariably structure-dependent in the sense that they apply
to a string of words
28by virtue of the organization of these words into phrases.
It is easy to imagine structure-independent operations that apply to a string of
elements quite independently of its abstract structure as a system of phrases.Forexample, the rule that forms the interrogatives of 71from the corresponding
declaratives of 72(see note 10)isastructure-dependent rule interchanging a
noun phrase with the ﬁrst element of the auxiliary.
71 a. Will the members of the audience who enjoyed the play stand?
b.Has Mary lived in Princeton?
c.Will the subjects who will act as controls be paid?
72 a. The members of the audience who enjoyed the play will stand.
b.Mary has lived in Princeton.
c.The subjects who will act as controls will be paid.
In contrast, consider the operation that inverts the ﬁrst and last words of a sen-
tence, or that arranges the words of a sentence in increasing length in termsof phonetic segments (“alphabetizing” in some speciﬁed way for items of thesame length), or that moves the left-most occurrence of the word “will” to theextreme left – call these O
1,O 2,and O 3,respectively. Applying O 1to72a,
we derive 73a;applying O 2to72b,wederive 73b;applying O 3to72c,w e
derive 73c:
73 a. stand the members of the audience who enjoyed the play will
b.in has lived Mary Princeton
c.will the subjects who act as controls will be paid
The operations O 1,O2,and O 3are structure-independent. Innumerable other
operations of this sort can be speciﬁed.
There is no a priori reason why human language should make use exclusively
of structure-dependent operations, such as English interrogation, instead ofstructure-independent operations, such as O
1,O2,and O 3.One can hardly argue
that the latter are more “complex” in some absolute sense; nor can they beshown to be more productive of ambiguity or more harmful to communicativeefﬁciency. Yet no human language contains structure-independent operationsamong (or replacing) the structure-dependent grammatical transformations. Thelanguage-learner knows that the operation that gives 71is a possible candidate
for a grammar, whereas O
1,O2,and O 3,and any operations like them, need not
be considered as tentative hypotheses.
28More properly, to a string of minimal linguistic units that may or may not be words.

--- Page 74 ---

Linguistic contributions: present 55
If we establish the proper “psychic distance” from such elementary and
commonplace phenomena as these, we will see that they really pose some
nontrivial problems for human psychology. We can speculate about the reasonfor the reliance on structure-dependent operations,
29butwemust recognize
that any such speculation must involve assumptions regarding human cognitivecapacities that are by no means obvious or necessary. And it is difﬁcult to avoidthe conclusion that whatever its function may be, the reliance on structure-dependent operations must be predetermined for the language-learner by arestrictive initial schematism of some sort that directs his attempts to acquirelinguistic competence. Similar conclusions seem to me warranted, a fortiori, inthe case of the deeper and more intricate principles discussed earlier, whatevertheir exact form may turn out to be.
Tosummarize: along the lines that have been outlined here, we might develop
on the one hand a system of general principles of universal grammar,
30and on
the other, particular grammars that are formed and interpreted in accordancewith these principles. The interplay of universal principles and particular rulesleads to empirical consequences such as those we have illustrated; at variouslevels of depth, these rules and principles provide explanations for facts aboutlinguistic competence – the knowledge of language possessed by each normalspeaker – and about some of the ways in which this knowledge is put to use inthe performance of the speaker or hearer.
The principles of universal grammar provide a highly restrictive schema
to which any human language must conform, as well as speciﬁc conditionsdetermining how the grammar of any such language can be used. It is easy toimagine alternatives to the conditions that have been formulated (or those thatare often tacitly assumed). These conditions have in the past generally escapednotice, and we know very little about them today. If we manage to establish theappropriate “psychic distance” from the relevant phenomena and succeed in“making them strange” to ourselves, we see at once that they pose very seriousproblems that cannot be talked or deﬁned out of existence. Careful considerationof such problems as those sketched here indicates that to account for the normal
29See G. A. Miller and N. Chomsky, “Finitary Models of Language Users, Part II,” in R. D. Luce,
R. Bush, and E. Galanter, eds., Handbook of Mathematical Psychology ,Vol. II (New York:
Wiley, 1963), for some proposals regarding this matter.
30Notice that we are interpreting “universal grammar” as a system of conditions on grammars. Itmay involve a skeletal substructure of rules that any human language must contain, but it alsoincorporates conditions that must be met by such grammars and principles that determine howthey are interpreted. This formulation is something of a departure from a traditional view thattook universal grammar to be simply a substructure of each particular grammar, a system ofrules at the very core of each grammar. This traditional view has also received expression inrecent work. It seems to me to have little merit. As far as information is available, there are heavyconstraints on the form and interpretation of grammar at all levels, from the deep structures ofsyntax, through the transformational component, to the rules that interpret syntactic structuressemantically and phonetically.

--- Page 75 ---

56 Language and Mind
use of language we must attribute to the speaker–hearer an intricate system
of rules that involve mental operations of a very abstract nature, applying torepresentations that are quite remote from the physical signal. We observe,furthermore, that knowledge of language is acquired on the basis of degenerateand restricted data and that it is to a large extent independent of intelligenceand of wide variations in individual experience.
If a scientist were faced with the problem of determining the nature of a
device of unknown properties that operates on data of the sort available to achild and gives as “output” (that is, as a “ﬁnal state of the device,” in this case) aparticular grammar of the sort that it seems necessary to attribute to the personwho knows the language, he would naturally search for inherent principles oforganization that determine the form of the output on the basis of the limiteddata available. There is no reason to adopt a more prejudiced or dogmatic viewwhen the device of unknown properties is the human mind; speciﬁcally, thereis no reason to suppose, in advance of any argument, that the general empiricistassumptions that have dominated speculation about these matters have anyparticular privileged claim. No one has succeeded in showing why the highlyspeciﬁc empiricist assumptions about how knowledge is acquired should betaken seriously. They appear to offer no way to describe or account for themost characteristic and normal constructions of human intelligence, such aslinguistic competence. On the other hand, certain highly speciﬁc assumptionsabout particular and universal grammar give some hope of accounting for thephenomena that we face when we consider knowledge and use of language.Speculating about the future, it seems not unlikely that continued researchalong the lines indicated here will bring to light a highly restrictive schematismthat determines both the content of experience and the nature of the knowledgethat arises from it, thus vindicating and elaborating some traditional thinkingabout problems of language and mind. It is to this matter, among others, that Ishall turn in the ﬁnal lecture.

--- Page 76 ---

3 Linguistic contributions to the study
of mind: future
In discussing the past, I referred to two major traditions that have enriched
the study of language in their separate and very different ways; and in my lastlecture, I tried to give some indication of the topics that seem on the immediatehorizon today, as a kind of synthesis of philosophical grammar and structurallinguistics begins to take shape. Each of the major traditions of study andspeculation that I have been using as a point of reference was associated with acertain characteristic approach to the problems of mind; we might say, withoutdistortion, that each evolved as a speciﬁc branch of the psychology of its time,to which it made a distinctive contribution.
It may seem a bit paradoxical to speak of structural linguistics in this way,
given its militant anti-psychologism. But the paradox is lessened when we takenote of the fact that this militant anti-psychologism is no less true of much ofcontemporary psychology itself, particularly of those branches that until a fewyears ago monopolized the study of use and acquisition of language. We live,after all, in the age of “behavioral science,” not of “the science of mind.” I do notwant to read too much into a terminological innovation, but I think that there is
some signiﬁcance in the ease and willingness with which modern thinking aboutman and society accepts the designation “behavioral science.” No sane personhas ever doubted that behavior provides much of the evidence for this study –all of the evidence, if we interpret “behavior” in a sufﬁciently loose sense. Butthe term “behavioral science” suggests a not-so-subtle shift of emphasis towardthe evidence itself and away from the deeper underlying principles and abstractmental structures that might be illuminated by the evidence of behavior. It is asif natural science were to be designated “the science of meter readings.” What,in fact, would we expect of natural science in a culture that was satisﬁed toaccept this designation for its activities?
Behavioral science has been much preoccupied with data and organization of
data, and it has even seen itself as a kind of technology of control of behavior.Anti-mentalism in linguistics and in philosophy of language conforms to thisshift of orientation. As I mentioned in my ﬁrst lecture, I think that one majorindirect contribution of modern structural linguistics results from its success inmaking explicit the assumptions of an anti-mentalistic, thoroughly operational
57

--- Page 77 ---

58 Language and Mind
and behaviorist approach to the phenomena of language. By extending this
approach to its natural limits, it laid the groundwork for a fairly conclusivedemonstration of the inadequacy of any such approach to the problems of mind.
More generally, I think that the long-range signiﬁcance of the study of lan-
guage lies in the fact that in this study it is possible to give a relatively sharpand clear formulation of some of the central questions of psychology and tobring a mass of evidence to bear on them. What is more, the study of languageis, for the moment, unique in the combination it affords of richness of data andsusceptibility to sharp formulation of basic issues.
It would, of course, be silly to try to predict the future of research, and it will
be understood that I do not intend the subtitle of this lecture to be taken veryseriously. Nevertheless, it is fair to suppose that the major contribution of thestudy of language will lie in the understanding it can provide as to the characterof mental processes and the structures they form and manipulate. Therefore,instead of speculating on the likely course of research into the problems thatare coming into focus today,
1Iwill concentrate here on some of the issues that
arise when we try to develop the study of linguistic structure as a chapter ofhuman psychology.
It is quite natural to expect that a concern for language will remain central
to the study of human nature, as it has been in the past. Anyone concernedwith the study of human nature and human capacities must somehow come togrips with the fact that all normal humans acquire language, whereas acqui-sition of even its barest rudiments is quite beyond the capacities of an other-wise intelligent ap e–af a c t that was emphasized, quite correctly, in Cartesian
philosophy.
2It is widely thought that the extensive modern studies of animal
communication challenge this classical view; and it is almost universally taken
1Anumber of such problems might be enumerated – for example, the problem of how the intrinsic
content of phonetic features determines the functioning of phonological rules, the role of universal
formal conditions in restricting the choice of grammars and the empirical interpretation of suchgrammars, the relations of syntactic and semantic structure, the nature of universal semantics,performance models that incorporate generative grammars, and so on.
2Modern attempts to train apes in behavior that the investigators regard as language-like conﬁrmthis incapacity, though it may be that the failures are to be attributed to the technique of operantconditioning and therefore show little about the animal’s actual abilities. See, for example, thereport by C. B. Ferster, “Arithmetic Behavior in Chimpanzees,” in Scientiﬁc American ,May 1964,
pp. 98–106. Ferster attempted to teach chimpanzees to match the binary numbers 001, . . . , 111to sets of one to seven objects. He reports that hundreds of thousands of trials were required for95 percent accuracy to be achieved, even in this trivial task. Of course, even at this stage the apeshad not learned the principle of binary arithmetic; they would not, for example, be able to matchafour-digit binary number correctly, and, presumably, they would have done just as badly in
the experiment had it involved an arbitrary association of the binary numbers to sets rather thanthe association determined by the principle of the binary notation. Ferster overlooks this crucialpoint and therefore concludes, mistakenly, that he has taught the rudiments of symbolic behavior.The confusion is compounded by his deﬁnition of language as “a set of symbolic stimuli thatcontrol behavior” and by his strange belief that the “effectiveness” of language arises from thefact that utterances “control almost identical performances in speaker and listener.”

--- Page 78 ---

Linguistic contributions: future 59
for granted that there exists a problem of explaining the “evolution” of human
language from systems of animal communication. However, a careful look atrecent studies of animal communication seems to me to provide little supportfor these assumptions. Rather, these studies simply bring out even more clearlythe extent to which human language appears to be a unique phenomenon, with-out signiﬁcant analogue in the animal world. If this is so, it is quite senselessto raise the problem of explaining the evolution of human language from moreprimitive systems of communication that appear at lower levels of intellec-tual capacity. The issue is important, and I would like to dwell on it for amoment.
The assumption that human language evolved from more primitive systems
is developed in an interesting way by Karl Popper in his recently publishedArthur Compton Lecture, “Clouds and Clocks.” He tries to show how prob-lems of freedom of will and Cartesian dualism can be solved by the analysisof this “evolution.” I am not concerned now with the philosophical conclu-sions that he draws from this analysis, but with the basic assumption that thereis an evolutionary development of language from simpler systems of the sortthat one discovers in other organisms. Popper argues that the evolution of lan-guage passed through several stages, in particular a “lower stage” in whichvocal gestures are used for expression of emotional state, for example, and a
“higher stage” in which articulated sound is used for expression of thought – inPopper’s terms, for description and critical argument. His discussion of stagesof evolution of language suggests a kind of continuity, but in fact he establishesno relation between the lower and higher stages and does not suggest a mech-anism whereby transition can take place from one stage to the next. In short,he gives no argument to show that the stages belong to a single evolutionaryprocess. In fact, it is difﬁcult to see what links these stages at all (except forthe metaphorical use of the term “language”). There is no reason to supposethat the “gaps” are bridgeable. There is no more of a basis for assuming anevolutionary development of “higher” from “lower” stages, in this case, than
there is for assuming an evolutionary development from breathing to walking;the stages have no signiﬁcant analogy, it appears, and seem to involve entirelydifferent processes and principles.
Amore explicit discussion of the relation between human language and ani-
mal communication systems appears in a recent discussion by the comparativeethologist W. H. Thorpe.
3He points out that mammals other than man appear
to lack the human ability to imitate sounds, and that one might therefore haveexpected birds (many of which have this ability to a remarkable extent) to
3W.H. Thorpe, “Animal V ocalization and Communication,” in F. L. Darley, ed., Brain Mechanisms
Underlying Speech and Language (New York: Grune and Stratton, 1967), pp. 2–10 and the
discussions on pp. 19 and 84–85.

--- Page 79 ---

60 Language and Mind
be “the group which ought to have been able to evolve language in the true
sense, and not the mammals.” Thorpe does not suggest that human language“evolved” in any strict sense from simpler systems, but he does argue that thecharacteristic properties of human language can be found in animal commu-nication systems, although “we cannot at the moment say deﬁnitely that theyare all present in one particular animal.” The characteristics shared by humanand animal language are the properties of being “purposive,” “syntactic,” and“propositional.” Language is purposive “in that there is nearly always in humanspeech a deﬁnite intention of getting something over to somebody else, alteringhis behavior, his thoughts, or his general attitude toward a situation.” Humanlanguage is “syntactic” in that an utterance is a performance with an inter-nal organization, with structure and coherence. It is “propositional” in that ittransmits information. In this sense, then, both human language and animalcommunication are purposive, syntactic, and propositional.
All this may be true, but it establishes very little, since when we move to
the level of abstraction at which human language and animal communicationfall together, almost all other behavior is included as well. Consider walking:
clearly, walking is purposive behavior, in the most general sense of “purposive.”Walking is also “syntactic” in the sense just deﬁned, as, in fact, Karl Lashley
pointed out a long time ago in his important discussion of serial order in behav-ior, to which I referred in the ﬁrst lecture.
4Furthermore, it can certainly be
informative; for example, I can signal my interest in reaching a certain goal bythe speed or intensity with which I walk.
It is, incidentally, precisely in this manner that the examples of animal com-
munication that Thorpe presents are “propositional.” He cites as an examplethe song of the European robin, in which the rate of alternation of high andlow pitch signals the intention of the bird to defend its territory; the higher therate of alternation, the greater the intention to defend the territory. The exam-ple is interesting, but it seems to me to show very clearly the hopelessnessof the attempt to relate human language to animal communication. Every ani-mal communication system that is known (if we disregard some science ﬁctionabout dolphins) uses one of two basic principles: either it consists of a ﬁxed,ﬁnite number of signals, each associated with a speciﬁc range of behavior oremotional state, as is illustrated in the extensive primate studies that have beencarried out by Japanese scientists for the past several years; or it makes use of aﬁxed, ﬁnite number of linguistic dimensions, each of which is associated with aparticular nonlinguistic dimension in such a way that selection of a point alongthe linguistic dimension determines and signals a certain point along the asso-ciated nonlinguistic dimension. The latter is the principle realized in Thorpe’s
4K. S. Lashley, “The Problem of Serial Order in Behavior,” in L. A. Jeffress, ed., Cerebral
Mechanisms in Behavior (New York: Wiley, 1951), pp. 112–36.

--- Page 80 ---

Linguistic contributions: future 61
bird-song example. Rate of alternation of high and low pitch is a linguistic
dimension correlated with the nonlinguistic dimension of intention to defendaterritory. The bird signals its intention to defend a territory by selecting a
correlated point along the linguistic dimension of pitch alternation – I use theword “select” loosely, of course. The linguistic dimension is abstract, but the
principle is clear. A communication system of the second type has an indeﬁ-nitely large range of potential signals, as does human language. The mechanismand principle, however, are entirely different from those employed by humanlanguage to express indeﬁnitely many new thoughts, intentions, feelings, and soon. It is not correct to speak of a “deﬁciency” of the animal system, in terms ofrange of potential signals; rather the opposite, since the animal system admitsin principle of continuous variation along the linguistic dimension (insofar as itmakes sense to speak of “continuity” in such a case), whereas human language isdiscrete. Hence, the issue is not one of “more” or “less,” but rather of an entirelydifferent principle of organization. When I make some arbitrary statement in ahuman language – say, that “the rise of supranational corporations poses newdangers for human freedom” – I am not selecting a point along some linguisticdimension that signals a corresponding point along an associated nonlinguisticdimension, nor am I selecting a signal from a ﬁnite behavioral repertoire, innateor learned.
Furthermore, it is wrong to think of human use of language as character-
istically informative, in fact or in intention. Human language can be used toinform or mislead, to clarify one’s own thoughts or to display one’s cleverness,or simply for play. If I speak with no concern for modifying your behavior orthoughts, I am not using language any less than if I say exactly the same thingswith such intention. If we hope to understand human language and the psycho-
logical capacities on which it rests, we must ﬁrst ask what it is, not how or forwhat purposes it is used. When we ask what human language is, we ﬁnd nostriking similarity to animal communication systems. There is nothing usefulto be said about behavior or thought at the level of abstraction at which animaland human communication fall together. The examples of animal communica-tion that have been examined to date do share many of the properties of humangestural systems, and it might be reasonable to explore the possibility of directconnection in this case. But human language, it appears, is based on entirelydifferent principles. This, I think, is an important point, often overlooked bythose who approach human language as a natural, biological phenomenon; inparticular, it seems rather pointless, for these reasons, to speculate about theevolution of human language from simpler systems – perhaps as absurd as it
would be to speculate about the “evolution” of atoms from clouds of elementary
particles.
As far as we know, possession of human language is associated with a spe-
ciﬁc type of mental organization, not simply a higher degree of intelligence.

--- Page 81 ---

62 Language and Mind
There seems to be no substance to the view that human language is simply
amore complex instance of something to be found elsewhere in the animal
world. This poses a problem for the biologist, since, if true, it is an example of
true “emergence” – the appearance of a qualitatively different phenomenon ataspeciﬁc stage of complexity of organization. Recognition of this fact, though
formulated in entirely different terms, is what motivated much of the classicalstudy of language by those whose primary concern was the nature of mind. Andit seems to me that today there is no better or more promising way to explorethe essential and distinctive properties of human intelligence than through thedetailed investigation of the structure of this unique human possession. A rea-sonable guess, then, is that if empirically adequate generative grammars can beconstructed and the universal principles that govern their structure and organi-zation determined, then this will be an important contribution to human psy-chology, in ways to which I will turn directly, in detail.
In the course of these lectures I have mentioned some of the classical ideas
regarding language structure and contemporary efforts to deepen and extendthem. It seems clear that we must regard linguistic competence – knowledgeof a language – as an abstract system underlying behavior, a system consti-tuted by rules that interact to determine the form and intrinsic meaning of apotentially inﬁnite number of sentences. Such a system – a generative gram-mar – provides an explication of the Humboldtian idea of “form of language,”which in an obscure but suggestive remark in his great posthumous work, ¨Uber
die Verschiedenheit des Menschlichen Sprachbaues ,Humboldt deﬁnes as “that
constant and unvarying system of processes underlying the mental act of raisingarticulated structurally organized signals to an expression of thought.” Such agrammar deﬁnes a language in the Humboldtian sense, namely as “a recur-sively generated system, where the laws of generation are ﬁxed and invariant,butthe scope and the speciﬁc manner in which they are applied remain entirely
unspeciﬁed.”
In each such grammar there are particular, idiosyncratic elements, selection
of which determines one speciﬁc human language; and there are general univer-sal elements, conditions on the form and organization of any human language,that form the subject matter for the study of “universal grammar.” Among theprinciples of universal grammar are those I discussed in the preceding lecture –for example, the principles that distinguish deep and surface structure and thatconstrain the class of transformational operations that relate them. Notice, inci-dentally, that the existence of deﬁnite principles of universal grammar makespossible the rise of the new ﬁeld of mathematical linguistics, a ﬁeld that sub-mits to abstract study the class of generative systems meeting the conditions setforth in universal grammar. This inquiry aims to elaborate the formal propertiesof any possible human language. The ﬁeld is in its infancy; it is only in thelast decade that the possibility of such an enterprise has been envisioned. It

--- Page 82 ---

Linguistic contributions: future 63
has some promising initial results, and it suggests one possible direction for
future research that might prove to be of great importance. Thus, mathematicallinguistics seems for the moment to be in a uniquely favorable position, amongmathematical approaches in the social and psychological sciences, to developnot simply as a theory of data, but as the study of highly abstract principles andstructures that determine the character of human mental processes. In this case,the mental processes in question are those involved in the organization of onespeciﬁc domain of human knowledge, namely knowledge of language.
The theory of generative grammar, both particular and universal, points to a
conceptual lacuna in psychological theory that I believe is worth mentioning.Psychology conceived as “behavioral science” has been concerned with behav-ior and acquisition or control of behavior. It has no concept corresponding to“competence,” in the sense in which competence is characterized by a genera-tive grammar. The theory of learning has limited itself to a narrow and surelyinadequate concept of what is learned – namely a system of stimulus-responseconnections, a network of associations, a repertoire of behavioral items, a habithierarchy, or a system of dispositions to respond in a particular way under speci-ﬁable stimulus conditions.
5Insofar as behavioral psychology has been applied
to education or therapy, it has correspondingly limited itself to this concept of“what is learned.” But a generative grammar cannot be characterized in theseterms. What is necessary, in addition to the concept of behavior and learning,is a concept of what is learned – a notion of competence – that lies beyond theconceptual limits of behaviorist psychological theory. Like much of modern lin-guistics and modern philosophy of language, behaviorist psychology has quiteconsciously accepted methodological restrictions that do not permit the studyof systems of the necessary complexity and abstractness.
6One important future
5This limitation is revealed, for example, in such statements as this from W. M. Wiest, in “Recent
Criticisms of Behaviorism and Learning,” in Psychological Bulletin ,Vol. 67, No. 3, 1967,
pp. 214–25: “An empirical demonstration . . . that a child has learned the rules of grammarwould be his exhibiting the verbal performance called ‘uttering the rules of grammar.’ That this
performance is not usually acquired without special training is attested to by many grammarschool teachers. One may even speak quite grammatically without having literally learned therules of grammar.” Wiest’s inability to conceive of another sense in which the child may be saidto have learned the rules of grammar testiﬁes to the conceptual gap we are discussing. Since herefuses to consider the question of what is learned, and to clarify this notion before asking how it
is learned, he can only conceive of “grammar” as the “behavioral regularities in the understandingand production of speech” – a characterization that is perfectly empty, as it stands, there beingno “behavioral regularities” associated with (let alone “in”) the understanding and production ofspeech. One cannot quarrel with the desire of some investigators to study “the acquisition andmaintenance of actual occurrences of verbal behavior ”(ibid.). It remains to be demonstrated
that this study has something to do with the study of language. As of now, I see no indicationthat this claim can be substantiated.
6See my paper, “Some Empirical Assumptions in Modern Philosophy of Language,” inS. Morgenbesser, P. Suppes, and M. White, eds., Essays in Honor of Ernest Nagel (New York:
St. Martin’s, 1969), for a discussion of the work of Quine and Wittgenstein from this point ofview.

--- Page 83 ---

64 Language and Mind
contribution of the study of language to general psychology may be to focus
attention on this conceptual gap and to demonstrate how it may be ﬁlled bythe elaboration of a system of underlying competence in one domain of humanintelligence.
There is an obvious sense in which any aspect of psychology is based ulti-
mately on the observation of behavior. But it is not at all obvious that the studyof learning should proceed directly to the investigation of factors that controlbehavior or of conditions under which a “behavioral repertoire” is established.It is ﬁrst necessary to determine the signiﬁcant characteristics of this behavioralrepertoire, the principles on which it is organized. A meaningful study of learn-ing can proceed only after this preliminary task has been carried out and hasled to a reasonably well-conﬁrmed theory of underlying competence – in thecase of language, to the formulation of the generative grammar that underliesthe observed use of language. Such a study will concern itself with the relationbetween the data available to the organism and the competence that it acquires;only to the extent that the abstraction to competence has been successful – inthe case of language, to the extent that the postulated grammar is “descrip-tively adequate” in the sense described in Lectur e2– can the investigation of
learning hope to achieve meaningful results. If, in some domain, the organi-zation of the behavioral repertoire is quite trivial and elementary, then therewill be little harm in avoiding the intermediate stage of theory construction, inwhich we attempt to characterize accurately the competence that is acquired.But one cannot count on this being the case, and in the study of language itsurely is not the case. With a richer and more adequate characterization of“what is learned” – of the underlying competence that constitutes the “ﬁnalstate” of the organism being studied – it may be possible to approach the taskof constructing a theory of learning that will be much less restricted in scopethan modern behavioral psychology has proved to be. Surely it is pointless toaccept methodological strictures that preclude such an approach to problems oflearning.
Are there other areas of human competence where one might hope to develop
afruitful theory, analogous to generative grammar? Although this is a very
important question, there is very little that can be said about it today. One might,for example, consider the problem of how a person comes to acquire a certainconcept of three-dimensional space, or an implicit “theory of human action,”in similar terms. Such a study would begin with the attempt to characterizethe implicit theory that underlies actual performance and would then turn tothe question of how this theory develops under the given conditions of timeand access to data – that is, in what way the resulting system of beliefs isdetermined by the interplay of available data, “heuristic procedures,” and theinnate schematism that restricts and conditions the form of the acquired system.At the moment, this is nothing more than a sketch of a program of research.

--- Page 84 ---

Linguistic contributions: future 65
There have been some attempts to study the structure of other, language-
like systems – the study of kinship systems and folk taxonomies comes to
mind, for example. But so far, at least, nothing has been discovered that is evenroughly comparable to language in these domains. No one, to my knowledge,has devoted more thought to this problem than L´ evi-Strauss. For example, his
recent book on the categories of primitive mentality
7is a serious and thoughtful
attempt to come to grips with this problem. Nevertheless, I do not see whatconclusions can be reached from a study of his materials beyond the fact thatthe savage mind attempts to impose some organization on the physical world –that humans classify, if they perform any mental acts at all. Speciﬁcally, L´ evi-
Strauss’s well-known critique of totemism seems to reduce to little more thanthis conclusion.
L´evi-Strauss models his investigations quite consciously on structural lin-
guistics, particularly on the work of Troubetzkoy and Jakobson. He repeatedlyand quite correctly emphasizes that one cannot simply apply procedures analo-gous to those of phonemic analysis to subsystems of society and culture. Rather,he is concerned with structures “where they may be found . . . in the kinshipsystem, political ideology, mythology, ritual, art,” and so on,
8and he wishes
to examine the formal properties of these structures in their own terms. Butseveral reservations are necessary when structural linguistics is used as a modelin this way. For one thing, the structure of a phonological system is of verylittle interest as a formal object; there is nothing of signiﬁcance to be said, fromaformal point of view, about a set of forty-odd elements cross-classiﬁed in
terms of eight or ten features. The signiﬁcance of structuralist phonology, asdeveloped by Troubetzkoy, Jakobson, and others, lies not in the formal proper-ties of phonemic systems but in the fact that a fairly small number of featuresthat can be speciﬁed in absolute, language-independent terms appear to providethe basis for the organization of all phonological systems. The achievement ofstructuralist phonology was to show that the phonological rules of a great vari-ety of languages apply to classes of elements that can be simply characterizedin terms of these features; that historical change affects such classes in a uni-form way; and that the organization of features plays a basic role in the use andacquisition of language. This was a discovery of the greatest importance, andit provides the groundwork for much of contemporary linguistics. But if weabstract away from the speciﬁc universal set of features and the rule systems inwhich they function, little of any signiﬁcance remains.
Furthermore, to a greater and greater extent, current work in phonology is
demonstrating that the real richness of phonological systems lies not in thestructural patterns of phonemes but rather in the intricate systems of rules by
7C. L´evi-Strauss, The Savage Mind (Chicago: University of Chicago Press, 1967).
8C. L´evi-Strauss, Structural Anthropology (New York: Basic Books, 1963), p. 85.

--- Page 85 ---

66 Language and Mind
which these patterns are formed, modiﬁed, and elaborated.9The structural pat-
terns that arise at various stages of derivation are a kind of epiphenomenon.
The system of phonological rules makes use of the universal features in a fun-damental way,
10butitisthe properties of the systems of rules, it seems to me,
that really shed light on the speciﬁc nature of the organization of language.Forexample, there appear to be very general conditions, such as the princi-
ple of cyclic ordering (discussed in the preceding lecture) and others that arestill more abstract, that govern the application of these rules, and there aremany interesting and unsolved questions as to how the choice of rules is deter-mined by intrinsic, universal relations among features. Furthermore, the ideaof a mathematical investigation of language structures, to which L´ evi-Strauss
occasionally alludes, becomes meaningful only when one considers systemsof rules with inﬁnite generative capacity. There is nothing to be said aboutthe abstract structure of the various patterns that appear at various stages ofderivation. If this is correct, then one cannot expect structuralist phonology, initself, to provide a useful model for investigation of other cultural and socialsystems.
In general, the problem of extending concepts of linguistic structure to other
cognitive systems seems to me, for the moment, in not too promising a state,although it is no doubt too early for pessimism.
Before turning to the general implications of the study of linguistic compe-
tence and, more speciﬁcally, to the conclusions of universal grammar, it is wellto make sure of the status of these conclusions in the light of current knowledgeof the possible diversity of language. In my ﬁrst lecture, I quoted the remarks ofWilliam Dwight Whitney about what he referred to as “the inﬁnite diversity of
human speech,” the boundless variety that, he maintained, undermines theclaims of philosophical grammar to psychological relevance.
Philosophical grammarians had typically maintained that languages vary lit-
tle in their deep structures, though there may be wide variability in surfacemanifestations. Thus there is, in this view, an underlying structure of grammat-ical relations and categories, and certain aspects of human thought and mentalityare essentially invariant across languages, although languages may differ as towhether they express the grammatical relations formally by inﬂection or wordorder, for example. Furthermore, an investigation of their work indicates that theunderlying recursive principles that generate deep structure were assumed to berestricted in certain ways – for example, by the condition that new structures areformed only by the insertion of new “propositional content,” new structures thatthemselves correspond to actual simple sentences, in ﬁxed positions in already
9See discussion in the preceding lecture and the references cited there.
10The study of universal features is itself in considerable ﬂux. See N. Chomsky and M. Halle, The
Sound Pattern of English (New York: Harper & Row, 1968), Chapter 7, for recent discussion.

--- Page 86 ---

Linguistic contributions: future 67
formed structures. Similarly, the grammatical transformations that form sur-
face structures through reordering, ellipsis, and other formal operations must
themselves meet certain ﬁxed general conditions, such as those discussed in thepreceding lecture. In short, the theories of philosophical grammar, and the morerecent elaborations of these theories, make the assumption that languages willdiffer very little, despite considerable diversity in superﬁcial realization, whenwe discover their deeper structures and unearth their fundamental mechanismsand principles.
It is interesting to observe that this assumption persisted even through the
period of German romanticism, which was, of course, much preoccupied withthe diversity of cultures and with the many rich possibilities for human intel-lectual development. Thus, Wilhelm von Humboldt, who is now best remem-bered for his ideas concerning the variety of languages and the association ofdiverse language structures with divergent “world-views,” nevertheless heldﬁrmly that underlying any human language we will ﬁnd a system that is univer-sal, that simply expresses man’s unique intellectual attributes. For this reason,it was possible for him to maintain the rationalist view that language is notreally learned – certainly not taught – but rather develops “from within,” in anessentially predetermined way, when the appropriate environmental conditionsexist. One cannot really teach a ﬁrst language, he argued, but can only “provide
the thread along which it will develop of its own accord,” by processes morelike maturation than learning. This Platonistic element in Humboldt’s thoughtis a pervasive one; for Humboldt, it was as natural to propose an essentiallyPlatonistic theory of “learning” as it was for Rousseau to found his critique ofrepressive social institutions on a conception of human freedom that derivesfrom strictly Cartesian assumptions regarding the limitations of mechanicalexplanation. And in general it seems appropriate to construe both the psychol-
ogy and the linguistics of the romantic period as in large part a natural outgrowthof rationalist conceptions.
11
The issue raised by Whitney against Humboldt and philosophical grammar
in general is of great signiﬁcance with respect to the implications of linguis-tics for general human psychology. Evidently, these implications can be trulyfar-reaching only if the rationalist view is essentially correct, in which case the
structure of language can truly serve as a “mirror of mind,” in both its partic-ular and its universal aspects. It is widely believed that modern anthropologyhas established the falsity of the assumptions of the rationalist universal gram-marians by demonstrating through empirical study that languages may, in fact,exhibit the widest diversity. Whitney’s claims regarding the diversity of lan-
guages are reiterated throughout the modern period; Martin Joos, for example,
11Forsome discussion of these matters, see my Cartesian Linguistics (New York: Harper & Row,
1966).

--- Page 87 ---

68 Language and Mind
is simply expressing the conventional wisdom when he takes the basic con-
clusion of modern anthropological linguistics to be that “languages can differwithout limit as to either extent or direction.”
12
The belief that anthropological linguistics has demolished the assumptions
of universal grammar seems to me to be quite false in two important respects.First, it misinterprets the views of classical rationalist grammar, which held thatlanguages are similar only at the deeper level, the level at which grammaticalrelations are expressed and at which the processes that provide for the creativeaspect of language use are to be found. Second, this belief seriously misinter-prets the ﬁndings of anthropological linguistics, which has, in fact, restricteditself almost completely to fairly superﬁcial aspects of language structure.
Tosay this is not to criticize anthropological linguistics, a ﬁeld that is faced
with compelling problems of its own – in particular, the problem of obtain-ing at least some record of the rapidly vanishing languages of the primitiveworld. Nevertheless, it is important to bear in mind this fundamental limita-
tion on its achievements in considering the light it can shed on the theses ofuniversal grammar. Anthropological studies (like structural linguistic studiesin general) do not attempt to reveal the underlying core of generative processesin language – that is, the processes that determine the deeper levels of struc-ture and that constitute the systematic means for creating ever novel sentencetypes. Therefore, they obviously cannot have any real bearing on the classicalassumption that these underlying generative processes vary only slightly fromlanguage to language. In fact, what evidence is now available suggests that ifuniversal grammar has serious defects, as indeed it does from a modern pointof view, then these defects lie in the failure to recognize the abstract nature oflinguistic structure and to impose sufﬁciently strong and restrictive conditionson the form of any human language. And a characteristic feature of currentwork in linguistics is its concern for linguistic universals of a sort that can only
be detected through a detailed investigation of particular languages, universalsgoverning properties of language that are simply not accessible to investiga-tion within the restricted framework that has been adopted, often for very goodreasons, within anthropological linguistics.
Ithink that if we contemplate the classical problem of psychology, that of
accounting for human knowledge, we cannot avoid being struck by the enor-mous disparity between knowledge and experience – in the case of language,between the generative grammar that expresses the linguistic competence of the
12M. Joos, ed., Readings in Linguistics ,4th edn. (Chicago: University of Chicago Press, 1966),
p. 228. This is put forth as the “Boas Tradition.” American linguistics, Joos maintains, “got its
decisive direction when it was decided that an indigenous language could be described withoutany preexistent scheme of what a language must b e... ”( p .1 ) .O f course this could not literally
be true – the procedures of analysis themselves express a hypothesis concerning the possiblediversity of language. But there is, nevertheless, much justice in Joos’s characterization.

--- Page 88 ---

Linguistic contributions: future 69
native speaker and the meager and degenerate data on the basis of which he has
constructed this grammar for himself. In principle the theory of learning shoulddeal with this problem; but in fact it bypasses the problem, because of the con-ceptual gap that I mentioned earlier. The problem cannot even be formulatedin any sensible way until we develop the concept of competence, alongside theconcepts of learning and behavior, and apply this concept in some domain. Thefact is that this concept has so far been extensively developed and applied only
in the study of human language. It is only in this domain that we have at least theﬁrst steps toward an account of competence, namely the fragmentary generativegrammars that have been constructed for particular languages. As the study oflanguage progresses, we can expect with some conﬁdence that these grammarswill be extended in scope and depth, although it will hardly come as a surpriseif the ﬁrst proposals are found to be mistaken in fundamental ways.
Insofar as we have a tentative ﬁrst approximation to a generative grammar
for some language, we can for the ﬁrst time formulate in a useful way theproblem of origin of knowledge. In other words, we can ask the question, Whatinitial structure must be attributed to the mind that enables it to construct suchagrammar from the data of sense? Some of the empirical conditions that must
be met by any such assumption about innate structure are moderately clear.Thus, it appears to be a species-speciﬁc capacity that is essentially independentof intelligence, and we can make a fairly good estimate of the amount of datathat is necessary for the task to be successfully accomplished. We know thatthe grammars that are in fact constructed vary only slightly among speakersof the same language, despite wide variations not only in intelligence but alsoin the conditions under which language is acquired. As participants in a certainculture, we are naturally aware of the great differences in ability to use language,in knowledge of vocabulary, and so on that result from differences in nativeability and from differences in conditions of acquisition; we naturally pay muchless attention to the similarities and to common knowledge, which we take forgranted. But if we manage to establish the requisite psychic distance, if weactually compare the generative grammars that must be postulated for differentspeakers of the same language, we ﬁnd that the similarities that we take forgranted are quite marked and that the divergences are few and marginal. Whatis more, it seems that dialects that are superﬁcially quite remote, even barelyintelligible on ﬁrst contact, share a vast central core of common rules andprocesses and differ very slightly in underlying structures, which seem to remaininvariant through long historical eras. Furthermore, we discover a substantialsystem of principles that do not vary among languages that are, as far as weknow, entirely unrelated.
The central problems in this domain are empirical ones that are, in principle
at least, quite straightforward, difﬁcult as they may be to solve in a satisfactoryway.Wemust postulate an innate structure that is rich enough to account for

--- Page 89 ---

70 Language and Mind
the disparity between experience and knowledge, one that can account for the
construction of the empirically justiﬁed generative grammars within the givenlimitations of time and access to data. At the same time, this postulated innatemental structure must not be so rich and restrictive as to exclude certain knownlanguages. There is, in other words, an upper bound and a lower bound on thedegree and exact character of the complexity that can be postulated as innatemental structure. The factual situation is obscure enough to leave room formuch difference of opinion over the true nature of this innate mental structurethat makes acquisition of language possible. However, there seems to me to beno doubt that this is an empirical issue, one that can be resolved by proceedingalong the lines that I have just roughly outlined.
My own estimate of the situation is that the real problem for tomorrow is that
of discovering an assumption regarding innate structure that is sufﬁciently rich,not that of ﬁnding one that is simple or elementary enough to be “plausible.”There is, as far as I can see, no reasonable notion of “plausibility,” no a prioriinsight into what innate structures are permissible, that can guide the searchfor a “sufﬁciently elementary assumption.” It would be mere dogmatism tomaintain without argument or evidence that the mind is simpler in its innatestructure than other biological systems, just as it would be mere dogmatismto insist that the mind’s organization must necessarily follow certain set prin-ciples, determined in advance of investigation and maintained in deﬁance ofany empirical ﬁndings. I think that the study of problems of mind has beenvery deﬁnitely hampered by a kind of apriorism with which these problems
are generally approached. In particular, the empiricist assumptions that havedominated the study of acquisition of knowledge for many years seem to me tohave been adopted quite without warrant and to have no special status amongthe many possibilities that one might imagine as to how the mind functions.
In this connection, it is illuminating to follow the debate that has arisen since
the views I have just sketched were advanced a few years ago as a programof researc h–I should say, since this position was resurrected, because to a
signiﬁcant extent it is the traditional rationalist approach, now ampliﬁed andsharpened and made far more explicit in terms of the tentative conclusions thathave been reached in the recent study of linguistic competence. Two outstand-ing American philosophers, Nelson Goodman and Hilary Putnam, have maderecent contributions to this discussion – both misconceived, in my opinion, butinstructive in the misconceptions that they reveal.
13
13N. Goodman, “The Epistemological Argument,” and H. Putnam, “The Innateness Hypothesis
and Explanatory Models in Linguistics.” Together with a paper of mine, these were pre-sented at the Innate Ideas Symposium of the American Philosophical Association and theBoston Colloquium for the Philosophy of Science in December 1966. The three essays appearinSynth `ese,Vol. 17, No. 1, 1967, pp. 2–28, and in R. S. Cohen and W. M. Wartofsky,
eds., Boston Studies in the Philosophy of Science ,Vol. III (New York: Humanities, 1968),

--- Page 90 ---

Linguistic contributions: future 71
Goodman’s treatment of the question suffers ﬁrst from an historical misun-
derstanding and second from a failure to formulate correctly the exact nature
of the problem of acquisition of knowledge. His historical misunderstandinghas to do with the issue between Locke and whomever Locke thought he wascriticizing in his discussion of innate ideas. According to Goodman, “Lockemade . . . acutely clear” that the doctrine of innate ideas is “false or meaningless.”In fact, however, Locke’s critique had little relevance to any familiar doctrineof the seventeenth century. The arguments that Locke gave were consideredand dealt with in quite a satisfactory way in the earliest seventeenth-centurydiscussions of innate ideas, for example those of Lord Herbert and Descartes,both of whom took for granted that the system of innate ideas and principleswould not function unless appropriate stimulation took place. For this reason,
Locke’s arguments, none of which took cognizance of this condition, are with-out force;
14for some reason, he avoided the issues that had been discussed in
the preceding half-century. Furthermore, as Leibnitz observed, Locke’s will-ingness to make use of a principle of “reﬂection” makes it almost impossibleto distinguish his approach from that of the rationalists, except for his failureto take even those steps suggested by his predecessors toward specifying thecharacter of this principle.
But, historical issues aside, I think that Goodman misconstrues the sub-
stantive problem as well. He argues that ﬁrst-language learning poses no realproblem, because prior to ﬁrst-language learning the child has already acquiredthe rudiments of a symbolic system in his ordinary dealings with the environ-ment. Hence, ﬁrst-language learning is analogous to second-language learningin that the fundamental step has already been taken, and details can be elabo-rated within an already existing framework. This argument might have someforce if it were possible to show that the speciﬁc properties of grammar – say,the distinction of deep and surface structure, the speciﬁc properties of grammat-ical transformations, the principles of rule ordering, and so on – were presentin some form in these already acquired prelinguistic “symbolic systems.” Butsince there is not the slightest reason to believe that this is so, the argument
pp. 81–107. A more extensive discussion of the papers of Putnam and Goodman, along with a
number of others, appears in my contribution to the symposium “Linguistics and Philosophy,”New York University, April 1968, in S. Hook, ed., Philosophy and Language (New York: New
York University Press, 1969). The essay is reprinted in this volume.
14This observation is a commonplace. See, for example, the commentary by A. C. Fraser in hisedition of Locke’s Essay Concerning Human Understanding ,1894 (reprinted by Dover, 1959),
notes 1 and 2, Chapter 1 (p. 38 of the Dover edition). As Fraser notes, Descartes’ position isone “which Locke’s argument always fails to reach . . . Locke assails [the hypothesis of innateideas] . . . in its crudest form, in which it is countenanced by no eminent advocate.” Goodman isfree to use the term “innate idea” in conformity with Locke’s misinterpretation of the doctrine ifhe wishes, but not to charge “sophistry,” as he does, when others examine and develop rationalistdoctrine in the form in which it was actually presented.

--- Page 91 ---

72 Language and Mind
collapses. It is based on an equivocation similar to that discussed earlier in con-
nection with the argument that language evolved from animal communication.In that case, as we observed, the argument turned on a metaphorical use of theterm “language.” In Goodman’s case, the argument is based entirely on a vagueuse of the term “symbolic system,” and it collapses as soon as we attempt to givethis term a precise meaning. If it were possible to show that these prelinguisticsymbolic systems share certain signiﬁcant properties with natural language,we could then argue that these properties of natural language are acquired byanalogy. Of course, we would then face the problem of explaining how theprelinguistic symbolic systems developed these properties. But since no onehas succeeded in showing that the fundamental properties of natural language –those discussed in Lecture 2, for example – appear in prelinguistic symbolicsystems or any others, the latter problem does not arise.
According to Goodman, the reason why the problem of second-language
learning is different from that of ﬁrst-language learning is that “once one lan-guage is available,” it “can be used for giving explanation and instruction.” Hethen goes on to argue that “acquisition of an initial language is acquisition of asecondary symbolic system” and is quite on a par with normal second-languageacquisition. The primary symbolic systems to which he refers are “rudimentaryprelinguistic symbolic systems in which gestures and sensory and perceptualoccurrences of all sorts function as signs.” But evidently these prelinguisticsymbolic systems cannot be “used for giving explanation and instruction” inthe way a ﬁrst language can be used in second-language instruction. Therefore,even on his own grounds, Goodman’s argument is incoherent.
Goodman maintains that “the claim we are discussing cannot be experimen-
tally tested even when we have an acknowledged example of a ‘bad’ language”and that “the claim has not even been formulated to the extent of citation of asingle general property of ‘bad’ languages.” The ﬁrst of these conclusions iscorrect, in his sense of “experimental test,” namely a test in which we “take aninfant at birth, isolate it from all the inﬂuences of our language-bound culture,and attempt to inculcate it with one of the ‘bad’ artiﬁcial languages.” Obviouslythis is not feasible. But there is no reason why we should be dismayed by theimpossibility of carrying out such a test as this. There are many other ways – forexample, those discussed in Lecture 2 and the references cited there – in which
evidence can be obtained concerning the properties of grammars and conclu-
sions regarding the general properties of such grammars can be put to empiricaltest. Any such conclusion immediately speciﬁes, correctly or incorrectly, cer-tain properties of “bad” languages. Since there are dozens of papers and booksthat attempt to formulate such properties, his second claim, that not “a singlegeneral property of ‘bad’ languages” has been formulated, is rather surprising.One might try to show that these attempts are misguided or questionable, butone can hardly maintain seriously that they do not exist. Any formulation of

--- Page 92 ---

Linguistic contributions: future 73
aprinciple of universal grammar makes a strong empirical claim, which can
be falsiﬁed by ﬁnding counter-instances in some human language, along the
lines of the discussion in Lecture 2. In linguistics, as in any other ﬁeld, it isonly in such indirect ways as this that one can hope to ﬁnd evidence bearing onnontrivial hypotheses. Direct experimental tests of the sort that Goodman men-tions are rarely possible, a matter that may be unfortunate but is neverthelesscharacteristic of most research.
At one point Goodman remarks, correctly, that even though “for certain
remarkable facts I have no alternative explanatio n... that alone does not dic-
tate acceptance of whatever theory may be offered; for the theory might beworse than none. Inability to explain a fact does not condemn me to accept
an intrinsically repugnant and incomprehensible theory.” But now consider thetheory of innate ideas that Goodman regards as “intrinsically repugnant andincomprehensible.” Notice, ﬁrst, that the theory is obviously not “incompre-hensible,” on his terms. Thus he appears to be willing, in this article, to acceptthe view that in some sense the mature mind contains ideas; it is obviously not“incomprehensible,” then, that some of these ideas are “implanted in the mindas original equipment,” to use his phraseology. And if we turn to the actual doc-trine as developed in rationalist philosophy, rather than Locke’s caricature, thetheory becomes even more obviously comprehensible. There is nothing incom-prehensible in the view that stimulation provides the occasion for the mind toapply certain innate interpretive principles, certain concepts that proceed from“the power of understanding” itself, from the faculty of thinking rather thanfrom external objects directly. To take an example from Descartes ( Reply to
Objections ,V):
When ﬁrst in infancy we see a triangular ﬁgure depicted on paper, this ﬁgure cannot
show us how a real triangle ought to be conceived, in the way in which geometriciansconsider it, because the true triangle is contained in this ﬁgure, just as the statue ofMercury is contained in a rough block of wood. But because we already possess withinus the idea of a true triangle, and it can be more easily conceived by our mind than themore complex ﬁgure of the triangle drawn on paper, we, therefore, when we see thecomposite ﬁgure, apprehend not it itself, but rather the authentic triangle.
15
In this sense the idea of a triangle is innate. Surely the notion is comprehensible;
there would be no difﬁculty, for example, in programing a computer to reactto stimuli along these lines (though this would not satisfy Descartes, for otherreasons). Similarly, there is no difﬁculty in principle in programing a com-puter with a schematism that sharply restricts the form of a generative gram-mar, with an evaluation procedure for grammars of the given form, with a
15E. S. Haldane and G. R. T. Ross, eds., Descartes’ Philosophical Works ,1911 (reprinted by
Dover, 1955). The citation, and the preceding remarks, appear in my contribution to the Innate
Ideas Symposium of December 1966 (see note 13).

--- Page 93 ---

74 Language and Mind
technique for determining whether given data are compatible with a grammar
of the given form, with a ﬁxed substructure of entities (such as distinctive fea-tures), rules, and principles, and so on – in short, with a universal grammarof the sort that has been proposed in recent years. For reasons that I havealready mentioned, I believe that these proposals can be properly regarded as afurther development of classical rationalist doctrine, as an elaboration of someof its main ideas regarding language and mind. Of course, such a theory will be“repugnant” to one who accepts empiricist doctrine and regards it as immuneto question or challenge. It seems to me that this is the heart of the matter.
Putnam’s paper (see note 13)deals more directly with the points at issue,
butitseems to me that his arguments are also inconclusive, because of certain
incorrect assumptions that he makes about the nature of the acquired grammars.Putnam assumes that on the level of phonetics the only property proposed inuniversal grammar is that a language has “a short list of phonemes.” This, heargues, is not a similarity among languages that requires elaborate explanatoryhypotheses. The conclusion is correct; the assumption is quite wrong. In fact, as Ihave now pointed out several times, very strong empirical hypotheses have beenproposed regarding the speciﬁc choice of universal features, conditions on theform and organization of phonological rules, conditions on rule application, andso on. If these proposals are correct or near correct, then “similarities amonglanguages” at the level of sound structure are indeed remarkable and cannotbe accounted for simply by assumptions about memory capacity, as Putnamsuggests.
Above the level of sound structure, Putnam assumes that the only signiﬁ-
cant properties of language are that they have proper names, that the grammarcontains a phrase structure component, and that there are rules “abbreviating”sentences generated by the phrase structure component. He argues that thenature of the phrase structure component is determined by the existence ofproper names; that the existence of a phrase structure component is explainedby the fact that “all the natural measures of complexity of an algorithm – sizeof the machine table, length of computations, time, and space required forthe computation – lead to th e... result”; that phrase structure systems pro-
vide the “algorithms which are ‘simplest’ for virtually any computing system,”hence also “for naturally evolved ‘computing systems’”; and that there is noth-ing surprising in the fact that languages contain rules of abbreviation.
Each of the three conclusions involves a false assumption. From the fact that a
phrase structure system contains proper names one can conclude almost nothingabout its other categories. In fact, there is much dispute at the moment aboutthe general properties of the underlying phrase structure system for naturallanguages; the dispute is not in the least resolved by the existence of propernames.

--- Page 94 ---

Linguistic contributions: future 75
As to the second point, it is simply untrue that all measures of complexity and
speed of computation lead to phrase structure rules as the “simplest possible
algorithm.” The only existing results that are even indirectly relevant show thatcontext-free phrase structure grammars (a reasonable model for rules generat-ing deep structures, when we exclude the lexical items and the distributionalconditions they meet) receive an automata-theoretic interpretation as nondeter-ministic pushdown storage automata, but the latter is hardly a “natural” notionfrom the point of view of “simplicity of algorithms” and so forth. In fact, itcan be argued that the somewhat similar but not formally related concept ofreal-time deterministic automation is far more “natural” in terms of time andspace conditions on computation.
16
However, it is pointless to pursue this topic, because what is at stake is not the
“simplicity” of phrase structure grammars but rather of transformational gram-mars with a phrase structure component that plays a role in generating deepstructures. And there is absolutely no mathematical concept of “ease of compu-tation” or “simplicity of algorithm” that even vaguely suggests that such systemsmay have some advantage over the kinds of automata that have been seriouslyinvestigated from this point of view – for example, ﬁnite state automata, linearbounded automata, and so on. The basic concept of “structure-dependent oper-ation” has never even been considered in a strictly mathematical concept. Thesource of this confusion is a misconception on Putnam’s part as to the nature ofgrammatical transformations. They are not rules that “abbreviate” sentences;rather, they are operations that form surface structures from underlying deepstructures, in such ways as are illustrated in the preceding lecture and the ref-erences there cited.
17Hence, to show that transformational grammars are the
“simplest possible,” one would have to demonstrate that the “optimal” comput-ing system would take a string of symbols as input and determine its surfacestructure, its underlying deep structure, and the sequence of transformationaloperations that relates them. Nothing of the sort has been shown; in fact, thequestion has never even been raised.
Putnam argues that even if signiﬁcant uniformities among languages were
to be discovered, there would be a simpler explanation than the hypothesis ofan innate universal grammar, namely their common origin. But this proposal
16Forsome discussion of these matters, see my “Formal Properties of Grammars,” in R. D. Luce,
R. Bush, and E. Galanter, eds., Handbook of Mathematical Psychology ,Vol. II (New York:
Wiley, 1963). For a more extensive discussion of the automata-theoretic framework, see R. J.
Nelson, Introduction to Automata (New York: Wiley, 1968). A detailed presentation of properties
of context-free grammars is given in S. Ginsburg, The Mathematical Theory of Context-Free
Languages (New York: McGraw-Hill, 1966). There have been a number of studies of speed of
computation, simplicity of algorithms, and so on, but none of them has any bearing on the issue
under discussion.
17See note 10 of Lecture 2, p. 29, for further comment.

--- Page 95 ---

76 Language and Mind
involves a serious misunderstanding of the problem at issue. The grammar of
alanguage must be discovered by the child from the data presented to him. As
noted earlier, the empirical problem is to ﬁnd a hypothesis about initial structurerich enough to account for the fact that a speciﬁc grammar is constructed bythe child, but not so rich as to be falsiﬁed by the known diversity of language.Questions of common origin are of potential relevance to this empirical issuein only one respect: if the existing languages are not a “fair sample” of the“possible languages,” we may be led mistakenly to propose too narrow a schemafor universal grammar. However, as I mentioned earlier, the empirical problemthat we face today is that no one has been able to devise an initial hypothesis richenough to account for the acquisition by the child of the grammar that we are,apparently, led to attribute to him when we try to account for his ability to usethe language in the normal way. The assumption of common origin contributesnothing to explaining how this achievement is possible. In short, the languageis “reinvented” each time it is learned, and the empirical problem to be facedby the theory of learning is how this invention of grammar can take place.
Putnam does face this problem and suggests that there might be “general mul-
tipurpose learning strategies” that account for this achievement. It is, of course,an empirical question whether the properties of the “language faculty” are spe-ciﬁc to language or are merely a particular case of much more general mentalfaculties (or learning strategies). This is a problem that has been discussed ear-
lier in this lecture, inconclusively and in a slightly different context. Putnamtakes for granted that it is only general “learning strategies” that are innate butsuggests no grounds for this empirical assumption. As I have argued earlier,anondogmatic approach to this problem can be pursued, without reliance on
unargued assumptions of this sort – that is, through the investigation of spe-ciﬁc areas of human competence, such as language, followed by the attempt todevise a hypothesis that will account for the development of this competence. Ifwe discover through such investigation that the same “learning strategies” aresufﬁcient to account for the development of competence in various domains, wewill have reason to believe that Putnam’s assumption is correct. If we discoverthat the postulated innate structures differ from case to case, the only rationalconclusion would be that a model of mind must involve separate “faculties,”with unique or partially unique properties. I cannot see how anyone can res-olutely insist on one or the other conclusion in the light of the evidence nowavailable to us. But one thing is quite clear: Putnam has no justiﬁcation for
his ﬁnal conclusion, that “invoking ‘Innateness’ only postpones the problem oflearning; it does not solve it.” Invoking an innate representation of universalgrammar does solve the problem of learning, if it is true that this is the basis forlanguage acquisition, as it well may be. If, on the other hand, there are generallearning strategies that account for the acquisition of grammatical knowledge,then postulation of an innate universal grammar will not “postpone” the problem

--- Page 96 ---

Linguistic contributions: future 77
of learning, but will rather offer an incorrect solution to this problem. The issue
is an empirical one of truth or falsity, not a methodological one of states ofinvestigation.
18
Tosummarize, it seems to me that neither Goodman nor Putnam offers a
serious counterargument to the proposals concerning innate mental structurethat have been advanced (tentatively, of course, as beﬁts empirical hypotheses)or suggests a plausible alternative approach, with empirical content, to theproblem of acquisition of knowledge.
Assuming the rough accuracy of conclusions that seem tenable today, it is
reasonable to suppose that a generative grammar is a system of many hundredsof rules of several different types, organized in accordance with certain ﬁxedprinciples of ordering and applicability and containing a certain ﬁxed substruc-ture which, along with the general principles of organization, is common to alllanguages. There is no a priori “naturalness” to such a system, any more thanthere is to the detailed structure of the visual cortex. No one who has given anyserious thought to the problem of formalizing inductive procedures or “heuris-tic methods” is likely to set much store by the hope that such a system as agenerative grammar can be constructed by methods of any generality.
Tomy knowledge, the only substantive proposal to deal with the problem of
acquisition of knowledge of language is the rationalist conception that I haveoutlined. To repeat: suppose that we assign to the mind, as an innate property,the general theory of language that we have called “universal grammar.” Thistheory encompasses the principles that I discussed in the preceding lecture andmany others of the same sort, and it speciﬁes a certain subsystem of rules thatprovides a skeletal structure for any language and a variety of conditions, formaland substantive, that any further elaboration of the grammar must meet. Thetheory of universal grammar, then, provides a schema to which any particulargrammar must conform. Suppose, furthermore, that we can make this schemasufﬁciently restrictive so that very few possible grammars conforming to theschema will be consistent with the meager and degenerate data actually availableto the language learner. His task, then, is to search among the possible grammarsand select one that is not deﬁnitely rejected by the data available to him. Whatfaces the language learner, under these assumptions, is not the impossible task
of inventing a highly abstract and intricately structured theory on the basis ofdegenerate data, but rather the much more manageable task of determiningwhether these data belong to one or another of a fairly restricted set of potentiallanguages.
18It is surprising to see that Putnam refers disparagingly to “vague talk of ‘classes of hypotheses’ –
and ‘weighting functions’” in the course of his discussion of “general learning strategies.” Forthe moment, the latter is a mere phrase without any describable content. On the other hand, thereis a substantial literature detailing the properties of the classes of hypotheses and weightingfunctions to which Putnam refers. Hence, the shoe seems to be on the other foot in this case.

--- Page 97 ---

78 Language and Mind
The tasks of the psychologist, then, divide into several subtasks. The ﬁrst
is to discover the innate schema that characterizes the class of potential lan-
guages – that deﬁnes the “essence” of human language. This subtask falls tothat branch of human psychology known as linguistics; it is the problem oftraditional universal grammar, of contemporary linguistic theory. The secondsubtask is the detailed study of the actual character of the stimulation and theorganism–environment interaction that sets the innate cognitive mechanism intooperation. This is a study now being undertaken by a few psychologists, and itis particularly active right here in Berkeley. It has already led to interesting andsuggestive conclusions. One might hope that such study will reveal a successionof maturational stages leading ﬁnally to a full generative grammar.
19
Athird task is that of determining just what it means for a hypothesis about
the generative grammar of a language to be “consistent” with the data of sense.Notice that it is a great oversimpliﬁcation to suppose that a child must discoveragenerative grammar that accounts for all the linguistic data that has been
presented to him and that “projects” such data to an inﬁnite range of potentialsound–meaning relations. In addition to achieving this, he must also differen-tiate the data of sense into those utterances that give direct evidence as to thecharacter of the underlying grammar and those that must be rejected by thehypothesis he selects as ill-formed, deviant, fragmentary, and so on. Clearly,everyone succeeds in carrying out this task of differentiation – we all know,
within tolerable limits of consistency, which sentences are well formed andliterally interpretable, and which must be interpreted as metaphorical, frag-mentary, and deviant along many possible dimensions. I doubt that it has beenfully appreciated to what extent this complicates the problem of accounting forlanguage acquisition. Formally speaking, the learner must select a hypothesisregarding the language to which he is exposed that rejects a good part of thedata on which this hypothesis must rest. Again, it is reasonable to suppose thisis possible only if the range of tenable hypotheses is quite limited – if the innateschema of universal grammar is highly restrictive. The third subtask, then, is tostudy what we might think of as the problem of “conﬁrmation” – in this context,the problem of what relation must hold between a potential grammar and a set
19It is not unlikely that detailed investigation of this sort will show that the conception of universal
grammar as an innate schematism is only valid as a ﬁrst approximation; that, in fact, an innateschematism of a more general sort permits the formulation of tentative “grammars” whichthemselves determine how later evidence is to be interpreted, leading to the postulation of richergrammars, and so on. I have so far been discussing language acquisition on the obviously falseassumption that it is an instantaneous process. There are many interesting questions that arisewhen we consider how the process extends in time. For some discussion relating to problems ofphonology, see my paper “Phonology and Reading,” in H. Levin, ed., Basic Studies on Reading.
Notice also that it is unnecessary to suppose, even in the ﬁrst approximation, that “very fewpossible grammars conforming to the schema” will be available to the language learner. It isenough to suppose that the possible grammars consistent with the data will be “scattered” interms of an evaluation procedure.

--- Page 98 ---

Linguistic contributions: future 79
of data for this grammar to be conﬁrmed as the actual theory of the language
in question.
Ihavebeen describing the problem of acquisition of knowledge of language
in terms that are more familiar in an epistemological than a psychologicalcontext, but I think that this is quite appropriate. Formally speaking, acquisitionof “common-sense knowledge” – knowledge of a language, for example – isnot unlike theory construction of the most abstract sort. Speculating about thefuture development of the subject, it seems to me not unlikely, for the reasons Ihave mentioned, that learning theory will progress by establishing the innatelydetermined set of possible hypotheses, determining the conditions of interactionthat lead the mind to put forth hypotheses from this set, and ﬁxing the conditionsunder which such a hypothesis is conﬁrmed – and, perhaps, under which muchof the data is rejected as irrelevant for one reason or another.
Such a way of describing the situation should not be too surprising to those
familiar with the history of psychology at Berkeley, where, after all, Edward
Tolman has given his name to the psychology building; but I want to stress
that the hypotheses I am discussing are qualitatively different in complexityand intricacy from anything that was considered in the classical discussionsof learning. As I have now emphasized several times, there seems to be littleuseful analogy between the theory of grammar that a person has internalized andthat provides the basis for his normal, creative use of language, and any othercognitive system that has so far been isolated and described; similarly, there islittle useful analogy between the schema of universal grammar that we must, Ibelieve, assign to the mind as an innate character, and any other known systemof mental organization. It is quite possible that the lack of analogy testiﬁes toour ignorance of other aspects of mental function, rather than to the absoluteuniqueness of linguistic structure; but the fact is that we have, for the moment,no objective reason for supposing this to be true.
The way in which I have been describing acquisition of knowledge of lan-
guage calls to mind a very interesting and rather neglected lecture given byCharles Sanders Peirce more than ﬁfty years ago, in which he developed somerather similar notions about acquisition of knowledge in general.
20Peirce argued
that the general limits of human intelligence are much more narrow than mightbe suggested by romantic assumptions about the limitless perfectibility of man(or, for that matter, than are suggested by his own “pragmaticist” conceptionsof the course of scientiﬁc progress in his better-known philosophical studies).He held that innate limitations on admissible hypotheses are a precondition forsuccessful theory construction, and that the “guessing instinct” that provideshypotheses makes use of inductive procedures only for “corrective action.”
20C. S. Peirce, “The Logic of Abduction,” in V . Tomas, ed., Peirce’s Essays in the Philosophy of
Science (New York: Liberal Arts Press, 1957).

--- Page 99 ---

80 Language and Mind
Peirce maintained in this lecture that the history of early science shows that
something approximating a correct theory was discovered with remarkable easeand rapidity, on the basis of highly inadequate data, as soon as certain problemswere faced; he noted “how few were the guesses that men of surpassing geniushad to make before they rightly guessed the laws of nature.” And, he asked,“How was it that man was ever led to entertain that true theory? You cannot saythat it happened by chance, because the chances are too overwhelmingly againstthe single true theory in the twenty or thirty thousand years during which manhas been a thinking animal, ever having come into any man’s head.” A fortiori,the chances are even more overwhelmingly against the true theory of each lan-guage ever having come into the head of every four-year-old child. Continuingwith Peirce: “Man’s mind has a natural adaptation to imagining correct theoriesof some kinds . . . If man had not the gift of a mind adapted to his requirements,he could not have acquired any knowledge.” Correspondingly, in our presentcase, it seems that knowledge of a language – a grammar – can be acquired onlyby an organism that is “preset” with a severe restriction on the form of gram-mar. This innate restriction is a precondition, in the Kantian sense, for linguisticexperience, and it appears to be the critical factor in determining the course and
result of language learning. The child cannot know at birth which languagehe is to learn, but he must know that its grammar must be of a predeterminedform that excludes many imaginable languages. Having selected a permissiblehypothesis, he can use inductive evidence for corrective action, conﬁrming ordisconﬁrming his choice. Once the hypothesis is sufﬁciently well conﬁrmed,the child knows the language deﬁned by this hypothesis; consequently, hisknowledge extends enormously beyond his experience and, in fact, leads himto characterize much of the data of experience as defective and deviant.
Peirce regarded inductive processes as rather marginal to the acquisition of
knowledge; in his words, “Induction has no originality in it, but only tests asuggestion already made.” To understand how knowledge is acquired, in therationalist view that Peirce outlined, we must penetrate the mysteries of what hecalled “abduction,” and we must discover that which “gives a rule to abductionand so puts a limit upon admissible hypotheses.” Peirce maintained that thesearch for principles of abduction leads us to the study of innate ideas, whichprovide the instinctive structure of human intelligence. But Peirce was no dual-ist in the Cartesian sense; he argued (not very persuasively, in my opinion)that there is a signiﬁcant analogy between human intelligence, with its abduc-tive restrictions, and animal instinct. Thus, he maintained that man discoveredcertain true theories only because his “instincts must have involved from thebeginning certain tendencies to think truly” about certain speciﬁc matters; sim-ilarly, “You cannot seriously think that every little chicken that is hatched, hasto rummage through all possible theories until it lights upon the good idea ofpicking up something and eating it. On the contrary, you think that the chicken

--- Page 100 ---

Linguistic contributions: future 81
has an innate idea of doing this; that is to say, that it can think of this, but has
no faculty of thinking anything else . . . But if you are going to think every poorchicken endowed with an innate tendency towards a positive truth, why shouldyou think to man alone this gift is denied?”
No one took up Peirce’s challenge to develop a theory of abduction, to deter-
mine those principles that limit the admissible hypotheses or present them in acertain order. Even today, this remains a task for the future. It is a task that neednot be undertaken if empiricist psychological doctrine can be substantiated;therefore, it is of great importance to subject this doctrine to rational analysis,as has been done, in part, in the study of language. I would like to repeat thatit was the great merit of structural linguistics, as of Hullian learning theory inits early stages and of several other modern developments, to have given pre-cise form to certain empiricist assumptions.
21Where this step has been taken,
the inadequacy of the postulated mechanisms has been clearly demonstrated,and, in the case of language at least, we can even begin to see just why anymethods of this sort must fail – for example, because they cannot, in principle,provide for the properties of deep structures and the abstract operations of for-mal grammar. Speculating about the future, I think it is not unlikely that thedogmatic character of the general empiricist framework and its inadequacy to
21In contrast, the account of language acquisition presented by B. F. Skinner in his Verbal Behavior
(New York: Appleton-Century-Crofts, 1957) seems to me either devoid of content or clearly
wrong, depending on whether one interprets it metaphorically or literally (see my review ofthis book in Language ,Vol. 35, No. 1, 1959, pp. 26–58). It is quite appropriate when a theory
is disproven in a strong form to replace it by a weaker variant. However, not infrequently thisstep leads to vacuity. The popularity of Skinner’s concept of “reinforcement,” after the virtualcollapse of Hullian theory, seems to me a case in point. (Note that the Skinnerian concepts canbe well deﬁned and can lead to interesting results, in a particular experimental situation – whatis at issue is the Skinnerian “extrapolation” to a wider class of cases.)
Another example appears in K. Salzinger, “The Problem of Response Class in Verbal
Behavior,” in K. Salzinger and S. Salzinger, eds., Research in Verbal Behavior and Some Neuro-
physiological Implications (New York: Academic Press, 1967), pp. 35–54. Salzinger argues that
George Miller is not justiﬁed in criticizing learning theory for its inability to explain linguisticproductivity – that is, the ability of a speaker to determine, of a sequence of words that he hasnever heard, whether or not it is a well-formed sentence and what it means. The defect can beovercome, he argues, by making use of the notion of “response class.” True, it cannot be that
each response is reinforced, but the class of acceptable sentences constitutes a response class,like the set of bar-presses in a particular Skinnerian experiment. Unfortunately, this is emptyverbiage until the condition that deﬁnes membership in this class is established. If the condition
involves the notion “generation by a given grammar,” then we are back where we started.
Salzinger also misconstrues the attempts to provide an experimental test that will distinguish
grammatical from ungrammatical strings. He states that such tests have failed to conﬁrm such adivision and therefore concludes, apparently, that the distinction does not exist. Obviously, thefailure indicates nothing more than that the tests were ineffective. One can invent innumerable
tests that would fail to provide some given classiﬁcation. Surely the classiﬁcation itself is notin question. Thus, Salzinger would agree, quite apart from any experimental test that might bedevised, that the sentences of this footnote share an important property that does not hold of theset of strings of words formed by reading each of these sentences, word by word, from right toleft.

--- Page 101 ---

82 Language and Mind
human and animal intelligence will gradually become more evident as speciﬁc
realizations, such as taxonomic linguistics, behaviorist learning theory, and theperception models,
22heuristic methods, and “general problem solvers” of the
early enthusiasts of “artiﬁcial intelligence,” are successively rejected on empir-ical grounds when they are made precise and on grounds of vacuity when theyare left vague. And – assuming this projection to be accurate – it will then bepossible to undertake a general study of the limits and capacities of humanintelligence, to develop a Peircean logic of abduction.
Modern psychology is not devoid of such initiatives. The contemporary study
of generative grammar and its universal substructure and governing principlesis one such manifestation. Closely related is the study of the biological bases ofhuman language, an investigation to which Eric Lenneberg has made substantialcontributions.
23It is tempting to see a parallel development in the very important
work of Piaget and others interested in “genetic epistemology,” but I am not
sure that this is accurate. It is not clear to me, for example, what Piaget takes tobe the basis for the transition from one of the stages that he discusses to the next,higher stage. There is, furthermore, a possibility, suggested by recent work ofMehler and Bever,
24that the deservedly well-known results on conservation, in
particular, may not demonstrate successive stages of intellectual development inthe sense discussed by Piaget and his coworkers, but something rather different.If the preliminary results of Mehler and Bever are correct, then it would followthat the “ﬁnal stage,” in which conservation is properly understood, was alreadyrealized at a very early period of development. Later, the child develops aheuristic technique that is largely adequate but that fails under the conditions ofthe conservation experiment. Still later, he adjusts this technique successfullyand once again makes the correct judgments in the conservation experiment.If this analysis is correct, then what we are observing is not a succession ofstages of intellectual development, in Piaget’s sense, but rather slow progressin bringing heuristic techniques into line with general concepts that have alwaysbeen present. These are interesting alternatives; either way, the results may bearin important ways on the topics we are considering.
Still more clearly to the point, I think, are the developments in comparative
ethology over the past thirty years, and certain current work in experimentaland physiological psychology. One can cite many examples: for example, in the
22Foradiscussion of such systems and their limitations, see M. Minsky and S. Papert, Perceptions
and Pattern Recognition ,Artiﬁcial Intelligence Memo No. 140, MAC-M-358, Project MAC,
Cambridge, Mass., September 1967.
23See E. H. Lenneberg, Biological Foundations of Language (New York: Wiley, 1967). My con-
tribution to this volume, “The Formal Nature of Language,” appears as the ﬁfth paper in this
book.
24See J. Mehler and T. G. Bever, “Cognitive Capacities of Young Children,” Science ,Vol. 158,
No. 3797, October 1967, pp. 141–42.

--- Page 102 ---

Linguistic contributions: future 83
latter category, the work of Bower suggesting an innate basis for the perceptual
constancies; studies in the Wisconsin primate laboratory on complex innatereleasing mechanisms in rhesus monkeys; the work of Hubel, Barlow, andothers on highly speciﬁc analyzing mechanisms in the lower cortical centersof mammals; and a number of comparable studies of lower organisms (forexample, the beautiful work of Lettvin and his associates on frog vision). There
is now good evidence from such investigations that perception of line, angle,motion, and other complex properties of the physical world is based on innateorganization of the neural system.
In some cases at least, these built-in structures will degenerate unless appro-
priate stimulation takes place at an early stage in life, but although such expe-rience is necessary to permit the innate mechanisms to function, there is noreason to believe that it has more than a marginal effect on determining how
they function to organize experience. Furthermore, there is nothing to suggestthat what has so far been discovered is anywhere near the limit of complexityof innate structures. The basic techniques for exploring the neural mechanismsare only a few years old, and it is impossible to predict what order of speci-ﬁcity and complexity will be demonstrated when they come to be extensivelyapplied. For the present, it seems that most complex organisms have highlyspeciﬁc forms of sensory and perceptual organization that are associated withtheUmwelt and the manner of life of the organism. There is little reason to
doubt that what is true of lower organisms is true of humans as well. Partic-ularly in the case of language, it is natural to expect a close relation betweeninnate properties of the mind and features of linguistic structure; for language,after all, has no existence apart from its mental representation. Whatever prop-erties it has must be those that are given to it by the innate mental processesof the organism that has invented it and that invents it anew with each suc-ceeding generation, along with whatever properties are associated with theconditions of its use. Once again, it seems that language should be, for this rea-son, a most illuminating probe with which to explore the organization of mentalprocesses.
Turning to comparative ethology, it is interesting to note that one of its
earliest motivations was the hope that through the “investigation of the a priori,of the innate working hypotheses present in subhuman organisms,” it would bepossible to shed light on the a priori forms of human thought. This formulationof intent is quoted from an early and little-known paper by Konrad Lorenz.
25
Lorenz goes on to express views very much like those Peirce had expressed ageneration earlier. He maintains:
25K. Lorenz, “Kants Lehre vom apriorischen in Lichte gegenw¨ artiger Biologie,” in Bl¨atter f ¨ur
Deutsche Philosophie ,Vol. 15, 1941, pp. 94–125. I am indebted to Donald Walker of the MITRE
Corporation, Bedford, Mass., for bringing this paper to my attention.

--- Page 103 ---

84 Language and Mind
One familiar with the innate modes of reaction of subhuman organisms can read-
ily hypothesize that the a priori is due to hereditary differentiations of the centralnervous system which have become characteristic of the species, producing heredi-tary dispositions to think in certain forms . . . Most certainly Hume was wrong when hewanted to derive all that is a priori from that which the senses supply to experience, just
as wrong as Wundt or Helmholtz who simply explain it as an abstraction from precedingexperience. Adaptation of the a priori to the real world has no more originated from
“experience” than adaptation of the ﬁn of the ﬁsh to the properties of water. Just as theform of the ﬁn is given a priori, prior to any individual negotiation of the young ﬁshwith the water, and just as it is this form that makes possible this negotiation, so it isalso the case with our forms of perception and categories in their relationship to ournegotiation with the real external world through experience. In the case of animals, weﬁnd limitations speciﬁc to the forms of experience possible for them. We believe wecan demonstrate the closest functional and probably genetic relationship between theseanimal a prioris and our human a priori. Contrary to Hume, we believe, just as did Kant,that a “pure” science of innate forms of human thought, independent of all experience,is possible.
Peirce, to my knowledge, is original and unique in stressing the problem of
studying the rules that limit the class of possible theories. Of course, his conceptof abduction, like Lorenz’s biological a priori, has a strongly Kantian ﬂavor, andall derive from the rationalist psychology that concerned itself with the forms,the limits, and the principles that provide “the sinews and connections” forhuman thought, that underlie “that inﬁnite amount of knowledge of which weare not always conscious,” of which Leibnitz spoke. It is therefore quite naturalthat we should link these developments to the revival of philosophical grammar,which grew from the same soil as an attempt, quite fruitful and legitimate, toexplore one basic facet of human intelligence.
In recent discussion, models and observations derived from ethology have
frequently been cited as providing biological support, or at least analogue, tonew approaches to the study of human intelligence. I cite these comments ofLorenz’s mainly in order to show that this reference does not distort the outlookof at least some of the founders of this domain of comparative psychology.
One word of caution is necessary in referring to Lorenz, now that he has
been discovered by Robert Ardrey and Joseph Alsop and popularized as aprophet of doom. It seems to me that Lorenz’s views on human aggressionhave been extended to near absurdity by some of his expositors. It is no doubttrue that there are innate tendencies in the human psychic constitution thatlead to aggressiveness under speciﬁc social and cultural conditions. But thereis little reason to suppose that these tendencies are so dominant as to leaveus forever tottering on the brink of a Hobbesian war of all against all – as,incidentally, Lorenz at least is fully aware, if I read him rightly. Skepticism iscertainly in order when a doctrine of man’s “inherent aggressiveness” comes tothe surface in a society that gloriﬁes competitiveness, in a civilization that has

--- Page 104 ---

Linguistic contributions: future 85
been distinguished by the brutality of the attack that it has mounted against less
fortunate peoples. It is fair to ask to what extent the enthusiasm for this curiousview of man’s nature is attributable to fact and logic and to what extent it merelyreﬂects the limited extent to which the general cultural level has advanced sincethe days when Clive and the Portuguese explorers taught the meaning of truesavagery to the inferior races that stood in their way.
In any event, I would not want what I am saying to be confused with other,
entirely different attempts to revive a theory of human instinct. What seemsto me important in ethology is its attempt to explore the innate properties thatdetermine how knowledge is acquired and the character of this knowledge.Returning to this theme, we must consider a further question: how did thehuman mind come to acquire the innate structure that we are led to attribute toit? Not too surprisingly, Lorenz takes the position that this is simply a matterof natural selection. Peirce offers a rather different speculation, arguing that“nature fecundates the mind of man with ideas which, when these ideas growup, will resemble their father, Nature.” Man is “provided with certain naturalbeliefs that are true” because “certain uniformities . . . prevail throughout theuniverse, and the reasoning mind is [it]self a product of this universe. Thesesame laws are thus, by logical necessity, incorporated in his own being.” Here, itseems clear that Peirce’s argument is entirely without force and that it offers littleimprovement over the preestablished harmony that it was presumably intendedto replace. The fact that the mind is a product of natural laws does not implythat it is equipped to understand these laws or to arrive at them by “abduction.”There would be no difﬁculty in designing a device (say, programing a computer)that is a product of natural law, but that, given data, will arrive at any arbitraryabsurd theory to “explain” these data.
In fact, the processes by which the human mind achieved its present stage of
complexity and its particular form of innate organization are a total mystery, asmuch so as the analogous questions about the physical or mental organizationof any other complex organism. It is perfectly safe to attribute this develop-ment to “natural selection,” so long as we realize that there is no substance tothis assertion, that it amounts to nothing more than a belief that there is somenaturalistic explanation for these phenomena. The problem of accounting forevolutionary development is, in some ways, rather like that of explaining suc-
cessful abduction. The laws that determine possible successful mutation andthe nature of complex organisms are as unknown as the laws that determinethe choice of hypotheses.
26With no knowledge of the laws that determine the
26It has been argued on statistical grounds – through comparison of the known rate of mutation
with the astronomical number of imaginable modiﬁcations of chromosomes and their parts – thatsuch laws must exist and must vastly restrict the realizable possibilities. See the papers by Eden,Sch¨utzenberger, and Gavadan in Mathematical Challenges to the Neo-Darwinian Interpretation
of Evolution ,Wistar Symposium Monograph No. 5, 1967.

--- Page 105 ---

86 Language and Mind
organization and structure of complex biological systems, it is just as senseless
to ask what the “probability” is for the human mind to have reached its presentstate as it is to inquire into the “probability” that a particular physical theory willbe devised. And, as we have noted, it is idle to speculate about laws of learninguntil we have some indication of what kind of knowledge is attainable – inthe case of language, some indication of the constraints on the set of potentialgrammars.
In studying the evolution of mind, we cannot guess to what extent there are
physically possible alternatives to, say, transformational generative grammar,for an organism meeting certain other physical conditions characteristic ofhumans. Conceivably, there are none – or very few – in which case talk aboutevolution of the language capacity is beside the point. The vacuity of such
speculation, however, has no bearing one way or another on those aspects ofthe problem of mind that can be sensibly pursued. It seems to me that theseaspects are, for the moment, the problems illustrated in the case of language bythe study of the nature, the use, and the acquisition of linguistic competence.
There is one ﬁnal issue that deserves a word of comment. I have been using
mentalistic terminology quite freely, but entirely without prejudice as to thequestion of what may be the physical realization of the abstract mechanismspostulated to account for the phenomena of behavior or the acquisition of knowl-edge. We are not constrained, as was Descartes, to postulate a second substancewhen we deal with phenomena that are not expressible in terms of matter inmotion, in his sense. Nor is there much point in pursuing the question of psy-chophysical parallelism, in this connection. It is an interesting question whetherthe functioning and evolution of human mentality can be accommodated withinthe framework of physical explanation, as presently conceived, or whether thereare new principles, now unknown, that must be invoked, perhaps principles thatemerge only at higher levels of organization than can now be submitted to phys-ical investigation. We can, however, be fairly sure that there will be a physicalexplanation for the phenomena in question, if they can be explained at all, for
an uninteresting terminological reason, namely that the concept of “physicalexplanation” will no doubt be extended to incorporate whatever is discovered
in this domain, exactly as it was extended to accommodate gravitational andelectromagnetic force, massless particles, and numerous other entities and pro-cesses that would have offended the common sense of earlier generations. Butit seems clear that this issue need not delay the study of the topics that are nowopen to investigation, and it seems futile to speculate about matters so remotefrom present understanding.
Ihavetried to suggest that the study of language may very well, as was tra-
ditionally supposed, provide a remarkably favorable perspective for the studyof human mental processes. The creative aspect of language use, when inves-tigated with care and respect for the facts, shows that current notions of habit

--- Page 106 ---

Linguistic contributions: future 87
and generalization, as determinants of behavior or knowledge, are quite inade-
quate. The abstractness of linguistic structure reinforces this conclusion, and itsuggests further that in both perception and learning the mind plays an activerole in determining the character of the acquired knowledge. The empiricalstudy of linguistic universals has led to the formulation of highly restrictiveand, I believe, quite plausible hypotheses concerning the possible variety ofhuman languages, hypotheses that contribute to the attempt to develop a theoryof acquisition of knowledge that gives due place to intrinsic mental activity. Itseems to me, then, that the study of language should occupy a central place ingeneral psychology.
Surely the classical questions of language and mind receive no ﬁnal solution,
or even the hint of a ﬁnal solution, from the work that is being actively pursuedtoday. Nevertheless, these problems can be formulated in new ways and seenin a new light. For the ﬁrst time in many years, it seems to me, there is somereal opportunity for substantial progress in the study of the contribution of themind to perception and the innate basis for acquisition of knowledge. Still, inmany respects, we have not made the ﬁrst approach to a real answer to theclassical problems. For example, the central problems relating to the creativeaspect of language use remain as inaccessible as they have always been. Andthe study of universal semantics, surely crucial to the full investigation of lan-guage structure, has barely advanced since the medieval period. Many othercritical areas might be mentioned where progress has been slow or nonexis-tent. Real progress has been made in the study of the mechanisms of language,the formal principles that make possible the creative aspect of language useand that determine the phonetic form and semantic content of utterances. Ourunderstanding of these mechanisms, though only fragmentary, does seem to meto have real implications for the study of human psychology. By pursuing thekinds of research that now seem feasible and by focusing attention on certainproblems that are now accessible to study, we may be able to spell out in somedetail the elaborate and abstract computations that determine, in part, the natureof percepts and the character of the knowledge that we can acquire – the highlyspeciﬁc ways of interpreting phenomena that are, in large measure, beyond ourconsciousness and control and that may be unique to man.

--- Page 107 ---

4F orm and meaning in natural languages
When we study human language, we are approaching what some might call
the “human essence,” the distinctive qualities of mind that are, so far as weknow, unique to man and that are inseparable from any critical phase of humanexistence, personal or social. Hence the fascination of this study, and, no less,
its frustration. The frustration arises from the fact that despite much progress,we remain as incapable as ever before of coming to grips with the core problemof human language, which I take to be this: having mastered a language, oneis able to understand an indeﬁnite number of expressions that are new to one’sexperience, that bear no simple physical resemblance and are in no simple way
analogous to the expressions that constitute one’s linguistic experience; and oneis able, with greater or less facility, to produce such expressions on an appro-priate occasion, despite their novelty and independently of detectable stimulusconﬁgurations, and to be understood by others who share this still mysteriousability. The normal use of language is, in this sense, a creative activity. This cre-ative aspect of normal language use is one fundamental factor that distinguisheshuman language from any known system of animal communication.
It is important to bear in mind that the creation of linguistic expressions that
are novel but appropriate is the normal mode of language use. If some individualwere to restrict himself largely to a deﬁnite set of linguistic patterns, to a set ofhabitual responses to stimulus conﬁgurations, or to “analogies” in the sense ofmodern linguistics, we would regard him as mentally defective, as being lesshuman than animal. He would immediately be set apart from normal humans byhis inability to understand normal discourse, or to take part in it in the normalway–the normal way being innovative, free from control by external stimuli,
and appropriate to new and ever changing situations.
It is not a novel insight that human speech is distinguished by these quali-
ties, though it is an insight that must be recaptured time and time again. Witheach advance in our understanding of the mechanisms of language, thought, andbehavior, comes a tendency to believe that we have found the key to understand-ing man’s apparently unique qualities of mind. These advances are real, but anhonest appraisal will show, I think, that they are far from providing such a key.Wedo not understand, and, for all we know, we may never come to understand
88

--- Page 108 ---

Form and meaning in natural languages 89
what makes it possible for a normal human intelligence to use language as an
instrument for the free expression of thought and feeling; or, for that matter,what qualities of mind are involved in the creative acts of intelligence that arecharacteristic, not unique and exceptional, in a truly human existence.
Ithink that this is an important fact to stress, not only for linguists and psy-
chologists whose research centers on these issues, but, even more, for those whohope to learn something useful in their own work and thinking from researchinto language and thought. It is particularly important that the limitations ofunderstanding be clear to those involved in teaching, in the universities, andeven more important, in the schools. There are strong pressures to make use
of new educational technology and to design curriculum and teaching methodsin the light of the latest scientiﬁc advances. In itself, this is not objectionable.It is important, nevertheless, to remain alert to a very real danger: that newknowledge and technique will deﬁne the nature of what is taught and how itis taught, rather than contribute to the realization of educational goals that areset on other grounds and in other terms. Let me be concrete. Technique andeven technology is available for rapid and efﬁcient inculcation of skilled behav-
ior, in language teaching, teaching of arithmetic, and other domains. There is,consequently, a real temptation to reconstruct curriculum in the terms deﬁnedby the new technology. And it is not too difﬁcult to invent a rationale, mak-ing use of the concepts of “controlling behavior,” enhancing skills, and so on.Nor is it difﬁcult to construct objective tests that are sure to demonstrate theeffectiveness of such methods in reaching certain goals that are incorporatedin these tests. But successes of this sort will not demonstrate that an impor-tant educational goal has been achieved. They will not demonstrate that it isimportant to concentrate on developing skilled behavior in the student. Whatlittle we know about human intelligence would at least suggest something quitedifferent: that by diminishing the range and complexity of materials presentedto the inquiring mind, by setting behavior in ﬁxed patterns, these methods mayharm and distort the normal development of creative abilities. I do not wantto dwell on the matter. I am sure that any of you will be able to ﬁnd exam-ples from your own experience. It is perfectly proper to try to exploit genuineadvances in knowledge, and within some given ﬁeld of study, it is inevitable,and quite proper, that research should be directed by considerations of feasi-bility as well as considerations of ultimate signiﬁcance. It is also highly likely,if not inevitable, that considerations of feasibility and signiﬁcance will lead indivergent paths. For those who wish to apply the achievements of one disciplineto the problems of another, it is important to make very clear the exact naturenot only of what has been achieved, but equally important, the limitations ofwhat has been achieved.
Imentioned a moment ago that the creative aspect of normal use of language
is not a new discovery. It provides one important pillar for Descartes’ theory

--- Page 109 ---

90 Language and Mind
of mind, for his study of the limits of mechanical explanation. The latter, in
turn, provides one crucial element in the construction of the anti-authoritariansocial and political philosophy of the Enlightenment. And, in fact, there wereeven some efforts to found a theory of artistic creativity on the creative aspect
of normal language use. Schlegel, for example, argues that poetry has a uniqueposition among the arts, a fact illustrated, he claims, by the use of the term“poetical” to refer to the element of creative imagination in any artistic effort,as distinct, say, from the term “musical,” which would be used metaphoricallyto refer to a sensual element. To explain this asymmetry, he observes that everymode of artistic expression makes use of a certain medium and that the mediumof poetry – language – is unique in that language, as an expression of the humanmind rather than a product of nature, is boundless in scope and is constructedon the basis of a recursive principle that permits each creation to serve as thebasis for a new creative act. Hence the central position among the arts of theart forms whose medium is language.
The belief that language, with its inherent creative aspect, is a unique human
possession did not go unchallenged, of course. One expositor of Cartesianphilosophy, Antoine Le Grand, refers to the opinion “of some people of the EastIndies, who think that Apes and Baboons, which are with them in great numbers,are imbued with understanding, and that they can speak but will not for fear theyshould be employed, and set to work.” If there is a more serious argument insupport of the claim that human language capacity is shared with other primates,then I am unaware of it. In fact, whatever evidence we do have seems to meto support the view that the ability to acquire and use language is a species-speciﬁc human capacity, that there are very deep and restrictive principles thatdetermine the nature of human language and are rooted in the speciﬁc characterof the human mind. Obviously arguments bearing on this hypothesis cannotbe deﬁnitive or conclusive, but it appears to me, nevertheless, that even in thepresent stage of our knowledge, the evidence is not inconsiderable.
There are any number of questions that might lead one to undertake a study
of language. Personally, I am primarily intrigued by the possibility of learn-ing something, from the study of language, that will bring to light inherentproperties of the human mind. We cannot now say anything particularly infor-mative about the normal creative use of language in itself. But I think thatwe are slowly coming to understand the mechanisms that make possible thiscreative use of language, the use of language as an instrument of free thoughtand expression. Speaking again from a personal point of view, to me the mostinteresting aspects of contemporary work in grammar are the attempts to formu-late principles of organization of language which, it is proposed, are universalreﬂections of properties of mind; and the attempt to show that on this assump-tion, certain facts about particular languages can be explained. Viewed in thisway,linguistics is simply a part of human psychology: the ﬁeld that seeks to

--- Page 110 ---

Form and meaning in natural languages 91
determine the nature of human mental capacities and to study how these capaci-
ties are put to work. Many psychologists would reject a characterization of theirdiscipline in these terms, but this reaction seems to me to indicate a serious inad-equacy in their conception of psychology, rather than a defect in the formulationitself. In any event, it seems to me that these are proper terms in which to setthe goals of contemporary linguistics, and to discuss its achievements and itsfailings.
Ithink it is now possible to make some fairly deﬁnite proposals about the
organization of human language and to put them to empirical test. The theoryof transformational-generative grammar, as it is evolving along diverse andsometimes conﬂicting paths, has put forth such proposals; and there has been,in the past few years, some very productive and suggestive work that attemptsto reﬁne and reconstruct these formulations of the processes and structures thatunderlie human language.
The theory of grammar is concerned with the question, What is the nature of
aperson’s knowledge of his language, the knowledge that enables him to make
use of language in the normal, creative fashion? A person who knows a languagehas mastered a system of rules that assigns sound and meaning in a deﬁnite wayfor an inﬁnite class of possible sentences. Each language thus consists (in part)of a certain pairing of sound and meaning over an inﬁnite domain. Of course,the person who knows the language has no consciousness of having masteredthese rules or of putting them to use, nor is there any reason to suppose that thisknowledge of the rules of language can be brought to consciousness. Throughintrospection, a person may accumulate various kinds of evidence about thesound–meaning relation determined by the rules of the language that he hasmastered; there is no reason to suppose that he can go much beyond this surfacelevel of data so as to discover, through introspection, the underlying rules andprinciples that determine the relation of sound and meaning. Rather, to discoverthese rules and principles is a typical problem of science. We have a collectionof data regarding sound–meaning correspondence, the form and interpretationof linguistic expressions, in various languages. We try to determine, for eachlanguage, a system of rules that will account for such data. More deeply, we tryto establish the principles that govern the formation of such systems of rulesfor any human language.
The system of rules that speciﬁes the sound–meaning relation for a given
language can be called the “grammar” – or, to use a more technical term, the“generative grammar” – of this language. To say that a grammar “generates”acertain set of structures is simply to say that it speciﬁes this set in a precise
way.In this sense, we may say that the grammar of a language generates an
inﬁnite set of “structural descriptions,” each structural description being anabstract object of some sort that determines a particular sound, a particularmeaning, and whatever formal properties and conﬁgurations serve to mediate

--- Page 111 ---

92 Language and Mind
the relation between sound and meaning. For example, the grammar of English
generates structural descriptions for the sentences I am now speaking; or, totake a simpler case for purposes of illustration, the grammar of English wouldgenerate a structural description for each of these sentences:
1 John is certain that Bill will leave.
2 John is certain to leave.
Each of us has mastered and internally represented a system of grammar that
assigns structural descriptions to these sentences; we use this knowledge, totallywithout awareness or even the possibility of awareness, in producing these sen-tences or understanding them when they are produced by others. The structuraldescriptions include a phonetic representation of the sentences and a speciﬁca-tion of their meaning. In the case of the cited examples 1and2,the structural
descriptions must convey roughly the following information: they must indi-cate that in the case of 1,agiven psychological state (namely, being certain that
Bill will leave) is attributed to John; whereas in the case of 2,agiven logical
property (namely, the property of being certain) is attributed to the propositionthat John will leave. Despite the superﬁcial similarity of form of these two sen-tences, the structural descriptions generated by the grammar must indicate thattheir meanings are very different: one attributes a psychological state to John,the other attributes a logical property to an abstract proposition. The secondsentence might be paraphrased in a very different form:
3 That John will leave is certain.
Forthe ﬁrst there is no such paraphrase. In the paraphrase 3the “logical form”
of2is expressed more directly, one might say. The grammatical relations in 2
and3are very similar, despite the difference of surface form; the grammatical
relations in 1and2are very different, despite the similarity of surface form. Such
facts as these provide the starting point for an investigation of the grammatical
structure of English – and more generally, for the investigation of the general
properties of human language.
Tocarry the discussion of properties of language further, let me introduce the
term “surface structure” to refer to a representation of the phrases that consti-tute a linguistic expression and the categories to which these phrases belong. Insentence 1,the phrases of the surface structure include: “that Bill will leave,”
which is a full proposition; the noun phrases “Bill” and “John”; the verb phrases“will leave” and “is certain that Bill will leave,” and so on. In sentence 2,
the surface structure includes the verb phrases “to leave” and “is certain toleave”; but the surface structure of 2includes no proposition of the form “John
will leave,” even though this proposition expresses part of the meaning of“John is certain to leave,” and appears as a phrase in the surface structure

--- Page 112 ---

Form and meaning in natural languages 93
of its paraphrase, “that John will leave is certain.” In this sense, surface
structure does not necessarily provide an accurate indication of the struc-tures and relations that determine the meaning of a sentence; in the case ofsentence 2,“John is certain to leave,” the surface structure fails to indicate
that the proposition “John will leave” expresses a part of the meaning ofthe sentence – although in the other two examples that I gave the surface struc-ture comes rather close to indicating the semantically signiﬁcant relations.
Continuing, let me introduce the further technical term “deep structure” to
refer to a representation of the phrases that play a more central role in thesemantic interpretation of a sentence. In the case of 1and3,the deep structure
might not be very different from the surface structure. In the case of 2,the deep
structure will be very different from the surface structure, in that it will includesome such proposition as “John will leave” and the predicate “is certain” appliedto this proposition, though nothing of the sort appears in the surface structure.In general, apart from the simplest examples, the surface structures of sentencesare very different from their deep structures.
The grammar of English will generate, for each sentence, a deep structure, and
will contain rules showing how this deep structure is related to a surface struc-ture. The rules expressing the relation of deep and surface structure are called“grammatical transformations.” Hence the term “transformational-generativegrammar.” In addition to rules deﬁning deep structures, surface structures, andthe relation between them, the grammar of English contains further rules thatrelate these “syntactic objects” (namely, paired deep and surface structures) tophonetic representations on the one hand, and to representations of meaningon the other. A person who has acquired knowledge of English has internal-ized these rules and makes use of them when he understands or produces thesentences just given as examples, and an indeﬁnite range of others.
Evidence in support of this approach is provided by the observation that
interesting properties of English sentences can be explained directly in termsof the deep structures assigned to them. Thus consider once again the twosentences 1(“John is certain that Bill will leave”) and 2(“John is certain
to leave”). Recall that in the case of the ﬁrst, the deep structure and surfacestructure are virtually identical, whereas in the case of the second, they are verydifferent. Observe also that in the case of the ﬁrst, there is a correspondingnominal phrase, namely, “John’s certainty that Bill will leave (surprised me)”;butinthe case of the second, there is no corresponding nominal phrase. We
cannot say “John’s certainty to leave surprised me.” The latter nominal phraseis intelligible, I suppose, but it is not well formed in English. The speaker ofEnglish can easily make himself aware of this fact, though the reason for it willvery likely escape him. This fact is a special case of a very general property of
English: namely, nominal phrases exist corresponding to sentences that are veryclose in surface form to deep structure, but not corresponding to such sentences

--- Page 113 ---

94 Language and Mind
that are remote in surface form from deep structure. Thus “John is certain that
Bill will leave,” being close in surface form to its deep structure, correspondsto the nominal phrase “John’s certainty that Bill will leave”; but there is nosuch phrase as “John’s certainty to leave” corresponding to “John is certain toleave,” which is remote from its deep structure.
The notions of “closeness” and “remoteness” can be made quite precise.
When we have made them precise, we have an explanation for the fact thatnominalizations exist in certain cases but not in others – though were they toexist in these other cases, they would often be perfectly intelligible. The expla-
nation turns on the notion of deep structure: in effect, it states that nominaliza-tions must reﬂect the properties of deep structure. There are many examplesthat illustrate this phenomenon. What is important is the evidence it providesin support of the view that deep structures which are often quite abstract existand play a central role in the grammatical processes that we use in produc-ing and interpreting sentences. Such facts, then, support the hypothesis thatdeep structures of the sort postulated in transformational-generative grammarare real mental structures. These deep structures, along with the transforma-tion rules that relate them to surface structure and the rules relating deep andsurface structures to representations of sound and meaning, are the rules thathave been mastered by the person who has learned a language. They consti-tute his knowledge of the language; they are put to use when he speaks andunderstands.
The examples I have given so far illustrate the role of deep structure in deter-
mining meaning, and show that even in very simple cases, the deep structuremay be remote from the surface form. There is a great deal of evidence indicat-ing that the phonetic form of a sentence is determined by its surface structure,by principles of an extremely interesting and intricate sort that I will not tryto discuss here. From such evidence it is fair to conclude that surface struc-ture determines phonetic form, and that the grammatical relations representedin deep structure are those that determine meaning. Furthermore, as alreadynoted, there are certain grammatical processes, such as the process of nominal-ization, that can be stated only in terms of abstract deep structures.
The situation is complicated, however, by the fact that surface structure also
plays a role in determining semantic interpretation.
1The study of this question
is one of the most controversial aspects of current work, and, in my opinion,likely to be one of the most fruitful. As an illustration, consider some of theproperties of the present perfect aspect in English – for example, such sentencesas “John has lived in Princeton.” An interesting and rarely noted feature of this
1Idiscuss this matter in some detail in “Deep Structure and Semantic Interpretation,” in
R. Jakobson and S. Kawamoto, eds., Studies in General and Oriental Linguistics ,commem-
orative volume for Shiro Hattori, TEC Corporation for Language and Educational Research,
Tokyo, 1970.

--- Page 114 ---

Form and meaning in natural languages 95
aspect is that in such cases it carries the presupposition that the subject is alive.
Thus it is proper for me to say “I have lived in Princeton” but, knowing thatEinstein is dead, I would not say “Einstein has lived in Princeton.” Rather, Iwould say “Einstein lived in Princeton.” (As always, there are complications,
butthis is accurate as a ﬁrst approximation.) But now consider active and passive
forms with present perfect aspect. Knowing that John is dead and Bill alive, Ican say “Bill has often been visited by John, but not “John has often visitedBill”; rather, “John often visited Bill.” I can say “I have been taught physics byEinstein” but not “Einstein has taught me physics”; rather, “Einstein taught mephysics.” In general, active and passive are synonymous and have essentially thesame deep structures. But in these cases, active and passive forms differ in thepresuppositions they express; put simply, the presupposition is that the persondenoted by the surface subject is alive. In this respect, the surface structurecontributes to the meaning of the sentence in that it is relevant to determiningwhat is presupposed in the use of a sentence.
Carrying the matter further, observe that the situation is different when we
have a conjoined subject. Thus given that Hilary is alive and Marco Polo dead,it is proper to say “Hilary has climbed Mt. Everest” but not “Marco Polo hasclimbed Mt. Everest”; rather, again, “Marco Polo climbed Mt. Everest.” (Again,Ioverlook certain subtleties and complications.) But now consider the sentence
“Marco Polo and Hilary (among others) have climbed Mt. Everest.” In this case,there is no expressed presupposition that Marco Polo is alive, as there is nonein the passive “Mt. Everest has been climbed by Marco Polo (among others).”
Notice further that the situation changes considerably when we shift from
the normal intonation, as in the cases I have just given, to an intonation contourthat contains a contrastive or expressive stress. The effect of such intonationon presupposition is fairly complex. Let me illustrate with a simple case. Con-sider the sentence “The Yankees played the Red Sox in Boston.” With normalintonation, the point of main stress and highest pitch is the word “Boston” andthe sentence might be an answer to such questions as “where did the Yankeesplay the Red Sox?” (“in Boston”); “what did the Yankees do?” (“they playedthe Red Sox in Boston”); “what happened?” (“the Yankees played the Red Soxin Boston”). But suppose that contrastive stress is placed on “Red Sox,” so thatwe have “The Yankees played the red sox in Boston.” Now, the sentence can
be the answer only to “Who did the Yankees play in Boston?” Note that thesentence presupposes that the Yankees played someone in Boston; if there wasno game at all, it is improper, not just false, to say “The Yankees played thered sox in Boston.” In contrast, if there was no game at all, it is false, but not
improper, to say “The Yankees played the Red Sox in Boston,” with normalintonation. Thus contrastive stress carries a presupposition in a sense in whichnormal intonation does not, though normal intonation also carries a presuppo-sition in another sense; thus it would be improper to answer the question “Who

--- Page 115 ---

96 Language and Mind
played the Red Sox in Boston?” with “The Yankees played the Red Sox in
Boston” (normal intonation). The same property of contrastive stress is shownby the so-called cleft sentence construction. Thus the sentence “It was theyank eeswho played the Red Sox in Boston” has primary stress on “Yankees,”
and presupposes that someone played the Red Sox in Boston. The sentence isimproper, not just false, if there was no game at all. These phenomena havegenerally been overlooked when the semantic role of contrastive stress has beennoted.
Tofurther illustrate the role of surface structure in determining meaning,
consider such sentences as this: “John is tall for a pygmy.” This sentence pre-supposes that John is a pygmy, and that pygmies tend to be short; hence givenour knowledge of the Watusi, it would be anomalous to say “John is tall for aWatusi.” On the other hand, consider what happens when we insert the word
“even” in the sentence. Inserting it before “John” we derive: “Even John is tallfor a pygmy.” Again, the presupposition is that John is a pygmy and that pyg-mies are short. But consider: “John is tall even for a pygmy.” This presupposesthat pygmies are tall; it is therefore a strange sentence, given our knowledge ofthe facts, as compared, say, to “John is tall even for a Watusi,” which is quiteall right. The point is that the position of “even” in the sentence “John is tallfor a pygmy” determines the presupposition with respect to the average heightof pygmies.
But the placement of the word “even” is a matter of surface structure. We can
see this from the fact that the word “even” can appear in association with phrasesthat do not have any representation at the level of deep structure: Consider, forexample, the sentence “John isn’t certain to leave at 10; in fact, he isn’t even
certain to leave at all.” Here, the word “even” is associated with “certain toleave,” a phrase which, as noted earlier, does not appear at the level of deepstructure. Hence in this case as well properties of surface structure play a rolein determining what is presupposed by a certain sentence.
The role of surface structure in determining meaning is illustrated once again
by the phenomenon of pronominalization.
2Thus if I say “Each of the men hates
his brothers,” the word “his” may refer to one of the men; but if I say “The meneach hate his brothers,” the word “his” must refer to some other person, nototherwise referred to in the sentence. However, the evidence is strong that “eachof the men” and “the men each” derive from the same deep structure. Similarly,it has been noted that placement of stress plays an important role in determiningpronominal reference. Consider the following discourse: “John washed the car;Iwas afraid someone else would do it.” The sentence implies that I hoped
2The examples that follow are due to Ray Dougherty, Adrian Akmajian, and Ray Jackendoff. See
my article in Jakobson and Kawamoto, eds., Studies in General and Oriental Linguistics ,for
references.

--- Page 116 ---

Form and meaning in natural languages 97
that John would wash the car, and I’m happy that he did. But now consider the
following: “John washed the car; I was afraid someone else would do it.” With
stress on “afraid,” the sentence implies that I hoped that John would not washthe car. The reference of “someone else” is different in the two cases. There aremany other examples that illustrate the role of surface structure in determiningpronominal reference.
Tocomplicate matters still further, deep structure too plays a role in determin-
ing pronominal reference. Thus consider the sentence “John appeared to Billto like him.” Here, the pronoun “him” may refer to Bill but not John. Compare“John appealed to Bill to like him.” Here, the pronoun may refer to John butnot Bill. Thus we can say “John appealed to Mary to like him,” but not “Johnappeared to Mary to like him,” where “him” refers to “John”; on the other hand,we can say “John appeared to Mary to like her,” but not “John appealed to Maryto like her,” where “her” refers to Mary. Similarly, in “John appealed to Billto like himself,” the reﬂexive refers to Bill; but in “John appeared to Bill tolike himself,” it refers to John. These sentences are approximately the samein surface structure; it is the differences in deep structure that determine thepronominal reference.
Hence pronominal reference depends on both deep and surface structure. A
person who knows English has mastered a system of rules which make use ofproperties of deep and surface structure in determining pronominal reference.Again, he cannot discover these rules by introspection. In fact, these rules arestill unknown, though some of their properties are clear.
Tosummarize: the generative grammar of a language speciﬁes an inﬁnite
set of structural descriptions, each of which contains a deep structure, a sur-face structure, a phonetic representation, a semantic representation, and other
formal structures. The rules relating deep and surface structure – the so-called“grammatical transformations” – have been investigated in some detail, andare fairly well understood. The rules that relate surface structure and phoneticrepresentation are also reasonably well understood (though I do not want toimply that the matter is beyond dispute: far from it). It seems that both deepand surface structure enter into the determination of meaning. Deep structureprovides the grammatical relations of predication, modiﬁcation, and so on, thatenter into the determination of meaning. On the other hand, it appears thatmatters of focus and presupposition, topic and comment, the scope of logicalelements, and pronominal reference are determined, in part at least, by surfacestructure. The rules that relate syntactic structures to representations of meaningare not at all well understood. In fact, the notion “representation of meaning” or“semantic representation” is itself highly controversial. It is not clear at all thatit is possible to distinguish sharply between the contribution of grammar to thedetermination of meaning, and the contribution of so-called “pragmatic con-siderations,” questions of fact and belief and context of utterance. It is perhaps

--- Page 117 ---

98 Language and Mind
worth mentioning that rather similar questions can be raised about the notion
“phonetic representation.” Although the latter is one of the best-established and
least controversial notions of linguistic theory, we can, nevertheless, raise thequestion whether or not it is a legitimate abstraction, whether a deeper under-standing of the use of language might not show that factors that go beyondgrammatical structure enter into the determination of perceptual representationsand physical form in an inextricable fashion, and cannot be separated, withoutdistortion, from the formal rules that interpret surface structure as phoneticform.
So far, the study of language has progressed on the basis of a certain
abstraction: namely, we abstract away from conditions of use of language andconsider formal structures and the formal operations that relate them. Amongthese formal structures are those of syntax, namely, deep and surface structures;and also the phonetic and semantic representations, which we take to be certainformal objects related to syntactic structures by certain well-deﬁned operations.This process of abstraction is in no way illegitimate, but one must understandthat it expresses a point of view, a hypothesis about the nature of mind, that isnot a priori obvious. It expresses the working hypothesis that we can proceedwith the study of “knowledge of language” – what is often called “linguisticcompetence” – in abstraction from the problems of how language is used. Theworking hypothesis is justiﬁed by the success that is achieved when it is adopted.
Agreat deal has been learned about the mechanisms of language, and, I would
say, about the nature of mind, on the basis of this hypothesis. But we must beaware that in part, at least, this approach to language is forced upon us by the
fact that our concepts fail us when we try to study the use of language. We are
reduced to platitudes, or to observations which, though perhaps quite interest-ing, do not lend themselves to systematic study by means of the intellectualtools presently available to us. On the other hand, we can bring to the study offormal structures and their relations a wealth of experience and understanding.It may be that at this point we are facing a problem of conﬂict between signiﬁ-cance and feasibility, a conﬂict of the sort that I mentioned earlier in this paper.Idonot believe that this is the case, but it is possible. I feel fairly conﬁdent that
the abstraction to the study of formal mechanisms of language is appropriate;my conﬁdence arises from the fact that many quite elegant results have beenachieved on the basis of this abstraction. Still, caution is in order. It may be thatthe next great advance in the study of language will require the forging of newintellectual tools that permit us to bring into consideration a variety of ques-tions that have been cast into the waste-bin of “pragmatics,” so that we couldproceed to study questions that we know how to formulate in an intelligiblefashion.
As noted, I think that the abstraction to linguistic competence is legitimate.
Togo further, I believe that the inability of modern psychology to come to

--- Page 118 ---

Form and meaning in natural languages 99
grips with the problems of human intelligence is in part, at least, a result of its
unwillingness to undertake the study of abstract structures and mechanisms ofmind. Notice that the approach to linguistic structure that I have been outlininghas a highly traditional ﬂavor to it. I think it is no distortion to say that thisapproach makes precise a point of view that was inherent in the very importantwork of the seventeenth- and eighteenth-century universal grammarians, and
that was developed, in various ways, in rationalist and romantic philosophy oflanguage and mind. The approach deviates in many ways from a more modern,and in my opinion quite erroneous, conception that knowledge of languagecan be accounted for as a system of habits, or in terms of stimulus-responseconnections, principles of “analogy” and “generalization,” and other notionsthat have been explored in twentieth-century linguistics and psychology, andthat develop from traditional empiricist speculation. The fatal inadequacy ofall such approaches, I believe, results from their unwillingness to undertakethe abstract study of linguistic competence. Had the physical sciences limitedthemselves by similar methodological strictures, we would still be in the era ofBabylonian astronomy.
One traditional concept that has reemerged in current work is that of
“universal grammar,” and I want to conclude by saying just a word about thistopic. There are two kinds of evidence suggesting that deep-seated formal con-ditions are satisﬁed by the grammars of all languages. The ﬁrst kind of evidenceis provided by the study of a wide range of languages. In attempting to con-struct generative grammars for languages of widely varied kinds, investigatorshave repeatedly been led to rather similar assumptions as to the form and orga-nization of such generative systems. But a more persuasive kind of evidencebearing on universal grammar is provided by the study of a single language.It may at ﬁrst seem paradoxical that the intensive study of a single languageshould provide evidence regarding universal grammar, but a little thought aboutthe matter shows that this is a very natural consequence.
Tosee this, consider the problem of determining the mental capacities that
make language acquisition possible. If the study of grammar – of linguisticcompetence – involves an abstraction from language use, then the study of themental capacities that make acquisition of grammar possible involves a further,second-order abstraction. I see no fault in this. We may formulate the problemof determining the intrinsic characteristics of a device of unknown propertiesthat accepts as “input” the kind of data available to the child learning his ﬁrstlanguage, and produces as “output” the generative grammar of that language.The “output,” in this case, is the internally represented grammar, mastery ofwhich constitutes knowledge of the language. If we undertake to study the intrin-sic structure of a language-acquisition device without dogma or prejudice, wearrive at conclusions which, though of course only tentative, still seem to meboth signiﬁcant and reasonably well-founded. We must attribute to this device

--- Page 119 ---

100 Language and Mind
enough structure so that the grammar can be constructed within the empirically
given constraints of time and available data, and we must meet the empiricalcondition that different speakers of the same language, with somewhat differ-ent experience and training, nevertheless acquire grammars that are remarkablysimilar, as we can determine from the ease with which they communicate andthe correspondences among them in the interpretation of new sentences. It isimmediately obvious that the data available to the child is quite limited – thenumber of seconds in his lifetime is trivially small as compared with the rangeof sentences that he can immediately understand and can produce in the appro-priate manner. Having some knowledge of the characteristics of the acquiredgrammars and the limitations on the available data, we can formulate quitereasonable and fairly strong empirical hypotheses regarding the internal struc-ture of the language-acquisition device that constructs the postulated grammarsfrom the given data. When we study this question in detail, we are, I believe,led to attribute to the device a very rich system of constraints on the form of apossible grammar; otherwise, it is impossible to explain how children come toconstruct grammars of the kind that seem empirically adequate under the givenconditions of time and access to data. But if we assume, furthermore, that chil-dren are not genetically predisposed to learn one rather than another language,then the conclusions we reach regarding the language-acquisition device areconclusions regarding universal grammar. These conclusions can be falsiﬁedby showing that they fail to account for the construction of grammars of otherlanguages, for example. And these conclusions are further veriﬁed if they serveto explain facts about other languages. This line of argument seems to me veryreasonable in a general way, and when pursued in detail it leads us to strongempirical hypotheses concerning universal grammar, even from the study of aparticular language.
Ihavediscussed an approach to the study of language that takes this study to
be a branch of theoretical human psychology. Its goal is to exhibit and clarify themental capacities that make it possible for a human to learn and use a language.As far as we know, these capacities are unique to man, and have no signiﬁcantanalogue in any other organism. If the conclusions of this research are anywherenear correct, then humans must be endowed with a very rich and explicit set ofmental attributes that determine a speciﬁc form of language on the basis of veryslight and rather degenerate data. Furthermore, they make use of the mentallyrepresented language in a highly creative way, constrained by its rules but freeto express new thoughts that relate to past experience or present sensationsonly in a remote and abstract fashion. If this is correct, there is no hope in thestudy of the “control” of human behavior by stimulus conditions, schedules ofreinforcement, establishment of habit structures, patterns of behavior, and soon. Of course, one can design a restricted environment in which such controland such patterns can be demonstrated, but there is no reason to suppose that

--- Page 120 ---

Form and meaning in natural languages 101
any more is learned about the range of human potentialities by such methods
than would be learned by observing humans in a prison or an army – or inmany a schoolroom. The essential properties of the human mind will alwaysescape such investigation. And if I can be pardoned a ﬁnal “nonprofessional”comment, I am very happy with this outcome.

--- Page 121 ---

5 The formal nature of language
General properties of language
Many generations of productive scholarship notwithstanding, the questions to
which this paper is addressed can receive only quite tentative answers. There arefew languages for which descriptions in depth are available, and only selectedaspects of language have been studied with sufﬁcient care and success to pro-vide support for conclusions of a general nature. Still, it is possible, with somedegree of conﬁdence, to outline certain properties and conditions that distin-guish human languages among arbitrary systems of symbolic manipulation,communication, and self-expression.
Competence and performance
At the crudest level of description, we may say that a language associates soundand meaning in a particular way; to have command of a language is to be able,in principle, to understand what is said and to produce a signal with an intendedsemantic interpretation. But aside from much unclarity, there is also a seriousambiguity in this crude characterization of command of language. It is quiteobvious that sentences have an intrinsic meaning determined by linguistic ruleand that a person with command of a language has in some way internalizedthe system of rules that determine both the phonetic shape of the sentence andits intrinsic semantic content – that he has developed what we will refer to asaspeciﬁc linguistic competence. However, it is equally clear that the actual
observed use of language – actual performance –does not simply reﬂect the
intrinsic sound–meaning connections established by the system of linguisticrules. Performance involves many other factors as well. We do not interpret whatis said in our presence simply by application of the linguistic principles that
determine the phonetic and semantic properties of an utterance. Extralinguisticbeliefs concerning the speaker and the situation play a fundamental role indetermining how speech is produced, identiﬁed, and understood. Linguisticperformance is, furthermore, governed by principles of cognitive structure (for
102

--- Page 122 ---

The formal nature of language 103
example, by memory restrictions) that are not, properly speaking, aspects of
language.
Tostudy a language, then, we must attempt to disassociate a variety of factors
that interact with underlying competence to determine actual performance; the
technical term “competence” refers to the ability of the idealized speaker–hearerto associate sounds and meanings strictly in accordance with the rules of hislanguage. The grammar of a language, as a model for idealized competence,
1
establishes a certain relation between sound and meaning – between phoneticand semantic representations. We may say that the grammar of the language L
generates a set of pairs ( s,I), where sis the phonetic representation of a certain
signal
2andIis the semantic interpretation assigned to this signal by the rules
of the language. To discover this grammar is the primary goal of the linguisticinvestigation of a particular language.
The general theory of linguistic structure is concerned with discovering the
conditions that any such grammar must meet. This general theory will be con-cerned with conditions of three kinds: conditions on the class of admissiblephonetic representations, the class of admissible semantic representations, andthe systems of rules that generate paired phonetic and semantic representa-tions. In all three respects, human languages are subject to stringent limitingconditions. There is no difﬁculty in constructing systems that do not meet theseconditions, and that do not, therefore, qualify as potential human languagesdespite the fact that they associate sound and meaning in some deﬁnite way.Human languages are systems of a highly speciﬁc kind. There is no a priorinecessity for a system relating sound and meaning to be of this kind. As thispaper proceeds, we shall mention some of the highly restrictive conditions thatappear to be essential properties of human language.
Agrammar generates a certain set of pairs ( s,I), where sis a phonetic
representation and Iits associated semantic interpretation. Similarly, we might
think of a performance model as relating sound and meaning in a speciﬁcway.Aperceptual model, PM,for example, might be described, as in 1, as a
device that accepts a signal as input (along with much else) and assigns variousgrammatical representations as “output.”
1
1The term “grammar” is often used ambiguously to refer both to the internalized system of rules
and to the linguist’s description of it.
2Tobe more precise, a certain class of signals that are repetitions of one another, in a sense to
which we return subsequently.

--- Page 123 ---

104 Language and Mind
Acentral problem for psychology is to discover the characteristics of a sys-
temPMof this sort. Clearly, in understanding a signal, a hearer brings to bear
information about the structure of his language. In other words, the model
PMincorporates the grammar Gof a language. The study of how sentences
are understood – the general problem of speech perception – must, obviously,remain within narrow limits unless it makes use of this basic property of a per-ceptual model. But it is important to distinguish clearly between the functionand properties of the perceptual model PMand the competence model Gthat it
incorporates. Both GandPMrelate sound and meaning; but PMmakes use of
much information beyond the intrinsic sound–meaning association determinedby the grammar G,and it operates under constraints of memory, time, and
organization of perceptual strategies that are not matters of grammar. Corre-spondingly, although we may describe the grammar Gas a system of processes
and rules that apply in a certain order to relate sound and meaning, we arenot entitled to take this as a description of the successive acts of a performancemodel such as PM–infact, it would be quite absurd to do so. What we have said
regarding perceptual models is equally applicable to production models. Thegrammatical rules that generate phonetic representations of signals with theirsemantic interpretations do not constitute a model for the production of sen-tences, although any such model must incorporate the system of grammaticalrules. If these simple distinctions are overlooked, great confusion must result.
In this paper, attention is focused on competence and the grammars that char-
acterize it; when speaking of semantic and phonetic interpretation of sentences,we refer exclusively to the idealized representations determined by this under-lying system. Performance provides data for the study of linguistic competence.Competence, in the sense just described, is one of many factors that interactto determine performance. In general, we would expect that in studying thebehavior of a complex organism, it will be necessary to isolate such essentiallyindependent underlying systems as the system of linguistic competence, eachwith its intrinsic structure, for separate attention.
Initial steps toward a study of competence
Turning to the study of underlying competence, let us ﬁrst take note of a few very
obvious properties of the grammar of a human language. It is, ﬁrst of all, quiteclear that the set of paired phonetic and semantic representations generatedby the grammar will be inﬁnite. There is no human language in which it ispossible, in fact or in principle, to specify a certain sentence as the longestsentence meaningful in this language. The grammar of any language containsdevices that make it possible to form sentences of arbitrary complexity, eachwith its intrinsic semantic interpretation. It is important to realize that this is nomere logical nicety. The normal use of language relies in an essential way on

--- Page 124 ---

The formal nature of language 105
this unboundedness, on the fact that language contains devices for generating
sentences of arbitrary complexity. Repetition of sentences is a rarity; innovation,in accordance with the grammar of the language, is the rule in ordinary day-by-day performance. The idea that a person has a “verbal repertoire” – a stock ofutterances that he produces by “habit” on an appropriate occasion – is a myth,totally at variance with the observed use of language. Nor is it possible to attachany substance to the view that the speaker has a stock of “patterns” in which heinserts words or morphemes. Such conceptions may apply to greetings, a fewcliches, and so on, but they completely misrepresent the normal use of language,as the reader can easily convince himself by unprejudiced observation.
3
Todiscover the grammar of some language user, we must begin by obtaining
information that bears on his interpretation of sentences, on the semantic, gram-matical, and phonetic structure that he assigns to them. For example, for thestudy of English, it would be important to discover such facts as the following.Consider the sentence frames 2and the words “persuaded,” “expected,” and
“happened”:
2a . John – Bill that he should leave
b.John – Bill to leave
c.John – to leave
d.It is – that Bill will leave
The word “persuaded” can be inserted in a and b, but not c or d; “expected” can
be inserted in b, c, d, but not a; “happened” can be inserted only in c. Inserting“persuaded” in a, we derive an ambiguous sentence, the interpretation of whichdepends on the reference of “he”; under one interpretation, the sentence is a nearparaphrase of b, with “persuaded” inserted. When “expected” appears in b andc, the subject–verb relation holds between “Bill” and “leave” in b, but between“John” and “leave” in c. The sentence “John happened to leave” has roughlythe same meaning as “It happened that John left,” but “John expected to leave”is not even a remote paraphrase of “It expected that John left.” Such facts asthese can be stated in many ways, and we might use one or another techniqueto make sure of their accuracy. These are facts about the competence of thespeaker of English. They can be used as a basis for discovering his internalizedgrammar.
Let us consider the status of such observations with slightly greater care.
These observations actually bear directly on the output of a perceptual model
3Or by some simple calculations of the number of sentences and “patterns” that might be needed,
for empirical adequacy, in such repertoires. For some relevant comments, see G. A. Miller,E. Galanter, and K. H. Pribram, Plans and the Structure of Behavior (New York: Holt, Rinehart
and Winston, 1960), pp. 145 f.; G. A. Miller and N. Chomsky, “Finitary Models of LanguageUsers,” in R. D. Luce, R. Bush, and E. Galanter, eds., Handbook of Mathematical Psychology
(New York: Wiley, 1963), V ol. II, p. 430.

--- Page 125 ---

106 Language and Mind
such as 1;they relate to the structures assigned to signals by the hearer. Our char-
acterization of the output of 1is a construct based on evidence of this sort. Then,
the perceptual model PMitself is a second-order construct. Abstracting further,
we can study the grammar that constitutes one fundamental component of 1
as a third-order construct. Thus the evidence cited in the preceding paragraph
actually has a bearing on grammar only indirectly. We must, in other words,presuppose the legitimacy of each abstraction. There seems little question of thelegitimacy of abstraction in such cases as these, and there is an overwhelmingmass of evidence of the sort cited. Once again, we note that idealization of thekind just described is inescapable if a complex organism is to be studied in aserious way.
This process of abstraction can be carried one step further. Consider an
acquisition model AMthat uses linguistic data to discover the grammar of the
language to which this data pertains.
3
Just how the device AMselects a grammar will be determined by its internal
structure, by the methods of analysis available to it, and by the initial constraints
that it imposes on any possible grammar. If we are given information about thepairing of linguistic data and grammars, we may try to determine the natureof the device AM. Although these are not the terms that have been used, lin-
guistics has always been concerned with this question. Thus modern structurallinguistics has attempted to develop methods of analysis of a general nature,independent of any particular language, and an older and now largely forgot-ten tradition attempted to develop a system of universal constraints that anygrammar must meet. We might describe both these attempts as concerned withthe internal structure of the device AM,with the innate conception of “human
language” that makes language acquisition possible.
4
Universal grammar
Let us now turn to the study of underlying competence, and consider the generalproblem of how a sound–meaning pairing might be established. As a prelimi-nary to this investigation of universal grammar, we must ask how sounds andmeanings are to be represented. Since we are interested in human languagesin general, such systems of representation must be independent of any partic-ular language. We must, in other words, develop a universal phonetics and a
4The existence of innate mental structure is, obviously, not a matter of controversy. What we may
question is just what it is and to what extent it is speciﬁc to language.

--- Page 126 ---

The formal nature of language 107
universal semantics that delimit, respectively, the set of possible signals and
the set of possible semantic representations for any human language. It willthen be possible to speak of a language as a particular pairing of signals withsemantic interpretations, and to investigate the rules that establish this pair-ing. Our review of the general properties of language thus falls naturally intothree parts: a discussion of universal phonetics, of universal semantics, and ofthe overarching system of universal grammar. The ﬁrst two topics involve therepresentation of idealized form and semantic content; the theory of universalgrammar deals with the mechanisms used in natural languages to determine theform of a sentence and its semantic content.
The importance of developing a universal semantics and universal phonet-
ics, in the sense of the last paragraph, was clearly recognized long before thedevelopment of modern linguistics. For example, Bishop Wilkins in his Essay
Towards a Real Character and a Philosophical Language (1668) attempted to
develop a universal phonetic alphabet and a universal catalogue of concepts interms of which, respectively, the signals and semantic interpretations for anylanguage can be represented. The phonetic alphabet is based on a system of pho-netic properties developed in terms of point and manner of articulation. Eachphonetic symbol is analyzable as a set of such properties; in modern terms, it isanalyzable as a set of distinctive features. It is furthermore tacitly assumed that
the physical signal is determined, by language-independent principles, from itsrepresentation in terms of phonetic symbols. The concepts that are proposed asunits of semantic interpretation are also analyzable into ﬁxed properties (seman-tic features) of some sort, for example, animate-inanimate, relational-absolute,agent-instrument, etc. It is tacitly assumed that the semantic interpretation of asentence is determined by universal, language-independent principles from theconcepts comprised in the utterance and the manner in which they are gram-matically related (for example, as subject-predicate).
5Although the defects in
execution in such pioneering studies as that of Wilkins are obvious, the gen-
eral approach is sound. The theory of universal phonetics has been intensivelypursued along the lines just indicated with considerable success; the paralleltheory of universal semantics has, in contrast, been very little studied.
Universal grammar: universal phonetics
The theory of universal phonetics attempts to establish a universal phoneticalphabet and a system of laws. The alphabet deﬁnes the set of possible signalsfrom which the signals of a particular language are drawn. If the theory is cor-rect, each signal of a language can be represented as a sequence of symbols of
5This assumption is not explicit in Wilkins, but is developed in other seventeenth- and eighteenth-
century work. See my Cartesian Linguistics (New York: Harper & Row, 1966) for references
and discussion.

--- Page 127 ---

108 Language and Mind
the phonetic alphabet. Suppose that two physical events are represented as the
same sequence. Then in any language they must be repetitions of one another.6
On the other hand, two physical events might be regarded by speakers of onelanguage as repetitions and by speakers of another language as nonrepetitions.In this case, the universal alphabet must provide the means for distinguishingthem. Representation in terms of the universal alphabet should provide what-ever information is necessary to determine how the signal may be produced,
and it should, at the same time, correspond to a reﬁned level of perceptual rep-resentation. We stress once again, however, that actual performance involvesother factors beyond ideal phonetic representation.
The symbols of the universal phonetic alphabet are not the “primitive
elements” of universal phonetic theory. These primitive elements include, rather,what have been called ( phonetic )distinctive features ,properties such as voic-
ing, frontness–backness, stress, etc.
7Each of these features can be thought of
as a scale in terms of which two or more values can be distinguished (how manyvalues need be distinguished is an open question, but the number is apparently
quite small for each feature). A symbol of the phonetic alphabet is properly tobe regarded as a set of features, each with a speciﬁed value. A signal, then, isrepresented as a sequence of such sets.
Three obvious properties of language are reﬂected in a phonetic theory of this
sort. The ﬁrst is its discreteness – the fact that only a determinable ﬁnite numberof signals of any given length can be nonrepetitions. The second property is theunboundedness of language – the fact that a signal can be of arbitrary length,so that a language will contain inﬁnitely many semantically interpreted signals.In addition to these formal properties, a phonetic theory of this sort reﬂects thefact that two segments of a signal, represented by two symbols of the universal
alphabet, may be alike in certain respects and distinct in others; and that thereare, furthermore, a ﬁxed number of such dimensions of sameness and differenceand a ﬁxed number of potentially signiﬁcant points along these dimensions.Thus, the initial segments of pinandbin
8differ with respect to voicing and
aspiration but not (signiﬁcantly) with respect to point of articulation; the twoconsonants of cocoa differ with respect to neither point of articulation nor
voicing, but only with respect to aspiration; etc.
6In an appropriate sense of repetition. Thus any two physical signals are in some way distinct, but
some of the differences are irrelevant in a particular language, and others are irrelevant in anylanguage.
7Atheory of phonetic distinctive features is developed in R. Jakobson, G. Fant, and M. Halle,
Preliminaries to Speech Analysis ,2nd edn (Cambridge, Mass.: MIT. Press, 1963). A revised
and, we think, improved version appears in N. Chomsky and M. Halle, Sound Pattern of English
(New York: Harper & Row, 1968).
8Observe that although the order of phonetic segments is a signiﬁcant fact, there is no reason toassume that the physical event represented by a particular sequence of phonetic symbols can beanalyzed into successive parts, each associated with a particular symbol.

--- Page 128 ---

The formal nature of language 109
It is important to note that the distinctive features postulated in universal
phonetic theory are absolute in several senses but relative in others. They are
absolute in the sense that they are ﬁxed for all languages. If phonetic represen-tation is to provide sufﬁcient information for identiﬁcation of a physical signal,then speciﬁcation of feature values must also be absolute. On the other hand,the features are relative when considered in terms of the notion of repetition–nonrepetition. For example, given three absolute values designated 1, 2, 3 interms of the feature front–back, we might ﬁnd that in language L1twoutter-
ances that differ only in the values 1, 2 of frontness–backness are distinguishedas nonrepetitions but utterances differing only in the values 2, 3 are not; whereasin language L2the opposite might be the case. Each language would use the
feature front–back to distinguish nonrepetitions, but the absolute value 2 thatis “front” in one language would be “back” in the other.
In addition to a system of distinctive features, a universal phonetic theory will
also attempt to formulate certain laws that govern the permitted sequences andpermitted variety of selection in a particular language. For example, Jakobsonhas observed that no language uses both the feature labialization and the fea-ture velarization for distinguishing nonrepetitions, and he has suggested a moregeneral formulation in terms of which these two features can be regarded asvariants of a single, more abstract feature. Generalizations of this sort – partic-
ularly when they can be supported by rational argument – can be proposed aslaws of universal phonetics.
Universal grammar: universal semantics
Although universal phonetics is a fairly well-developed subject, the same cannotbe said of universal semantics. Here, too, we might hope to establish a universalsystem of semantic features and laws regarding their interrelations and permittedvariety. In fact, the problem of determining such features and such laws has once
again become a topic of serious investigation in the past few years,
9and there
is some promise of fruitful development. It can be seen at once that an analysisof concepts in terms of such features as animateness, action, etc. (see p. 107),will hardly be adequate, and that certain features must be still more abstract. Itis, for example, a fact of English that the phrase “a good knife” means “a knifewhich cuts well.” Consequently the concept “knife” must be speciﬁed in part
9See J. Katz, The Philosophy of Language (New York: Harper & Row, 1965), for a review of some
recent work. For another view, see U. Weinreich, “Explorations in Semantic Theory,” in T. A.
Sebeok, ed., Current Trends in Linguistics ,Vol. III of Linguistic Theory (The Hague: Mouton,
1966); and for comments on this and more extensive development of the topic, see J. Katz,Semantic Theory (New York: Harper & Row, Publishers, 1972). In addition, there has been quite
abit of recent work in descriptive semantics, some of which is suggestive with respect to the
problems discussed here.

--- Page 129 ---

110 Language and Mind
in terms of features having to do with characteristic functions (not just physical
properties), and in terms of an abstract “evaluation feature”10that is determined
by such modiﬁers as “good,” “terrible,” etc. Only by such an analysis can thesemantic relationship between “this is a good knife” and “this knife cuts well” beestablished. In contrast, the irrelevance of “this is a good knife for digging with”to “this knife cuts well” shows that the semantic interpretation of a sentence isdetermined by grammatical relations of a sort that are by no means transparent.
As in the case of universal phonetics, we might hope to establish general
principles regarding the possible systems of concepts that can be representedin a human language and the intrinsic connections that may exist among them.With the discovery of such principles, universal semantics would become a
substantive discipline.
Universal grammar: universal syntax
Suppose that a satisfactory theory of universal phonetics and of universal seman-tics were at hand. We could then deﬁne a language as a set of sentences, whereasentence is a particular kind of sound–meaning pair, and go on to study the
systems of rules that deﬁne human languages. But in fact only the theory ofuniversal phonetics is sufﬁciently well established to support this enterprise.Consequently, we must approach the study of language structure in a slightlymore indirect way.
Notice that although the notion “semantic representation” is itself far from
clear, we can, nevertheless, ﬁnd innumerable empirical conditions that an expli-cation of this notion must meet. Consider, for example, the following sentence:
4 What disturbed John was being disregarded by everyone.
It is clear, ﬁrst of all, that this expression has two distinct interpretations.
Under one interpretation, it means that John was disturbed by the fact that
everyone disregarded him; under the second, it means that everyone was dis-
regarding the things that disturb John. Under the ﬁrst of these interpretations,acertain grammatical relation holds between “disregard” and “John,” namely
the same relation that holds between these items in “Everyone disregards John”(the “verb–object” relation). Under the second interpretation neither this nor anyother grammatically signiﬁcant relation holds between “disregard” and “John.”On the other hand, if we insert the word “our” between “was” and “being,” thesentence is unambiguous, and no grammatical relation holds between “disre-gard” and “John,” although the verb–object relation now holds between “dis-regard” and “we” (an underlying element of “our”).
10Fordiscussion of this notion, see J. Katz, “Semantic Theory and the Meaning of ‘Good,’”
Journal of Philosophy ,Vol. 61, No. 23, 1964.

--- Page 130 ---

The formal nature of language 111
Examples of this sort can be elaborated indeﬁnitely. They provide conditions
of adequacy that the notion “semantic interpretation” must meet (for example,
relations of paraphrase and implication and the property of ambiguity mustbe correctly reﬂected), and they illustrate clearly some of the ways in whichthe semantic interpretations of linguistic expressions must be determined fromthose of their grammatically related parts.
From such considerations, we are led to formulate a more restricted but quite
signiﬁcant immediate goal for the study of linguistic structure. Still taking alanguage to be a set of sentences, let us consider each abstract “sentence” to be aspeciﬁc pairing of a phonetic representation with an abstract structure of somesort (let us call it a deep structure )that incorporates information relevant to
semantic interpretation. We can then study the system of rules that determinesthis pairing, in a particular language, and the general characteristics of suchrules. This enterprise will be signiﬁcant to the extent that these underlyingdeep structures do actually provide a way to meet the empirical conditions onsemantic interpretation. Semantic theory, as it progresses, will then providemeans for enriching deep structures and associating semantic interpretationswith them. The empirical signiﬁcance of a full theory of grammar, comprisingauniversal phonetics, semantics, and syntax, will depend in part on the extent
to which conditions on semantic interpretation can be satisﬁed by systematicuse of the devices and principles that this theory supplies.
Summarizing these remarks, let us establish the following frame-work for
the study of linguistic structure. The grammar of a language is a system of
rules that determines a certain pairing of sound and meaning. It consists of asyntactic component ,asemantic component ,and a phonological component .
The syntactic component deﬁnes a certain (inﬁnite) class of abstract objects(D,S), where Dis adeep structure andSasurface structure. The deep struc-
ture contains all information relevant to semantic interpretation; the surfacestructure, all information relevant to phonetic interpretation. The semantic andphonological components are purely interpretive. The former assigns semanticinterpretations to deep structures; the latter assigns phonetic interpretations tosurface structures. Thus the grammar as a whole relates semantic and phoneticinterpretations, the association being mediated by the rules of the syntactic com-ponent that deﬁne paired deep and surface structures. The study of the threecomponents will, of course, be highly integrated; each can be investigated tothe extent that it is clear what conditions the others impose upon it.
This formulation should be regarded as an informal ﬁrst approximation.
When we develop a precise theory of grammatical structure – for example, theparticular version of the theory of transformational grammar sketched below –we will provide a technical meaning for the terms “deep structure” and “sur-face structure,” and in terms of these technical meanings, we can then raise
the empirical (not conceptual) question of how deep and surface structures

--- Page 131 ---

112 Language and Mind
contribute to and determine semantic and phonetic interpretations. In the tech-
nical sense that is given to the concepts of deep and surface structure in thetheory outlined below, it seems to me that present information suggests thatsurface structure completely determines phonetic interpretation and that deepstructure completely determines certain highly signiﬁcant aspects of seman-tic interpretation. But the looseness of the latter term makes a more deﬁnitestatement impossible. In fact, I think that a reasonable explication of the term“semantic interpretation” would lead to the conclusion that surface structurealso contributes in a restricted but important way to semantic interpretation, butIwill say no more about this matter here.
Universal grammar might be deﬁned as the study of the conditions that
must be met by the grammars of all human languages. Universal semantics andphonetics, in the sense described earlier, will then be a part of universal grammar.So deﬁned, universal grammar is nothing other than the theory of languagestructure. This seems in accord with traditional usage. However, only certainaspects of universal grammar were studied until quite recently. In particular, theproblem of formulating the conditions that must be met by the rules of syntax,phonology, and semantics was not raised in any explicit way in traditionallinguistics, although suggestive and nontrivial steps toward the study of thisproblem are implicit in much traditional work.
11
Agrammar of the sort described previously, which attempts to character-
ize in an explicit way the intrinsic association of phonetic form and semanticcontent in a particular language, might be called a generative grammar
12to
distinguish it from descriptions that have some different goal (for example,pedagogic grammars). In intention, at least, traditional scholarly grammars aregenerative grammars, although they fall far short of achieving the goal of deter-mining how sentences are formed or interpreted. A good traditional grammargives a full exposition of exceptions to rules, but it provides only hints andexamples to illustrate regular structures (except for trivial cases – for example,
inﬂectional paradigms). It is tacitly presumed that the intelligent reader willuse his “linguistic intuition” – his latent, unconscious knowledge of universalgrammar – to determine the regular structures from the presented examplesand remarks. The grammar itself does not express the deep-seated regularitiesof the language. For the purpose of the study of linguistic structure, particularor universal, such grammars are, therefore, of limited value. It is necessary to
11See Chomsky, Cartesian Linguistics ,for discussion.
12See p. 91.Ingeneral, a set of rules that recursively deﬁne an inﬁnite set of objects may be said
togenerate this set. Thus a set of axioms and rules of inference for arithmetic may be said to
generate a set of proofs and a set of theorems of arithmetic (last lines of proofs). Similarly, a
(generative) grammar may be said to generate a set of structural descriptions, each of which,ideally, incorporates a deep structure, a surface structure, a semantic interpretation (of the deepstructure), and a phonetic interpretation (of the surface structure).

--- Page 132 ---

The formal nature of language 113
extend them to full generative grammars if the study of linguistic structure is
to be advanced to the point where it deals signiﬁcantly with regularities and
general principles. It is, however, important to be aware of the fact that theconcept “generative grammar” itself is no very great innovation. The fact thatevery language “makes inﬁnite use of ﬁnite means” (Wilhelm von Humboldt)
has long been understood. Modern work in generative grammar is simply anattempt to give an explicit account of how these ﬁnite means are put to inﬁniteuse in particular languages and to discover the deeper properties that deﬁne“human language,” in general (that is, the properties that constitute universalgrammar).
Wehave been concerned thus far only with clariﬁcation of concepts and
setting of goals. Let us now turn to the problem of formulating hypotheses ofuniversal grammar.
Structure of the phonological component
The syntactic component of a generative grammar deﬁnes (generates) an inﬁniteset of pairs ( D,S), where Dis a deep structure and Sis a surface structure; the
interpretive components of the grammar assign a semantic representation to D
and a phonetic representation to S.
Let us ﬁrst consider the problem of assigning phonetic representations to
surface structures. As in the previous discussion of universal phonetics, wetake a phonetic representation to be a sequence of symbols of the universalphonetic alphabet, each symbol being analyzed into distinctive features withspeciﬁc values. Stating the same idea slightly differently, we may think of aphonetic representation as a matrix in which rows correspond to features of theuniversal system, columns correspond to successive segments (symbols of thephonetic alphabet), and each entry is an integer that speciﬁes the value of aparticular segment with respect to the feature in question. Our problem, then,is to determine what information must be contained in the surface structure,and how the rules of the phonological component of the grammar use thisinformation to specify a phonetic matrix of the sort just described.
Consider once again the example 4,which we repeat in 5for ease of reference:
5 What # disturb-ed # John # was # be-ing # dis-regard-ed # by # every-one.
Toﬁrst approximation,
13we may think of 5as a sequence of the formatives
“what,” “disturb,” “ed,” “John,” “was,” “be,” “ing,” “dis,” “regard,” “ed,” “by,”“every,” “one,” with the junctures represented by the symbols # and – in the
13The analysis that is presented here for purposes of exposition would have to be reﬁned for
empirical adequacy.

--- Page 133 ---

114 Language and Mind
positions indicated in 5.These junctures specify the manner in which formatives
are combined; they provide information which is required by the interpretive
rules of the phonological component. A juncture must, in fact, be analyzed as aset of features, that is, as a single-column matrix in which the rows correspondto certain features of the junctural system and each entry is one of two valueswhich we may represent as +or –. Similarly, each formative will be analyzed
as a matrix in which columns stand for successive segments, rows correspondto certain categorial features ,and each entry is either +or –. Therefore, the
entire sentence 5can be regarded as a single matrix with the entries +and –.
14
The categorial features include the universal features of the phonetic system,
along with diacritic features which essentially indicate exceptions to rules. Thus
the matrix corresponding to “what,” in the dialect in which the correspondingphonetic representation is [wat], will contain three segments, the ﬁrst speciﬁedas a labial glide, the second as a low back unrounded vowel, the third as anunvoiced dental stop consonant (these speciﬁcations given completely in termsof the+and – values of features supplied by the universal phonetic system). The
rules of the phonological component, in this case, will convert this speciﬁcationin terms of +and – values into a more detailed speciﬁcation in terms of integers,
in which the value of each segment with respect to the phonetic features (forexample, tongue height, degree of aspiration, etc.) is indicated to whatever
degree of accuracy is required by the presupposed theory of universal phonetics,and with whatever range of variation is allowed by the language. In this example,the assigned values will simply reﬁne the bifurcation into +and – values given
in the underlying matrix for “what” in 5.
The example just cited is unusually simple, however. In general, the rules
of the phonological component will not only give a ﬁner speciﬁcation of theunderlying division into +and – values, but will also change values signiﬁcantly
and, perhaps, insert, delete, or rearrange segments. For example, the formative“by” will be represented with an underlying matrix consisting of two columns,the second of which is speciﬁed as a high front-vowel (speciﬁcation given interms of values of features). The corresponding phonetic matrix, however, willconsist of three columns, the second of which is speciﬁed as a low back-voweland the third as a palatal glide (the speciﬁcation here being in terms of integralvalued entries in a phonetic matrix).
15
The surface structure of 5,then, is represented as a matrix in which one of two
values appears in each entry. The fact that only two values may appear indicates
14Notice that every two successive formatives are separated by a juncture, as is necessary if the
representation of 5as a single matrix is to preserve the formative structure. For present purposes,
we may think of each segment of a formative as unmarked for all junctural features and eachjuncture as unmarked for each formative feature.
15The reasons for this analysis go beyond the scope of this discussion. For details see Chomskyand Halle, Sound Pattern of English.

--- Page 134 ---

The formal nature of language 115
that this underlying matrix really serves a purely classiﬁcatory function. Each
sentence is classiﬁed in such a way as to distinguish it from all other sentences,and in such a way as to determine just how the rules of the phonological compo-nent assign speciﬁc positional phonetic values. We see, then, that the distinctivefeatures of the universal phonetic system have a classiﬁcatory function in the
underlying matrix constituting a part of the surface structure, and a phonetic
function in the matrix constituting the phonetic representation of the sentence
in question. Only in the former function are the distinctive features uniformlybinary; only in the latter do they receive a direct physical interpretation.
The underlying classiﬁcatory matrix just described does not exhaust the infor-
mation required by the interpretive phonological rules. Beyond this, it is neces-sary to know how the sentence in question is subdivided into phrases of varyingsize, and what types of phrase these are. In the case of 5,for example, phonolog-
ical interpretation requires the information that “disturb” and “disregard” areverbs, that “what disturbed John” is a noun phrase, that “John was being” is not
aphrase at all, and so on. The relevant information can be indicated by a proper
bracketing of the sentence with labeled brackets.
16The unit contained within
paired brackets [ Aand] Awill be referred to as a phrase of the category A.For
example, the sequence “what # disturbed # John” in 5will be enclosed within
the brackets [ np,]np,indicating that it is a noun phrase; the formative “disturb”
will be enclosed within the brackets [ v,]v,indicating that it is a verb; the whole
expression 5will be enclosed within the brackets [ s,]s,indicating that it is a
sentence; the sequence “John was being” will not be enclosed within pairedbrackets, since it is no phrase at all. To take an extremely simple example, thesentence “John saw Bill” might be represented in the following way as a surfacestructure, where each orthographically represented item is to be regarded as aclassiﬁcatory matrix:
6
/bracketleftBig
s/bracketleftBig
np/bracketleftBig
nJohn/bracketrightBig
n/bracketrightBig
np/bracketleftBig
vp/bracketleftBig
vsaw/bracketrightBig
v/bracketleftBig
np/bracketleftBig
nBill/bracketrightBig
n/bracketrightBig
np/bracketrightBig
vp/bracketrightBig
s
This representation indicates that “John” and “Bill” are nouns ( N’s)and “saw”
averb ( V); that “John” and “Bill” are, furthermore, noun phrases ( NP’s); that
“saw Bill” is a verb phrase ( VP); and that “John saw Bill” is a sentence ( S). It
seems that interpretation of a sentence by the phonological component of the
grammar invariably requires information which can be represented in the wayjust described. We therefore postulate that the surface structure of a sentenceis a properly labeled bracketing of a classiﬁcatory matrix of formatives andjunctures.
16In the obvious sense. Thus [ a...[b... ]b...[c... ]c... ]awould, for example, be a proper
bracketing of the string ...i n terms of the labeled brackets [ a,]a,[b,]b,[c,]c,but neither of
the following would be proper bracketing:
[a...[b... ]a;[a...[b... ]a... ]b

--- Page 135 ---

116 Language and Mind
The phonological component of a grammar converts a surface structure into a
phonetic representation. We have now given a rough speciﬁcation of the notions
“surface structure” and “phonetic representations.” It remains to describe therules of the phonological component and the manner in which they are orga-nized.
The evidence presently available suggests that the rules of the phonological
component are linearly ordered in a sequence R
1,...Rn,and that this sequence
of rules applies in a cyclic fashion to a surface structure in the following way. Inthe ﬁrst cycle of application, the rules R
1,..., Rnapply in this order to a maximal
continuous part of the surface structure containing no internal brackets. Afterthe last of these rules has applied, innermost brackets are erased and the secondcycle of application is initiated. In this cycle, the rules again apply in the given
order to a maximal continuous part of the surface structure containing no internalbrackets. Innermost brackets are then erased, and the third cycle is initiated.The process continues until the maximal domain of phonological processes (insimple cases, the entire sentence) is reached. Certain of the rules are restrictedin application to the level of word-boundary – they apply in the cycle only whenthe domain of application is a full word. Others are free to iterate at every stageof application. Notice that the principle of cyclic application is highly intuitive.It states, in effect, that there is a ﬁxed system of rules that determines the formof large units from the (ideal) form of their constituent parts.
Wecan illustrate the principle of cyclic application with some rules of
stress assignment in English. It seems to be a fact that although phoneticrepresentations for English must allow ﬁve or six different values along thedistinctive feature of stress, nevertheless, all segments can be unmarked withrespect to stress in surface structures – that is, stress has no categorial function(except highly marginally) as a distinctive feature for English. The complexstress contours of the phonetic representation are determined by such rules as7and8.
17
7 Assign primary stress to the left-most of two primary stressed vowels,in nouns.
8 Assign primary stress to the right-most stress-peak, where a vowel V
is a stress-peak in a certain domain if this domain contains no vowelmore heavily stressed than V.
Rule 7applies to nouns with two primary stresses; rule 8applies to a unit of
any other kind. The rules apply in the order 7,8,inthe cyclic manner described
above. By convention, when primary stress is assigned in a certain position, all
17These are simpliﬁed, for expository purposes. See Chomsky and Halle, Sound Pattern of English ,
for a more accurate account. Notice that in this exposition we are using the term “applies”
ambiguously, in the sense of “available for application” and also in the sense of “actuallymodiﬁes the sequence under consideration.”

--- Page 136 ---

The formal nature of language 117
other stresses are weakened by one. Notice that if a domain contains no stressed
vowel, then rule 8will assign primary stress to its right-most vowel.
Toillustrate these rules, consider ﬁrst the surface structure 6.Inaccordance
with the general principle of cyclic application, the rules 7and8ﬁrst apply to the
innermost units [ nJohn] n,[vsaw]v,and [ nBill]n.Rule 7is inapplicable; rule
8applies, assigning primary stress to the single vowel in each case. Innermost
1
brackets are then erased. The next cycle deals with the units [ npJohn] npand
1
[npBill]npand simply reassigns primary stress to the single vowel, by rule 8
11
Innermost brackets are then erased, and we have the unit [ vpsaw Bill] vpas the
domain of application of the rules. Rule 7is again inapplicable, since this is not
anoun; rule 8assigns primary stress to the vowel of “Bill,” weakening the stress
on “saw” to secondary. Innermost brackets are erased, and we have the unit
[s1
John2
saw1
Bill]sas the domain of application. Rule 7is again inapplicable,
and rule 8assigns primary stress to “Bill,” weakening the other stresses and
giving “2
John3
saw1
Bill”which can be accepted as an ideal representation of
the stress contour.
Consider now the slightly more complex example “John’s black-board
eraser.” In the ﬁrst application of the cycle, rules 7and8apply to the inner-
most bracketed units “John,” “black,” “board,” “erase”; rule 7is inapplicable,
and rule 8assigns primary stress in each case to the right-most vowel (the
only vowel, in the ﬁrst three). The next cycle involves the units “John’s” and
“eraser,” and is vacuous.18The domain of application for the next cycle is
[n1
black1
board ]n.Being a noun, this unit is subject to rule 7,which assigns
primary stress to “black,” weakening the stress on “board” to secondary. Inner-most brackets are erased, and the domain of application for the next cycle is
[n 1
balck2
board1
eraser ]n.Again rule 7applies, assigning primary stress to
“black” and weakening all other stresses by one. In the ﬁnal cycle, the domainof application of the rules is [ np
1
John’s1
black3
board2
eraser ]np.Rule 7is
inapplicable, since this is a full noun phrase. Rule 8assigns primary stress
to the right-most primary stressed vowel, weakening all the others and giving“
2
John’s1
black4
board3
eraser .” In this way, a complex phonetic representation
18The word “eraser” is, at this stage, bisyllabic.

--- Page 137 ---

118 Language and Mind
is determined by independently motivated and very simple rules, applying in
accordance with the general principle of the cycle.
This example is characteristic and illustrates several important points. The
grammar of English must contain the rule 7so as to account for the fact that
the stress contour is falling in the case of the noun “blackboard,” and it mustcontain rule 8,toaccount for the rising contour of the phrase “black board”
(“board which is black”). The principle of the cycle is not, strictly speaking,
part of the grammar of English but is rather a principle of universal gram-mar that determines the application of the particular rules of English or anyother language, whatever these rules may be. In the case illustrated, the generalprinciple of cyclic application assigns a complex stress contour, as indicated.Equipped with the principle of the cycle and the two rules 7and8,aperson
will know
19the proper stress contour for “John’s blackboard eraser” and innu-
merable other expressions which he may never have heard previously. This isasimple example of a general property of language; certain universal princi-
ples must interrelate with speciﬁc rules to determine the form (and meaning )
of entirely new linguistic expressions.
This example also lends support to a somewhat more subtle and far-reaching
hypothesis. There is little doubt that such phenomena as stress contours inEnglish are a perceptual reality; trained observers will, for example, reach a highdegree of unanimity in recording new utterances in their native language. Thereis, however, little reason to suppose that these contours represent a physical
reality. It may very well be the case that stress contours are not represented inthe physical signal in anything like the perceived detail. There is no paradoxin this. If just two levels of stress are distinguished in the physical signal, thenthe person who is learning English will have sufﬁcient evidence to constructthe rules 7and8(given the contrast “blackboard,” “black board,” for example).
Assuming then that he knows the principle of the cycle, he will be able toperceive the stress contour of “John’s blackboard eraser” even if it is not aphysical property of the signal. The evidence now available strongly suggeststhat this is an accurate description of how stress is perceived in English.
It is important to see that there is nothing mysterious in this description. There
would be no problem in principle in designing an automaton that uses the rules 7
and8,the rules of English syntax, and the principle of the transformational cycle
to assign a multi-leveled stress contour even to an utterance in which stress is notrepresented at all (for example, a sentence spelled in conventional orthography).The automaton would use the rules of syntax to determine the surface structureof the utterance, and would then apply the rules 7and8,inaccordance with
the principle of the cycle, to determine the multi-leveled contour. Taking suchan automaton as a ﬁrst approximation to a model for speech perception (see 1,
19As earlier, we refer here to “tacit” or “latent knowledge,” which can, perhaps, be brought to
consciousness with proper attention but is surely not presented to “unguided intuition.”

--- Page 138 ---

The formal nature of language 119
p. 103), we might propose that the hearer uses certain selected properties of the
physical signal to determine which sentence of the language was produced andto assign to it a deep and surface structure. With careful attention, he will then beable to “hear” the stress contour assigned by the phonological component of hisgrammar, whether or not it corresponds to any physical property of the presentedsignal. Such an account of speech perception assumes, putting it loosely, thatsyntactic interpretation of an utterance may be a prerequisite to “hearing” itsphonetic representation in detail; it rejects the assumption that speech perceptionrequires a full analysis of phonetic form followed by a full analysis of syntacticstructure followed by semantic interpretation, as well as the assumption thatperceived phonetic form is an accurate point-by-point representation of thesignal. But it must be kept in mind that there is nothing to suggest that eitherof the rejected assumptions is correct, nor is there anything at all mysteriousin the view just outlined that rejects these assumptions. In fact, the view justoutlined is highly plausible, since it can dispense with the claim that somepresently undetectable physical properties of utterances are identiﬁed with anaccuracy that goes beyond anything experimentally demonstrable even underideal conditions, and it can account for the perception of stress contours of novelutterances
20on the very simple assumption that rules 7and8and the general
principle of cyclic application are available to the perceptual system.
There is a great deal more to be said about the relative merits of various kinds
of perceptual models. Instead of pursuing this topic, let us consider further thehypothesis that rules 7and8,and the principle of cyclic application, are available
to the perceptual system and are used in the manner suggested. It is clear howrules 7and8might be learned from simple examples of rising and falling contour
(for example, “black board” contrasted with “blackboard”). But the questionthen arises: how does a person learn the principle of cyclic application? Beforefacing this question, it is necessary to settle one that is logically prior to it:
why assume that the principle is learned at all? There is much evidence that theprinciple is used, but from this it does not follow that it has been learned. In fact,it is difﬁcult to imagine how such a principle might be learned, uniformly byall speakers, and it is by no means clear that sufﬁcient evidence is available inthe physical signal to justify this principle. Consequently, the most reasonableconclusion seems to be that the principle is not learned at all, but rather that itis simply part of the conceptual equipment that the learner brings to the taskof language acquisition. A rather similar argument can be given with respect toother principles of universal grammar.
Notice again that there should be nothing surprising in such a conclusion.
There would be no difﬁculty, in principle, in designing an automaton which
20And other aspects. The argument is, in fact, much more general. It must be kept in mind that
speech perception is often impaired minimally, or not at all, even by signiﬁcant distortion of thesignal, a fact difﬁcult to reconcile with the view that phonetic analysis in detail is a prerequisitefor analysis of the syntactic and semantic structure.

--- Page 139 ---

120 Language and Mind
incorporates the principles of universal grammar and puts them to use to deter-
mine which of the possible languages is the one to which it is exposed. A priori,there is no more reason to suppose that these principles are themselves learnedthan there is to suppose that a person learns to interpret visual stimuli in termsof line, angle, contour, distance, or, for that matter, that he learns to have twoarms. It is completely a question of empirical fact; there is no information of anygeneral extralinguistic sort that can be used, at present, to support the assump-tion that some principle of universal grammar is learned, or that it is innate,or (in some manner) both. If linguistic evidence seems to suggest that someprinciples are unlearned, there is no reason to ﬁnd this conclusion paradoxicalor surprising.
Returning to the elaboration of principles of universal grammar, it seems that
the phonological component of a grammar consists of a sequence of rules thatapply in a cyclic manner, as just described, to assign a phonetic representationto a surface structure. The phonetic representation is a matrix of phonetic fea-ture speciﬁcations and the surface structure is a properly labeled bracketing offormatives which are, themselves, represented in terms of marking of categorialdistinctive features. What evidence is now available supports these assumptions;they provide the basis for explaining many curious features of phonetic fact.
It is important to notice that there is no a priori necessity for the phonological
component of a grammar to have just these properties. These assumptions aboutuniversal grammar restrict the class of possible human languages to a veryspecial subset of the set of imaginable “languages.” The evidence available tous suggests that these assumptions pertain to the language acquisition deviceAMof 3, p. 106, that is that they form one part of the schematism that the child
brings to the problem of language learning. That this schematism must be quiteelaborate and highly restrictive seems fairly obvious. If it were not, languageacquisition, within the empirically known limits of time, access, and variability,would be an impenetrable mystery. Considerations of the sort mentioned in the
foregoing discussion are directly relevant to the problem of determining thenature of these innate mechanisms, and, therefore, deserve extremely carefulstudy and attention.
Structure of the semantic component
Let us now consider the second interpretive component of a generative grammar,the system of rules that converts a deep structure into a semantic representationthat expresses the intrinsic meaning of the sentence in question. Although manyaspects of semantic interpretation remain quite obscure, it is still quite possibleto undertake a direct investigation of the theory of deep structures and theirinterpretation, and certain properties of the semantic component seem fairlyclear. In particular, as we have noted earlier, many empirical conditions on

--- Page 140 ---

The formal nature of language 121
semantic interpretation can be clearly formulated. For example, we know that
sentence 4on p. 110 must be assigned at least two semantic representations,
and that one of these must be essentially the same as the interpretation assignedto both 9and10.
9 Being disregarded by everyone disturbed John.
10 The fact that everyone disregarded John disturbed him.
21
Furthermore, it is clear that the semantic representation of a sentence dependson the representation of its parts, as in the parallel case of phonetic interpreta-tion. For example, in the case of 10,itisobvious that the semantic interpretation
depends, in part, on the semantic interpretation of “Everyone disregarded John”;if the latter were replaced in 10by “Life seemed to pass John by,” the interpreta-
tion of the whole would be changed in a ﬁxed way. This much is transparent, andit suggests that a principle like the principle of cyclic application in phonologyshould hold in the semantic component.
Aslightly more careful look at the problem shows that semantic interpreta-
tion must be signiﬁcantly more abstract than phonological interpretation withrespect to the notion of “constituent part.” Thus the interpretation of “Every-one disregarded John” underlies not only 10,but also 9and4,and in exactly
the same way. But neither 4nor9contains “everyone disregarded John” as a
constituent part, as does 10.Inother words, the deep structures underlying 9
and10should both be identical (or very similar) to one of two deep structures
underlying 4,despite the wide divergence in surface structure and phonetic
form. It follows that we cannot expect deep structure to be very close to surfacestructure in general.
In the case of a sentence like 6(“John saw Bill”), there is little difference
between deep and surface structure. Semantic interpretation would not be farfrom the mark, in this case, if it were quite parallel to phonetic interpretation.Thus the interpretation of “saw Bill” can be derived from that of “saw”
22and
that of “Bill,” and the interpretation of 6can be determined from that of “John”
and that of “saw Bill.” To carry out such interpretation we must know not onlythe bracketing of 6into constituents, but also the grammatical relations that
are represented; that is, we must know that “Bill” is the direct-object of “saw”
and that the subject–predicate relation holds between “John” and “saw Bill” in“John saw Bill.” Similarly, in the slightly more complex case of “John saw Billleave,” we must know that the subject–predicate relation holds between “John”and “saw Bill leave” and also between “Bill” and “leave.”
21The latter is again ambiguous in an entirely different way from 4,depending on the reference
of “him.” We will assume, throughout, that it refers to John.
22But the interpretation of this depends on that of “see” and that of “past tense”; hence, these
separate items must be represented in the deep structure, though not, in this case, in the surfacestructure.

--- Page 141 ---

122 Language and Mind
Notice that at least in such simple cases as 6,wealready have a mechanism for
representing grammatical relations of just the sort that are required for semantic
interpretation. Suppose that we deﬁne the relations subject-of as the relation
holding between a noun phrase and a sentence of which it is an immediateconstituent
23and the relation predicate-of as holding between a verb phrase
and a sentence of which it is an immediate constituent. The subject–predicaterelation can then be deﬁned as the relation holding between the subject of asentence and the predicate of this sentence. Thus, in these terms, “John” isthe subject and “saw Bill (leave)” the predicate of “John saw Bill (leave),”and the subject–predicate relation holds between the two. In the same way, wecan deﬁne the relation direct-object (in terms of the immediate constituency of
verb and noun phrase in verb phrase) and others in a perfectly appropriate and
satisfactory way. But returning now to 6,this observation implies that a labeled
bracketing will serve as the deep structure (just as a labeled bracketing will serve
as the surface structure); it contains just the information about constituency andabout grammatical relations that is required for semantic interpretation.
Wenoted that in “John saw Bill leave” the subject–predicate relation holds
between “Bill” and “leave,” as well as between “John” and “saw Bill leave.” If6or something very much like it – see, for example, note 22–i st ob e taken as
the deep structure, with grammatical relations deﬁned as previously, then thedeep structure of “John saw Bill leave” will have to be something like 11(many
details omitted):
11/bracketleftbig
s/bracketleftbig
np
John/bracketrightbig
np/bracketleftbig
vp/bracketleftbig
vsaw/bracketrightbig
v/bracketleftbig
s/bracketleftbig
npBill/bracketrightbig
np/bracketleftbig
vp/bracketleftbig
vleave/bracketrightbig
v/bracketrightbig
vp/bracketrightbig
s/bracketrightbig
vp/bracketrightbig
s
The labeled bracketing 11expresses the subject–predicate relation between
“John” and “saw Bill leave” and between “Bill” and “leave,” as required.
Moving to a somewhat more complex example, the sentences 9and10(as
well as 4under one interpretation) will each have to contain something like 12
in the deep structure:12/bracketleftbig
s/bracketleftbig
np
everyone/bracketrightbig
np/bracketleftbig
vp/bracketleftbig
vdisregards/bracketrightbig
v/bracketleftbig
npJohn/bracketrightbig
np/bracketrightbig
vp/bracketrightbig
s
If this requirement is met, then we will be able to account for the fact that,
obviously, the meaning of 4(=“what disturbed John was being disregarded
by everyone”) in one interpretation of 9(=“being disregarded by everyone
disturbed John”) is determined in part by the fact that the direct-object rela-tion holds between “disregard” and “John” and the subject–predicate relation
23Aphrase Xis an immediate constituent of the phrase Ycontaining Xif there is no phrase Zwhich
contains Xand is contained in Y.Thus, the noun phrase “John” is an immediate constituent of the
sentence “John saw Bill” [analyzed as in 6], but the noun phrase “Bill” is not, being contained in
the intervening phrase “saw Bill.” “John saw” is not an immediate constituent of the sentence,
since it is not a phrase; “John” is not an immediate constituent of “John saw,” since the latter isnot a phrase. Notice that the deﬁnitions proposed here for grammatical functions and relationsmake sense only when restricted to deep structures, in general.

--- Page 142 ---

The formal nature of language 123
between “everyone” and “disregards John,” despite the fact that these relations
are in no way indicated in the surface structure in 4or9.
From many such examples, we are led to the following conception of how the
semantic component functions. This interpretive component of the full genera-tive grammar applies to a deep structure and assigns to it a semantic representa-tion, formulated in terms of the still quite obscure notions of universal seman-tics. The deep structure is a labeled bracketing of minimal “meaning-bearing”elements. The interpretive rules apply cyclically, determining the semantic inter-pretation of a phrase Xof the deep structure from the semantic interpretations
of the immediate constituents of Xand the grammatical relation represented in
this conﬁguration of Xand its parts.
Superﬁcially, at least, the two interpretive components of the grammar are
rather similar in the way in which they operate, and they apply to objects ofessentially the same sort (labeled bracketings). But the deep structure of asentence will, in nontrivial cases, be quite different from its surface structure.
Notice that if the notions “noun phrase,” “verb phrase,” “sentence,” “verb,”
can receive a language-independent characterization within universal grammar,then the grammatical relations deﬁned above (similarly, others that we mightdeﬁne in the same way) will also receive a universal characterization. It seemsthat this may be possible, and certain general lines of approach to such a charac-terization seem clear (see p. 139). We might then raise the question of whetherthe semantic component of a grammar contains such particular rules as the rules7and8of the phonological component of English or whether, alternatively, the
principles of semantic interpretation belong essentially to universal grammar.However, we will put aside these and other questions relating to the semanticcomponent, and turn next to the discussion of the one noninterpretive compo-nent of the grammar – which we have called its “syntactic component.” Noticethat as in the case of the phonological component, insofar as principles of inter-pretation can be assigned to universal rather than particular grammar, there islittle reason to suppose that they are learned or that they could in principle belearned.
Structure of the syntactic component
The syntactic component of a grammar must generate (see note 12)pairs ( D,S),
where Dis a deep structure and San associated surface structure. The surface
structure Sis a labeled bracketing of a sequence of formatives and junctures.
The deep structure Dis a labeled bracketing that determines a certain network
of grammatical functions and grammatical relations among the elements andgroups of elements of which it is composed. Obviously, the syntactic compo-nent must have a ﬁnite number of rules (or rule schemata), but these must be soorganized that an inﬁnite number of pairs ( D,S)ofdeep and surface structures

--- Page 143 ---

124 Language and Mind
can be generated, one corresponding to each interpreted sentence (phonetically
and semantically interpreted, that is) of the language.24In principle, there are
various ways in which such a system might be organized. It might, for example,
consist of independent rules generating deep and surface structures and certainconditions of compatibility relating them, or of rules generating surface struc-tures combined with rules mapping these into the associated deep structure,or of rules generating deep structures combined with rules mapping these intosurface structures.
25Choice among these alternatives is a matter of fact, not
decision. We must ask which of the alternatives makes possible the deepestgeneralizations and the most far-reaching explanation of linguistic phenomenaof various sorts. As with other aspects of universal grammar, we are dealing herewith a set of empirical questions; crucial evidence may be difﬁcult to obtain,butwecannot conclude from this that there is, in principle, no right and wrong
in the matter.
Of the many alternatives that might be suggested, the linguistic evidence
now available seems to point consistently to the conclusion that the syntacticcomponent consists of rules that generate deep structures combined with rulesmapping these into associated surface structures. Let us call these two systems ofrules the base and the transformational components of the syntax, respectively.
The base system is further subdivided into two parts: the categorial system and
thelexicon. Each of these three subparts of the syntax has a speciﬁc function to
perform, and there seem to be heavy universal constraints that determine theirform and interrelation. The general structure of a grammar would, then, be asdepicted in diagram 13:
13
The mapping Sis carried out by the semantic component; Tby the transforma-
tional component; and Pby the phonological component. Generation of deep
structures by the base system (by operation B)isdetermined by the categorial
system and the lexicon.
The lexicon is a set of lexical entries; each lexical entry, in turn, can be
regarded as a set of features of various sorts. Among these are the phonological
24In fact, we might think of a grammar as assigning a semantic interpretation to all possible
sentences (this being a clear notion, given theories of universal phonetics and semantics),including those that deviate from rules of the language. But this is a matter that we will not gointo any further here.
25The question of how the syntactic component is organized should not be confused, as it all toooften is, with the problem of developing a model of performance (production or perception). Infact, any of the kinds of organization just described (and others) could be used as the basis for
atheory of performance of either kind.

--- Page 144 ---

The formal nature of language 125
features and the semantic features that we have already mentioned brieﬂy.
The phonological features can be thought of as indexed as to position (thatis, ﬁrst, second, etc.); aside from this, each is simply an indication of markingwith respect to one of the universal distinctive features (regarded here in theircategorial function) or with respect to some diacritic feature (see p. 114), inthe case of irregularity. Thus the positionally indexed phonological featuresconstitute a distinctive feature matrix with the entries given as +or – values,
as described earlier. The semantic features constitute a “dictionary deﬁnition.”As noted previously, some of these at least must be quite abstract; there may,furthermore, be intrinsic connections of various sorts among them that aresometimes referred to as “ﬁeld structure.” In addition, the lexical entry containssyntactic features that determine the positions in which the entry in questionmay appear, and the rules that may apply to structures containing it as theseare converted into surface structures. In general, the lexical entry contains allinformation about the item in question that cannot be accounted for by generalrule.
Aside from lexical entries, the lexicon will contain redundancy rules that
modify the feature content of a lexical entry in terms of general regularities.Forexample, the fact that vowels are voiced or that humans are animate requires
no speciﬁc mention in particular lexical entries. Much of the redundant lexicalinformation can, no doubt, be provided by general conventions (that is, rules ofuniversal grammar) rather than by redundancy rules of the language.
The lexicon is concerned with all properties, idiosyncratic or redundant, of
individual lexical items. The categorial component of the base determines allother aspects of deep structure. It seems that the categorial component is whatis called a simple orcontext-free phrase-structure grammar. Just what such a
system is can be understood quite easily from a simple example. Suppose thatwe have the rules 14:
14 S→NP VP
VP→VN P
NP→N
N→/Delta1
V→/Delta1
With these rules we construct the derivation 15in the following way. First write
down the symbol Sas the ﬁrst line of the derivation. We interpret the ﬁrst rule
of14as permitting Sto be replaced by NP VP, giving the second line of 15.
Interpreting the second rule of 14in a similar way, we form the third line of
the derivation 15with VP replaced by V NP. We form the fourth line of 15
by applying the rule NP →No f 14,interpreted the same way, to both of the
occurrences of NP in the third line. Finally, we form the ﬁnal two lines of 15
by applying the rules N →/Delta1and V →/Delta1.

--- Page 145 ---

126 Language and Mind
15 S
NP VPNPVN P
NVN/Delta1V/Delta1
/Delta1/Delta1/Delta1
Clearly, we can represent what is essential to the derivation 15by the tree
diagram 16.
16
In the diagram 16,each symbol dominates the symbols by which it was replaced
in forming 15.Infact, we may think of the rules of 14as simply describing the
wayinwhich a tree diagram such as 16can be constructed. Evidently, 16is just
another notation for the labeled bracketing 17:
17/bracketleftbig
s/bracketleftbig
np/bracketleftbig
n/Delta1/bracketrightbig
n/bracketrightbig
np/bracketleftbig
vp/bracketleftbig
v/Delta1/bracketrightbig
v/bracketleftbig
np/bracketleftbig
n/Delta1/bracketrightbig
n/bracketrightbig
np/bracketrightbig
vp/bracketrightbig
s
Domination of some element by a symbol A in 16(as, for example, V NP is
dominated by VP) is indicated in 17by enclosing this element by the labeled
brackets [ a,]a.Ifwehavealexicon which tells us that “John” and “Bill” can
replace the symbol /Delta1when this symbol is dominated by N (that is, is enclosed
by [n,]n), and that “saw” can replace /Delta1when it is dominated by V , then we
can extend the derivation 15to derive “John saw Bill,” with the associated
structure that we have given as 6.Infact,6,derives from 17by replacing the
ﬁrst occurrence of /Delta1by “John,” the second by “saw,” and the third by “Bill.”
Notice that the rules 14in effect deﬁne grammatical relations, where the deﬁ-
nitions are given as on pp. 121–22. Thus, the ﬁrst rule of 14deﬁnes the subject–
predicate relation and the second, the verb–object relation. Similarly, othersemantically signiﬁcant grammatical functions and relations can be deﬁned byrules of this form, interpreted in the manner indicated.
Restating these notions in a more formal and general way, the categorial
component of the base is a system of rules of the form A→Z,where Ais a
category symbol such as S (for “sentence”), NP (for “noun phrase”), N (for“noun”), etc., and Z is a string of one or more symbols which may again becategory symbols or which may be terminal symbols (that is, symbols which
do not appear on the left-hand side of the arrow in any base rule). Given such

--- Page 146 ---

The formal nature of language 127
asystem, we can form derivations ,aderivation being a sequence of lines that
meets the following conditions: the ﬁrst line is simply the symbol S(standing for
sentence); the last line contains only terminal symbols; if X, Yare two successive
lines, then Xmust be of the for m... A...a n d Yoftheform... Z...,where A→
Zis one of the rules. A derivation imposes a labeled bracketing on its terminal
string in the obvious way. Thus given the successive lines X=...A...,
Y=...Z...,where Ywasderived from Xby the rule A→Z,wewill
say that the string derived from Z(orZitself, if it is terminal) is bracketed by
[a,]a.Equivalently, we can represent the labeled bracketing by a tree diagram
in which a node labeled A(in this example) dominates the successive nodes
labeled by the successive symbols of Z.
Weassume that one of the terminal symbols of the categorial component is
the dummy symbol /Delta1.Among the nonterminal symbols are several that stand
forlexical categories ,inparticular N (for “noun”), V (for “verb”), ADJ (for
“adjective”). A lexical category Acan appear on the left-hand side of a rule
A→Zonly if Zis/Delta1.Lexical entries will then be inserted in derivations in
place of /Delta1by rules of a different sort, extending the derivations provided by the
categorial component. Aside from /Delta1,indicating the position in which an item
from the lexicon may appear, the terminal symbols of the categorial component
are grammatical elements such as be,of,etc. Some of the terminal symbols
introduced by categorial rules will have an intrinsic semantic content.
Alabeled bracketing generated by base rules (that is, by the phrase-structure
rules of the categorial component and by the rule of lexical insertion men-tioned in the preceding paragraph) will be called a base phrase-marker. More
generally, we will use the term “phrase-marker” here to refer to any string ofelements properly bracketed with labeled brackets.
26The rules of the transfor-
mational component modify phrase-markers in certain ﬁxed ways. These rulesare arranged in a sequence T
1,..., Tm.This sequence of rules applies to a
base phrase-marker in a cyclic fashion. First, it applies to a conﬁguration domi-nated by S (that is, a conﬁguration [ s... ]s)and containing no other occurrence
of S. When the transformational rules have applied to all such conﬁgurations,then they next apply to a conﬁguration dominated by S and containing onlyS-dominated conﬁgurations to which the rules have already applied. This pro-cess continues until the rules apply to the full phrase-marker dominated bythe initial occurrence of S in the base phrase-marker. At this point, we have asurface structure. It may be that the ordering conditions on transformations arelooser – that there are certain ordering conditions on the set {T
1,..., Tm},and
that at a given stage in the cycle, a sequence of transformations can applyif it does not violate these conditions – but I will not go into this matterhere.
26It may be that a slightly more general notion of “phrase-marker” is needed, but we will put this
question aside here.

--- Page 147 ---

128 Language and Mind
The properties of the syntactic component can be made quite clear by an
example (which, naturally, must be much oversimpliﬁed). Consider a subpart
of English with the lexicon 18and the categorial component 19.
18 Lexicon: it, fact, John, Bill, boy,
future(Noun)
dream, see, persuade,
annoy(Verb)
sad (Adjective)
will (Modal)
the (Determiner)
19 S→(Q) NP AUX VP
VP→be ADJ
VP→V(NP) (of NP)
NP→(DET) N (that S)
AUX→past
AUX→M
N, V , ADJ, DET, M →/Delta1
In19,parentheses are used to indicate an element that may or may not be
present in the rule. Thus the ﬁrst line of 19is an abbreviation for two rules,
one in which Sis rewritten QN PA UX VP ,the other in which Sis rewritten NP
AUX VP. Similarly, the third line of 19is actually an abbreviation for four rules,
etc. The last line of 19stands for ﬁve rules, each of which rewrites one of the
categorial symbols on the left as the dummy terminal symbol /Delta1.
This categorial component provides such derivations as the following:
20 a. SNP AUX VPNP AUX be ADJNA U X be ADJ
Npast be ADJ
/Delta1past be /Delta1
b.SNP AUX VPNP AU XVN Po fN P
DETNA U XVNo fD E TN that S
DETNMVNo fD E TN that S
/Delta1/Delta1/Delta1/Delta1/Delta1 of/Delta1/Delta1 that S
/Delta1/Delta1/Delta1/Delta1/Delta1 of/Delta1/Delta1 that NP VP
/Delta1/Delta1/Delta1/Delta1/Delta1 of/Delta1/Delta1 that NP AUX V
/Delta1/Delta1/Delta1/Delta1/Delta1 of/Delta1/Delta1 that N AUX V
/Delta1/Delta1/Delta1/Delta1/Delta1 of/Delta1/Delta1 that N past V
/Delta1/Delta1/Delta1/Delta1/Delta1 of/Delta1/Delta1 that/Delta1past/Delta1

--- Page 148 ---

The formal nature of language 129
These derivations are constructed in the manner just described. They impose
labeled bracketings which, for clarity, we will give in the equivalent treerepresentation:
21 a.
b.
Wenow use the lexicon to complete the base derivations 20a,20b.Each entry
in the lexicon contains syntactic features which identify the occurrences of /Delta1
that it can replace in a derivation. For example, the items of the ﬁve rows of 18
can replace occurrences of /Delta1that are dominated, in the tree representations of
21,bythe categorial symbols N, V , ADJ, M, DET, respectively.
But the restrictions are much narrower than this. Thus of the verbs in 18
(line 2), only persuade can replace an occurrence of /Delta1dominated by V when this
occurrence of V is followed in the VP by: NP of NP. We can form “. . . persuade
John of the fact ,” but not “. . . dream (see,annoy )John of the fact. ”Similarly,
of the nouns in 18(ﬁrst line) only factcan appear in the context DET – thatS
(that is, “the fact that John left”); only itin a NP of the form – thatS;27only
fact,boy,andfuture in a NP of the form DET – ( “the fact,” “the boy,” “the
future”), etc. Details aside, the general character of such restrictions is quite
clear. Assuming, then, that the lexical entries contain the appropriate lexicalfeatures, we can extend the base derivations of 20to give the terminal strings
22,inserting the items enclosed in brackets in 21.
22 a. John past be sad
b.the boy will persuade John of the fact that Bill past dream
27This may not seem obvious. We return to the example directly.

--- Page 149 ---

130 Language and Mind
Wecan also form such terminal strings as 23,with other choices in deriva-
tions.
23 Qthe boy will dream of the future
it that John past see Bill past annoy the boy
John will be sad
John past see the future
In this way, we form full base derivations, using the rules of the categorialcomponent and then substituting lexical entries for particular occurrences ofthe dummy symbol /Delta1in accordance with the syntactic features of these lexical
entries. Correspondingly, we have the labeled brackets represented as 21,with
lexical entries substituted for occurrences of /Delta1in the permitted ways. These
are the base phrase-markers.
Notice that the rules that introduce lexical entries into base phrase-markers
are entirely different in character from the rules of the categorial component. Therules of 19that were used to form 20are of a very elementary sort. Each such rule
allows a certain symbol Ain the string . . . A...toberewritten as a certain string
Z,independently of the context of A and the source of A in the derivation. But
in introducing lexical entries in place of /Delta1,wemust consider selected aspects
of the phrase-marker in which /Delta1appears. For example, an occurrence of /Delta1can
be replaced by “John” if it is dominated in the phrase-marker by N, but not byV.Thus the rules of lexical insertion really apply not to strings of categorial and
terminal symbols, as do the rules of the categorial component, but to phrase-markers such as 21.Rules which apply to phrase-markers, modifying them
in some speciﬁc way, are referred to in current terminology as (grammatical)
transformations. Thus the rules of lexical insertion are transformational rules,
whereas the rules of the categorial component are simply rewriting rules.
Let us now return to the examples 22a,22b.Consider ﬁrst 22a,with the
base phrase-marker 21a.
28Wesee at once that 21contains just the information
required in the deep structure of the sentence “John was sad.” Clearly, thestring past be is simply a representation of the formative “was,” just as past
seerepresents “saw,” past persuade represents “persuaded,” etc. With a rule
that converts past be to the formative “was,” we form the surface structure of
the sentence “John was sad.” Furthermore, if we deﬁne grammatical functionsand relations in the manner described earlier (see pp. 121–22), 21expresses the
fact that the subject–predicate relation holds between Johnandpast be sad ,and
it also contains semantic information about the meaning-bearing items John,
past,sad;wemay assume, in fact, that past is itself a symbol of a universal
terminal alphabet with a ﬁxed semantic interpretation, and the semantic featuresof the lexical entries of John andsadcan also be assumed to be selected,
like the phonological features of these entries, from some universal system of
28Wehenceforth suppose 21aand21b to be extended to full phrase-markers by insertion of
appropriate lexical entries, as indicated.

--- Page 150 ---

The formal nature of language 131
representation of the sort discussed above. In short, 21acontains all information
required for semantic interpretation, and we can, therefore, take it to be the deep
structure underlying the sentence “John was sad.”
What is true of this example is true quite generally. That is, the base phrase-
markers generated by the categorial component and the lexicon are the deepstructures that determine semantic interpretation. In this simple case, only onerule is needed to convert the deep structure to a surface structure, namely, therule converting past be to the formative was. Since this rule is clearly a special
case of a rule that applies as well to any string of the form past V,it is really
avery simple transformational rule (in the terminology just given) rather than
an elementary rule of the type that we ﬁnd in the categorial component. Thisobservation can be generalized. The rules that convert deep structures to surfacestructures are transformational rules.
Suppose now that instead of the derivation 20awe had formed the very similar
derivation 20:
24 SQN PA U XV PQN PA U X be ADJ
QNA U X be ADJ
QNMb eA D JQ/Delta1/Delta1 be/Delta1
QJohn will be sad
with its associated phrase-marker. We intend the symbol Qto be a symbol of
the universal terminal alphabet with a ﬁxed semantic interpretation, namely,that the associated sentence is a question. Suppose that the transformationalcomponent of the syntax contains rules that convert phrase-markers of the formQN PA U X ...tocorresponding phrase-markers of the form AUX NP ...(that
is, the transformation replaces QbyAUX ,leaving the phrase-marker otherwise
unchanged). Applied to the phrase-marker corresponding to 24,this rule gives
the labeled bracketing of the sentence “Will John be sad?”; that is, it forms thesurface structure for this sentence.
Suppose that in place of 24we had used the rule rewriting AUX as past. The
question transformation of the preceding paragraph would give a phrase-markerwith the terminal string “past John be sad,” just as it gives “Will John be sad?”in the case of 24.Evidently, we must modify the question transformation so
that it inverts not just past,inthis case, but the string past be ,sothat we derive
ﬁnally, “Was John sad?” This modiﬁcation is, in fact, straightforward, when therules are appropriately formulated.
Whether we select M or past in24,the generated base phrase-marker once
again qualiﬁes as a deep structure. The grammatical relation of Johntowill
(past)be sad is exactly the same in 24as in 20a,with the deﬁnitions proposed
previously, as required for empirical adequacy. Of course, the surface forms

--- Page 151 ---

132 Language and Mind
do not express these grammatical relations, directly; as we have seen earlier,
signiﬁcant grammatical relations are rarely expressed directly in the surfacestructure.
Let us now turn to the more complex example 20b–21b–22b.Once again,
the base phrase-marker 21bof22bexpresses the information required for the
semantic interpretation of the sentence “The boy will persuade John of thefact that Bill dreamt,” which derives from 22bby a transformational rule that
forms “dreamt” from past dream. Therefore, 21bcan serve as the deep structure
underlying this sentence, exactly as 21acan serve for “John was sad,” and the
phrase-marker corresponding to 24for “Will John be sad?”
Suppose that in rewriting NP in the third line of 20b,wehad selected not
DET N thatSb u t NthatS[see the fourth line of 19]. The only lexical item
of18that can appear in the position of this occurrence of N is it.Therefore,
instead of 22b,wewould have derived
25 the boy will persuade John of it that Bill past dream,
with grammatical relations and lexical content otherwise unmodiﬁed. Supposenow that the transformational component of the syntax contains rules with thefollowing effect:
26 a. itis deleted before thatS
b.ofis deleted before thatS
Applying 26aand26bto25in that order, with the rule that converts past dream
to “dreamt,” we derive the surface structure of “The boy will persuade John that
Bill dreamt.” The base phrase-marker corresponding to 25serves as the deep
structure underlying this sentence.
Notice that the rule 26ais much more general. Thus suppose we select the
NPit that Bill past dream as the subject of past annoy John ,asispermitted by
the rules 18,19.This gives
27 it that Bill past dream past annoy John
Applying the rule 26a(and the rules for forming past tense of verbs), we derive,
“That Bill dreamt annoyed John.” Alternatively, we might have applied thetransformational rule with the effect of 28:
28 Aphrase-marker of the form it that S X is restructured as the corre-
sponding phrase-marker of the form it X that S.
Applying 28to27,wederive “It annoyed John that Bill dreamt.” In this case, 26a
is inapplicable. Thus 27underlies two surface structures, one determined by 28
and the other by 26a;having the same deep structure, these are synonymous. In
the case of 25,28is inapplicable and, therefore, we have only one corresponding
surface structure.
Wecan carry the example 25further by considering additional transforma-
tional rules. Suppose that instead of selecting Billin the embedded sentence of

--- Page 152 ---

The formal nature of language 133
25,wehad selected Johnasecond time. There is a very general transformational
rule in English and other languages providing for the deletion of repeated items.
Applying this rule along with other minor ones of an obvious sort, we derive
29 The boy will persuade John to dream
from a deep structure that contains, as it must, a subphrase-marker that expresses
the fact that Johnis the subject of dream. Actually, in this case the deep phrase-
marker would be slightly different, in ways that need not concern us here, inthis rough expository sketch.
Suppose now that we were to add a transformation that converts a phrase-
marker of the form NP AUX V NP into the corresponding passive, in the obviousway.
29Applying to phrase-markers very much like 21b,this rule would provide
surface structures for the sentences “John will be persuaded that Bill dreamt (bythe boy)” [from 25]and “John will be persuaded to dream (by the boy)” [from
29]. In each case, the semantic interpretation will be that of the underlying
deep phrase-marker. In certain cases, the signiﬁcant grammatical relations areentirely obscured in the surface structure. Thus in the case of the sentence“John will be persuaded to dream,” the fact that “John” is actually the subjectof “dream” is not indicated in the surface structure, although the underlyingdeep structure, as we have noted, expresses this fact directly.
From these examples we can see how a sequence of transformations can
form quite complicated sentences in which signiﬁcant relations among the partsare not represented in any direct way. In fact, it is only in artiﬁcially simpleexamples that deep and surface structure correspond closely. In the normal
sentences of everyday life, the relation is much more complex; long sequencesof transformations apply to convert underlying deep structures into the surfaceform.
The examples that we have been using are stilted and unnatural. With a less
rudimentary grammar, quite natural ones can be provided. For example, in placeof the sentences formed from 27by26or28we could use more acceptable
sentences such as “That you should believe this is not surprising,” “It is notsurprising that you should believe this,” etc. Actually, the unnaturalness of theexamples we have used illustrates a simple but often neglected point, namely,
that the intrinsic meaning of a sentence and its other grammatical properties aredetermined by rule, not by conditions of use, linguistic context, frequency ofparts, etc.
30Thus the examples of the last few paragraphs may never have been
29Notice that this transformation would modify the phrase-marker to which it applies in a more
radical way than those discussed above. The principles remain the same, however.
30These factors may affect performance, however. Thus they may affect the physical signal andplay a role in determining how a person will interpret sentences. In both producing and under-standing sentences, the speaker–hearer makes use of the ideal phonetic and semantic interpreta-tions, but other factors also play a role. The speaker may be simply interested in making himselfunderstood–the hearer, in determining what the speaker intended (which may not be identical

--- Page 153 ---

134 Language and Mind
produced in the experience of some speaker (or, for that matter, in the history
of the language), but their status as English sentences and their ideal phoneticand semantic interpretations are unaffected by this fact.
Since the sequence of transformations can effect drastic modiﬁcations in a
phrase-marker, we should not be surprised to discover that a single structure
31
may result from two very different deep structures – that is, that certain sen-tences are ambiguous (for example, sentence 4on p. 110). Ambiguous sentences
provide a particularly clear indication of the inadequacy of surface structure asarepresentation of deeper relations.
32
More generally, we can easily ﬁnd paired sentences with essentially the same
surface structure but entirely different grammatical relations. To mention justone such example, compare the sentences of 30:
30 a. Ipersuaded the doctor to examine John.
b.Iexpected the doctor to examine John.
The surface structures are essentially the same. The sentence 30ais of the same
form as 29.Itderives from a deep structure which is roughly of the form 31:
31 Ipast persuade the doctor of it that the doctor AUX examine John
with the literal semantic interpretation of the sentence or sentence fragment that he produced).
Once again, we must insist on the necessity for distinguishing performance from competenceif either is to be studied in a serious way.
31More accurately, surface structures that are sufﬁciently close so as to determine the samephonetic representation.
32Modern linguistics has made occasional use of this property of language as a research tool.The ﬁrst general discussion of how ambiguity can be used to illustrate the inadequacy ofcertain conceptions of syntactic structure is in C. F. Hockett’s “Two Models for GrammaticalDescription,” Word ,Vol. 10, 1954, pp. 210–31, reprinted in M. Joos, ed., Readings in Linguistics
One,4th edn. (Chicago: University of Chicago Press, 1966).

--- Page 154 ---

The formal nature of language 135
This deep structure is essentially the same as 21b,and by the transformational
process described in connection with 29,wederive from it the sentence 30a.
But in the case of 30bthere are no such related structures as “I expected the
doctor of the fact that he examined John,” “ ...o ft h e necessity (for him) to
examine John,” etc., as there are in the case of 30a.Correspondingly, there is no
justiﬁcation for an analysis of 30bas derived from a structure like 31.Rather,
the deep structure underlying 30bwill be something like 32(again omitting
details):
32 Ipast expect it that the doctor AUX examine John
There are many other facts that support this analysis of 30aand30b.For exam-
ple, from a structure like 32we can form “What I expected was that the doctor
(will, should, etc.) examine John,” by the same rule that forms “What I saw
wasthe book,” from the underlying NP-V-NP structure “I saw the book.” But
we cannot form “What I persuaded was that the doctor should examine John,”corresponding to 30a,because the underlying structure 31is not of the form
NP-V-NP as required by this transformation. Applying rule 26ato32,wederive
“I expected that the doctor (will, should, etc.) examine John.” We derive 30b,
instead, by the use of the same rule that gives 29,with “to” rather than “that”
appearing with the embedded sentence, which, in this case, contains no otherrepresentative of the category AUX.
Details aside, we see that 30ais derived from 31and30bfrom 32,sothat
despite near identity of surface structure, the deep structures underlying 30aand
30bare very different. That there must be such a divergence in deep structure
is not at all obvious.
33It becomes clear, however, if we consider the effect of
replacing “the doctor to examine John” by its passive, “John to be examined bythe doctor,” in 30aand30b.Thus we have under examination the sentences 33
and34:
33It seems, in fact, that this phenomenon has escaped the attention of English grammarians, both
traditional and modern.

--- Page 155 ---

136 Language and Mind
33 a. Ipersuaded the doctor to examine John [ =30a].
b.Ipersuaded John to be examined by the doctor.
34 a. Iexpected the doctor to examine John [ =30b].
b.Iexpected John to be examined by the doctor.
The semantic relation between the paired sentences of 34is entirely different
from the relation between the sentences of 33.Wecan see this by considering the
relation in truth value. Thus 34aand34bare necessarily the same in truth value;
if I expected the doctor to examine John then I expected John to be examined
by the doctor, and conversely. But there is no necessary relation in truth valuebetween 33aand33b.IfIpersuaded the doctor to examine John, it does not
follow that I persuaded John to be examined by the doctor, or conversely.
In fact, exchange of active and passive in the embedded sentence preserves
meaning, in a rather clear sense, in the case of 30bbutnot30a.The explanation is
immediate from consideration of the deep structures underlying these sentences.Replacing active by passive in 32,wethen go on to derive 34bin just the way
that30bis derived from 32.But to derive 33b,wemust not only passivize the
embedded sentence in 31,but we must also select “John” instead of “the doctor”
as the object of the verb “persuade”; otherwise, the conditions for deletion of therepeated noun phrase, as in the derivation of 29,will not be met. Consequently,
the deep structure underlying 33bis quite different from that underlying 33a.Not
only is the embedded sentence passivized, but the object “the doctor” must bereplaced in 31by “John.” The grammatical relations are, consequently, quite
different, and the semantic interpretation differs correspondingly. It remainstrue, in both cases, that passivization does not affect meaning (in the sense of“meaning” relevant here). The change of meaning in 30awhen “the doctor to
examine John” is replaced by “John to be examined by the doctor” is occasioned
by the change of grammatical relations, “John” now being the direct object ofthe verb phrase in the underlying structure rather than “the doctor.” There is nocorresponding change in the case of 34a,sothat the meaning remains unaltered
when the embedded sentence is passivized.
The example 30a,30b illustrates, once again, the inadequacy (and, quite
generally, irrelevance) of surface structure for the representation of semanti-cally signiﬁcant grammatical relations. The labeled bracketing that conveysthe information required for phonetic interpretation is in general very differentfrom the labeled bracketing that provides the information required for semanticinterpretation. The examples 30a,30balso illustrate how difﬁcult it may be to
bring one’s “linguistic intuition” to consciousness. As we have seen, the gram-mar of English, as a characterization of competence (see pp. 102f.), must, fordescriptive adequacy, assign different deep structures to the sentences 30aand
30b.The grammar that each speaker has internalized does distinguish these
deep structures, as we can see from the fact that any speaker of English is

--- Page 156 ---

The formal nature of language 137
capable of understanding the effect of replacing the embedded sentence by its
passive in the two cases of 30.But this fact about his internalized grammatical
competence may escape even the careful attention of the native speaker (seenote 33).
Perhaps such examples as these sufﬁce to give something of the ﬂavor of
the syntactic structure of a language. Summarizing our observations about thesyntactic component, we conclude that it contains a base and a transformationalpart. The base generates deep structures, and the transformational rules con-vert them to surface structures. The categorial component of the base deﬁnes
the signiﬁcant grammatical relations of the language, assigns an ideal orderto underlying phrases, and, in various ways, determines which transforma-tions will apply.
34The lexicon speciﬁes idiosyncratic properties of individual
lexical items. Together, these two components of the base seem to providethe information relevant for semantic interpretation in the sense in which wehave been using this term, subject to the qualiﬁcations mentioned earlier. Thetransformational rules convert phrase-markers to new phrase-markers, affectingvarious kinds of reordering and reorganization. The kinds of changes that can
be effected are quite limited; we will, however, not go into this matter here.Applying in sequence, the transformations may affect the organization of abase phrase-marker quite radically, however. Thus the transformations provideawide variety of surface structures that have no direct or simple relation to
the base structures from which they originate and which express their semanticcontent.
It is a fact of some signiﬁcance that the mapping of deep to surface structures
is not a matter of a single step but is, rather, analyzable into a sequence ofsuccessive transformational steps. The transformations that contribute to thismapping of deep to surface structures can be combined in many different ways,depending on the form of the deep structure to which they apply. Since thesetransformations apply in sequence, each must produce a structure of the sortto which the next can apply. This condition is met in our formulation, sincetransformations apply to phrase-markers and convert them into new phrase-markers. But there is very good empirical evidence that the surface structuresthat determine phonetic form are, in fact, phrase-markers (that is, labeled brack-eting of formatives). It follows, then, that the deep structures to which trans-formations originally apply should themselves be phrase-markers, as in ourformulation.
In principle, there are many ways in which a network of grammatical relations
might be represented. One of the major reasons for selecting the method ofphrase-markers generated by base rules is precisely the fact that transformationsmust apply in sequence and therefore must apply to objects of the sort that they
34It is an open question whether this determination is unique.

--- Page 157 ---

138 Language and Mind
themselves produce, ultimately, to phrase-markers that have the same formal
properties as surface structures.35
Concluding observations
The grammatical theory just presented calls for several comments. We pointedout earlier that the grammar of a language must, for empirical adequacy, allowfor inﬁnite use of ﬁnite means, and we assigned this recursive property to thesyntactic component, which generates an inﬁnite set of paired deep and surfacestructures. We have now further localized the recursive property of the grammar,assigning it to the categorial component of the base. Certain base rules introducethe initial symbol Sthat heads derivations, for example, the fourth rule of 19.
It may be that introduction of “propositional content” in deep structures bythis means is the only recursive device in the grammar apart from the rulesinvolved in forming coordinated constructions, which raise various problemsgoing beyond what we have been discussing here.
It is reasonable to ask why human languages should have a design of this
sort – why, in particular, they should use grammatical transformations of thesort described to convert deep structures to surface form. Why should theynot make use of deep structures in a more direct way?
36Tworeasons suggest
themselves at once. We have already observed that the conditions of lexicalinsertion are essentially transformational rather than phrase-structural (see p.130). More generally, we ﬁnd many nonphrase-structural constraints (for exam-ple, those involved in deletion of identical items – see pp. 132 and 136) whenwe study a language carefully. Thus transformations not only convert a deepstructure to a surface structure, but they also have a “ﬁltering effect,” rulingout certain potential deep structures as not well-formed.
37Apart from this, we
would naturally be inclined to seek an explanation for the use of grammatical
transformations in the empirical constraints that linguistic communication mustmeet. Even the simple fact that sound is unrecoverable imposes conditions onspeech that need not, for example, be imposed on a linguistic system designedonly for writing (for example, the artiﬁcial systems mentioned in note 36).
Awritten system provides an “external memory” that changes the perceptual
problem in quite a signiﬁcant way. We would expect a system designed for the
35There are other supporting reasons. For one thing, grammatical relations are not among words
or morphemes but among phrases, in general. For another, empirical investigation has uniformlyshown that there is an optimal ideal order of phrases in underlying structures, consistent withthe assumption that these are generated by a base system of the sort discussed above.
36It is interesting to observe, in this connection, that the theory of context-free phrase-structuregrammar (see p. 125) is very close to adequate for “artiﬁcial languages” invented for variouspurposes, for example, for mathematics or logic or as computer languages.
37And hence, in certain cases, as underlying “semigrammatical sentences” that deviate, in theindicated way, from grammatical rule. This suggests one approach to the problem touched onin note 24.

--- Page 158 ---

The formal nature of language 139
conditions of speech communication to be somehow adapted to the load on
memory. In fact, grammatical transformations characteristically reduce theamount of grammatical structure in phrase-markers in a well-deﬁned way, andit may be that one consequence of this is to facilitate the problem of speechperception by a short-term memory of a rather limited sort.
38This observation
suggests some promising directions for further research, but little of substancecan be said with any conﬁdence on the basis of what is understood today.
One further point requires some clariﬁcation. We noted at the outset that per-
formance and competence must be sharply distinguished if either is to be studiedsuccessfully. We have now discussed a certain model of competence. It wouldbe tempting, but quite absurd, to regard it as a model of performance as well.Thus we might propose that to produce a sentence, the speaker goes through thesuccessive steps of constructing a base-derivation, line by line from the initialsymbol S,then inserting lexical items and applying grammatical transforma-
tions to form a surface structure, and ﬁnally applying the phonological rulesin their given order, in accordance with the cyclic principle discussed earlier.There is not the slightest justiﬁcation for any such assumption. In fact, in imply-ing that the speaker selects the general properties of sentence structure beforeselecting lexical items (before deciding what he is going to talk about), such aproposal seems not only without justiﬁcation but entirely counter to whatevervague intuitions one may have about the processes that underlie production. A
theory of performance (production or perception) will have to incorporate thetheory of competence – the generative grammar of a language – as an essentialpart. But models of performance can be constructed in many different ways,consistently with ﬁxed assumptions about the competence on which they arebased. There is much that can be said about this topic, but it goes beyond thebounds of this paper.
Specifying the properties of the various components and subcomponents of
agrammar precisely, along the lines outlined in this discussion, we formulate a
highly restrictive hypothesis about the structure of any human language. As wehave remarked several times, it is far from necessary, on any a priori grounds, thatalanguage must have a structure of this sort. Furthermore, it seems quite likely
that very heavy conditions can be placed on grammars beyond those outlinedabove. For example, it may be (as, in fact, was traditionally assumed) thatbase structures can vary only very slightly from language to language; and, bysufﬁciently restricting the possible range of base structures, it may be possible toarrive at quite general deﬁnitions for the categories that function as “nonterminalsymbols” in the rules of the categorial component. As observed previously, this
38Forsome speculations about this matter and discussion of the general problem, see G. A. Miller
and N. Chomsky, “Finitary Models for the User,” in R. D. Luce, E. Galanter, and R. Bush, eds.,
Handbook of Mathematical Psychology (New York: Wiley, 1963), V ol. II. The suggestion that
transformations may facilitate performance is implicit in V . Yngve, “A Model and a Hypothesisfor Language Structure,” Proceedings of the American Philosophical Society ,1960, pp. 444–66.

--- Page 159 ---

140 Language and Mind
would provide language-independent deﬁnitions of grammatical relations, and
would raise the possibility that there exist deep-seated universal principles of
semantic interpretation.
In mentioning such possibilities, we must take note of the widespread view
that modern investigations have not only conclusively refuted the principles of
traditional universal grammar but have, moreover, shown that the search forsuch principles was ill-conceived from the start. But it seems to me that suchconclusions are based on a serious misunderstanding of traditional universalgrammar, and on an erroneous interpretation of the results of modern work.Traditional universal grammar tried to demonstrate, on the basis of what infor-
mation was then available, that deep structures vary little from language tolanguage. That surface structures might be highly diverse was never doubted.It was also assumed that the categories of syntax, semantics, and phonetics areuniversal and quite restricted in variety. Actually, modern “anthropological lin-guistics” has provided little evidence that bears on the assumption of uniformityof deep structures, and insofar as the universality of categories is concerned,conclusions rather like the traditional ones are commonly accepted in practicein descriptive work.
39
Modern linguistics and anthropological linguistics have concerned them-
selves only marginally with deep structure, either in theory or practice. A greatdiversity of surface structures has been revealed in descriptive work, as antici-pated in traditional universal grammar. Nevertheless, a good case can be madefor the conclusion that the fundamental error of traditional universal grammarwasthat it was not sufﬁciently restrictive in the universal conditions it pro-
posed for human language – that much heavier constraints must be postulatedto account for the empirical facts.
Our discussion of the structure of English in the illustrative examples given
previously has necessarily been quite superﬁcial and limited to very simplephenomena. But even a discussion of the topics we have touched on requiresafairly intimate knowledge of the language and a reasonably well-articulated
theory of generative grammar. Correspondingly, it is only when problems ofthe sort illustrated are seriously studied that any contribution can be madeto the theory of universal grammar. Under these circumstances, it is not toosurprising that even today, the hypotheses of universal grammar that can beformulated with any conviction are supported by evidence from a fairly smallnumber of studies of very few of the languages of the world, and that theymust therefore be highly tentative. Still, the inadequacy of the evidence shouldnot be overstated. Thus it is surely true – and there is nothing paradoxical
39Traditional theories of universal phonetics have been largely accepted as a basis for modern
work, and have been reﬁned and ampliﬁed in quite important ways. See the references in
note 7.

--- Page 160 ---

The formal nature of language 141
in this – that a single language can provide strong evidence for conclusions
regarding universal grammar. This becomes quite apparent when we consideragain the problem of language acquisition (see p. 106). The child must acquireagenerative grammar of his language on the basis of a fairly restricted amount
of evidence.
40Toaccount for this achievement, we must postulate a sufﬁciently
rich internal structur e–a sufﬁciently restricted theory of universal grammar
that constitutes his contribution to language acquisition.
Forexample, it was suggested earlier that in order to account for the percep-
tion of stress contours in English, we must suppose that the user of the languageis making use of the principle of cyclic application. We also noted that he couldhardly have sufﬁcient evidence for this principle. Consequently, it seems reason-able to assume that this principle is simply part of the innate schematism that heuses to interpret the limited and fragmentary evidence available to him. It is, inother words, part of universal grammar. Similarly, it is difﬁcult to imagine what“inductive principles” might lead the child unerringly to the assumptions aboutdeep structure and about organization of grammar that seem to be necessary ifwe are to account for such facts as those we have mentioned. Nor is a search forsuch principles particularly well-motivated. It seems reasonable to assume thatthese properties of English are, in reality, facts of universal grammar. If suchproperties are available to the child, the task of language acquisition becomesfeasible. The problem for the child is not the apparently insuperable induc-tive feat of arriving at a transformational generative grammar from restricteddata, but rather that of discovering which of the possible languages he is beingexposed to. Arguing in this way, we can arrive at conclusions about universal
grammar from study of even a single language.
The child is presented with data, and he must inspect hypotheses (grammars)
of a fairly restricted class to determine compatibility with these data. Havingselected a grammar of the predetermined class, he will then have command ofthe language generated by this grammar.
41Thus he will know a great deal about
40Furthermore, evidence of a highly degraded sort. For example, the child’s conclusions about
the rules of sentence formation must be based on evidence that consists, to a large extent, ofutterances that break rules, since a good deal of normal speech consists of false starts, discon-nected phrases, and other deviations from idealized competence.
The issue here is not one of “normative grammar.” The point is that a person’s normal speech
departs from the rules of his own internalized grammar in innumerable ways, because of themany factors that interact with underlying competence to determine performance. Correspond-ingly, as a language learner, he acquires a grammar that characterizes much of the evidence onwhich it was based as deviant and anomalous.
41Weare presenting an “instantaneous model” of language acquisition which is surely false in
detail, but can very well be accepted as a reasonable ﬁrst approximation. This is not to denythat the ﬁne structure of learning deserves study. The question, rather, is what the range ofpossibilities may be within which experience can cause knowledge and belief to vary. If therange is quite narrow (as, it seems to me, is suggested by considerations of the sort mentionedabove), then a ﬁrst approximation of the sort suggested will be a prerequisite to any fruitful

--- Page 161 ---

142 Language and Mind
phenomena to which he has never been exposed, and which are not “similar” or
“analogous” in any well-deﬁned sense to those to which he has been exposed.42
He will, for example, know the relations among the sentences 33and34,despite
their novelty; he will know what stress contours to assign to utterances, despitethe novelty and lack of physical basis for these phonetic representations; and soon, for innumerable other similar cases. This disparity between knowledge andexperience is perhaps the most striking fact about human language. To account
for it is the central problem of linguistic theory.
The basic conclusion that seems to be emerging with increasing clarity from
contemporary work in linguistics is that very restrictive initial assumptionsabout the form of generative grammar must be imposed if explanations are tobe forthcoming for the facts of language use and language acquisition. Fur-thermore, there is, so far, no evidence to suggest that the variety of generativegrammars for human languages is very great. The theory of universal grammarsuggested by the sketchy description that we have just given will no doubt beproven incorrect in various respects. But it is not unlikely that its fundamen-tal defect will be that it permits far too much latitude for the construction ofgrammars, and that the kinds of languages that can be acquired by humans inthe normal way are actually of a much more limited sort than this theory wouldsuggest. Yet even as the theory of generative grammar stands today, it imposesfairly narrow conditions on the structure of human language. If this general
conclusion can be ﬁrmly established – and, furthermore, signiﬁcantly strength-ened – this will be a highly suggestive contribution to theoretical psychology.It is hardly open to controversy that today, as in the seventeenth century, thecentral and critical problem for linguistics is to use empirical evidence fromparticular languages to reﬁne the principles of universal grammar. I have tried,in this paper, to suggest some of the principles that seem well established andto illustrate some of the empirical considerations that bear on such principles.
43
investigation of learning. Given an instantaneous model that is empirically well supported, as a
ﬁrst approximation, there are many questions that can immediately be raised: for example, whatare the strategies by which hypotheses are sampled, how does the set of hypotheses availableat one stage depend on those tested at earlier stages, etc.
42Except, tautologically, in the sense that they are accounted for by the same theory.
43In addition to works mentioned in earlier notes the following books can be consulted for furtherdevelopment of topics touched on in this paper: N. Chomsky, Syntactic Structures (The Hague:
Mouton, 1957); N. Chomsky, Aspects of the Theory of Syntax (Cambridge, Mass.: MIT Press,
1965); M. Halle, Sound Pattern of Russian (The Hague: Mouton, 1959); J. Katz and P. Postal,
An Integrated Theory of Linguistic Descriptions (Cambridge, Mass.: MIT Press, 1964). See also
many papers in J. Fodor and J. Katz, eds., Structure of Language: Readings in the Philosophy
of Language (Englewood Cliffs, N.J.: Prentice-Hall, 1964). For more information on aspects
of English structure touched on here, see also R. Lees, Grammar of English Nominalizations
(New York: Humanities Press, 1963), and P. Rosenbaum, “Grammar of English Predicate Com-plement Constructions,” unpublished Ph.D. dissertation, MIT, 1965. For further material seethe bibliographies of the works cited.

--- Page 162 ---

6 Linguistics and philosophy
The methods and concerns of linguists and philosophers are similar in so many
respects that it would be folly, I believe, to insist on a sharp separation ofthese disciplines, or for either to maintain a parochial disregard for insightsachieved in the other. A number of examples might be cited to illustrate thepossibility of fruitful interchange between the two. Zeno Vendler, in his recentbook Linguistics and Philosophy ,goes so far as to maintain that “the science of
structural linguistics” provides “a new technique” for analytic philosophy, onethat “is nothing but the natural continuation of the line of development that goesthrough the philosophers of ordinary language to J. L. Austin.” For reasons towhich I will return in a moment, I am a bit skeptical about the contributionthat linguistics might make to philosophy along the lines that he sketches, butIthink that he has shown that certain concepts of linguistics can be used in
arewarding way in the investigation of problems that have arisen in analytic
philosophy.
Conversely, as the attention of linguists begins to turn to problems of meaning
and use, there is no question that they can learn much from the long traditionof philosophical investigation of such problems, although here too, I think, anote of skepticism is in order.
Tofacilitate the discussion of this and other topics, let me present a small
illustration of a problem that is at the frontier of research today. In the descriptivestudy of any language a central problem is to formulate a set of rules that generatewhat we may call the “surface structures” of utterances. By the term “surfacestructure,” I refer to the analysis of an utterance into a hierarchy of phrases,each belonging to a speciﬁc category. This hierarchy can be represented as alabeled bracketing of the utterance, in an obvious sense. For example, considerthe two sentences:
1 John is certain that Bill will leave.
2 John is certain to leave.
The surface structures of these utterances can be represented, in a natural way,
with the following labeled bracketing:
143

--- Page 163 ---

144 Language and Mind
1/prime/bracketleftbig
S/bracketleftbig
NPJohn/bracketrightbig/bracketleftbig
VPis/bracketleftbig
APcertain
/bracketleftbig
Sthat/bracketleftbig
NPBill/bracketrightbig
VPwill leave/bracketrightbig/bracketrightbig/bracketrightbig/bracketrightbig/bracketrightbig
2/prime/bracketleftbig
S/bracketleftbig
NPJohn/bracketrightbig/bracketleftbig
VPis/bracketleftbig
APcertain/bracketrightbig/bracketleftbig
VPto leave/bracketrightbig/bracketrightbig/bracketrightbig
Paired brackets bound phrases; the label assigned to a pair of brackets indi-
cates the category of the bounded phrase. Thus in 1,“certain that Bill will
leave” is a phrase of the category Adjective Phrase; in both 1and2,“John”
is a phrase of the category Noun Phrase; “will leave” is a Verb Phrase in 1;
and both 1and2are phrases of the category Sentence. One may question the
details of these particular analyses, but there is little doubt that at some level
of description, these, or representations very much like them, constitute a sig-niﬁcant aspect of the structure of the sentences 1and2,and, more generally,
that every sentence of the language has a surface structure of roughly this sort.There is, for example, strong evidence that the perceived phonetic form of theutterance is determined, by phonological rules of considerable generality, fromrepresentations of essentially this sort.
Granting this much, the linguist studying English will try to formulate a set
of rules that generate an inﬁnite number of surface structures, one for eachsentence of English. Correspondingly, linguistic theory will be concerned withthe problem of how such structures are generated in any human language, andwill try to formulate general principles governing the systems of rules thatexpress the facts of one or another such language.
Given the evidence available to us today, it seems to me reasonable to propose
that in every human language surface structures are generated from structuresof a more abstract sort, which I will refer to as “deep structures,” by certainformal operations of a very special kind generally called “grammatical transfor-mations.” Each transformation is a mapping of labeled bracketings onto labeledbracketings. Deep structures are themselves labeled bracketings. The inﬁniteclass of deep structures is speciﬁed by a set of “base rules.” Transformationsapplied in sequence to deep structures in accordance with certain ﬁxed conven-tions and principles ultimately generate the surface structures of the sentencesof the language. Thus a set of base rules deﬁning an inﬁnite class of deep struc-tures and a set of grammatical transformations can serve to generate the surfacestructures.
Toillustrate, consider sentences 1and2again. The underlying deep structures
might be represented roughly in the form 1
/prime/prime,2/prime/prime:
1/prime/primesame as 1/prime
2/prime/prime/bracketleftbig
S/bracketleftbig
NP/bracketleftbig
S/bracketleftbig
NPJohn/bracketrightbig/bracketleftbig
VPto leave/bracketrightbig/bracketrightbig/bracketrightbig/bracketleftbig
VPis/bracketleftbig
APcertain/bracketrightbig/bracketrightbig/bracketrightbig
Wemay think of these deep structures as expressing the fact that in 1,wepred-
icate of John that he is certain that Bill will leave, whereas in 2,which is rather

--- Page 164 ---

Linguistics and philosophy 145
similar to 1in surface structure, we predicate of the proposition that John leaves,
that it is certain, in a very different sense of “certain.” There is no difﬁculty
in deﬁning the concepts Subject and Predicate, in terms of conﬁgurations indeep structures, so that they express the intended interpretation. The operationsthat derive 2
/primefrom 2/prime/primeinclude an operation of “extraposition,” which from a
structure very much like 2/prime/primewould yield the structure 3,and an operation of
“it-replacement” which derives 2/primefrom a structure almost exactly like 3,b u t
with “to” in place of “will” and “that” deleted:
3/bracketleftbig
S/bracketleftbig
NPit/bracketrightbig/bracketleftbig
VPis/bracketleftbig
APcertain/bracketrightbig
/bracketleftbig
Sthat/bracketleftbig
NPJohn/bracketrightbig/bracketleftbig
VPwill leave/bracketrightbig/bracketrightbig/bracketrightbig/bracketrightbig
Details aside, the theory of “transformational-generative grammar” maintains
that all surface structures are formed by application of such transformations –each of which maps labeled bracketings onto labeled bracketings – from deepstructures that are often quite abstract. The sentences 1and2are similar in
surface structure, but very different in deep structure; the sentences 2and3are
very similar in deep structure, but quite different in surface structure. The deep
structures of the language are quite restricted in their variety, and it appears thatthere are universal conditions that sharply restrict the class of possible rules.
Consider now the matter of semantic interpretation. It is clear from these
quite typical examples that the surface structures give little indication of thesemantic interpretation, whereas the deep structures are quite revealing in thisrespect. Pursuing this line of reasoning, one might propose a further elaborationof the theory just outlined, in the following terms. Let us suppose that there isasystem of “universal semantics” that speciﬁes the class of possible semantic
representations for a natural language much in the way that universal phoneticsspeciﬁes the class of possible phonetic representations, by specifying a class ofdistinctive features and certain conditions on their combination. Observe thatit would be perfectly reasonable to study universal semantics even without anyclear idea as to what its constituent elements might be, just as one could drawfairly persuasive conclusions regarding universal phonetics from consideration
of the slow growth of the number of distinct sentences with increasing length,the phenomena of rhyme and assonance, the lack of slow drift through the“space” of sentences under chains of repetition, etc., even without any concep-tion of what the distinctive features of this system might be. In any event, stillsupposing this to be a reasonable approach, one might propose that a languagecontains rules associating deep structures with representations drawn from uni-versal semantics, as it contains phonological rules relating surface structures to
representations drawn from universal phonetics.
At this point in the development of such a theory, the linguist would do
well to turn to work in analytic philosophy, particularly to the many stud-ies of referential opacity. One essential empirical assumption in the preceding

--- Page 165 ---

146 Language and Mind
account is that surface structure cannot contribute to meaning; whatever con-
tribution the expression P makes to the meaning of the sentence XPY must bedetermined by the deep structure underlying P. The investigation of referentialopacity has turned up a great number of examples illustrating how replacementof one expression by another changes meaning, even when the semantic con-nection between the two is very close. The approach just outlined would haveto guarantee that in each such case there is a corresponding difference in deepstructure to which the difference in meaning can be attributed. Without pursu-ing the matter, I would simply note that the nature of these examples makesit appear very unlikely that such an approach can succeed; but, in any event,the study of this aspect of linguistic theory must certainly take into accountamass of evidence that has been accumulated in the course of philosophical
investigation.
Ihavementioned the possibility that insights developed in the course of
philosophical analysis might be relevant to the study of a central part of linguistictheory, and that concepts of linguistics might be useful to the philosopher inhis work. Nevertheless, it seems to me that one should not expect too muchfrom an interchange of this sort, for a number of reasons. In the cases I havementioned, what is proposed is that the incidental by-products of research inone ﬁeld will be of use for the central concerns of another. Furthermore, it isafact that neither ﬁeld makes use of research techniques of a sophisticated or
specialized nature. Thus one would expect that in each ﬁeld, it would be quitepossible to collect and analyze the information relevant to its speciﬁc concernsdirectly. It is, therefore, something of an accident when one ﬁeld can builddirectly on results of the other.
Forthese reasons, I think that Vendler may be expecting too much of the
method he suggests, namely, “an appeal to the facts of language already orga-nized by the science of structural linguistics.” I believe that modern linguisticshas real achievements to its credit, and that some of these do have relevance tophilosophical questions. But it must be kept in mind that these achievementsowelittle to modern science and less to modern technology. The gathering
of data is informal; there has been very little use of experimental approaches(outside of phonetics) or of complex techniques of data collection and dataanalysis of a sort that can easily be devised, and that are widely used in thebehavioral sciences. The arguments in favor of this informal procedure seemto me quite compelling; basically, they turn on the realization that for the theo-retical problems that seem most critical today, it is not at all difﬁcult to obtainamass of crucial data without use of such techniques. Consequently, linguistic
work, at what I believe to be its best, lacks many of the features of the behav-
ioral sciences. Nor is it obvious that the development of explanatory theoriesin linguistics merits the honoriﬁc designation “scientiﬁc.” I think that theseintellectual constructions are nontrivial and often illuminating. However, apart

--- Page 166 ---

Linguistics and philosophy 147
from certain insights owed to modern logic and mathematics, there is no reason
why they could not have been developed many years ago. In fact, were it not forthe dominance of certain empiricist assumptions to which I will return directly,Isuspect that they would have been developed long before now and that much
of what is new and exciting in linguistics today would be taken for granted byany educated person.
There are many questions about language that a philosopher might ask to
which linguistics provides no answer and no reasonable hope for an answer.Forexample, a philosopher concerned with problems of knowledge, or causality
(to take an example of Vendler’s), might well be interested in investigating indetail the properties of the words “know” and “cause.” Since linguistics offersno privileged access to data of this sort, it would be merely a lucky accidentif acquaintance with linguistics proved to be of substantial help in this inquiry.Alinguistic form is not of importance to linguistics because of the intrinsic
interest of the concept or proposition it expresses (if any), but because of theevidence it provides concerning some assumption about the nature of language.
Thus the analysis of sentences 1,2,and3has been of interest to linguistics
because of the light it sheds on the nature of deep and surface structures andthe grammatical transformations that link them. Such data are of importanceto linguistics insofar as they can be explained on the basis of some interestingassumptions about the organization of grammar, and are inconsistent with othersuch assumptions. In themselves, these facts are of no more interest than thefact that certain marks appear on a photographic plate at the base of a South
African mine shaft. The latter is critical for elementary particle theory forthe same reason that the facts related to sentences 1–3 are important for the
theory of language. Similar remarks can be made about the likelihood that theconclusions of philosophers or the data they accumulate will be important forlinguistics.
Tomake the matter more concrete, consider again the examples 1–3.Con-
ceivably, such sentences, and others like them, might be of some interest to aphilosopher concerned with the various concepts of certainty. These examplesare of interest to linguistics, at the moment, for entirely different reasons. Thusit is interesting that there is a nominalized expression corresponding to 1,b u t
no nominalized expression corresponding to 2;4is a nominalized form of 1,
butwecannot form 5,corresponding to 2:
4 John’s certainty that Bill would leave
5 John’s certainty to leave
The distinction is more general; thus consider 6and7:
6 John is eager to leave.
7 John is easy to leave.

--- Page 167 ---

148 Language and Mind
Corresponding to 6,wehavethe nominal phrase 8;but we cannot form 9
corresponding to 7:
8 John’s eagerness to leave
9 John’s easiness to leave
Notice that sentence 6is like 1in that the deep structure is very close to the
surface structure; whereas 7is like 2in that the deep structure is very different
from the surface structure. In fact, the surface of 7would be formed by opera-
tions much like those that form 2from 2/prime/primeand3,byaderivation of roughly the
form 10:
10 a./bracketleftbig
Sfor one to leave John/bracketrightbig
Sis easy (analogous to 2/prime/prime)
b.it is easy/bracketleftbig
Sfor one to leave John/bracketrightbig
S(analogous to 3)
c.John is easy to leave ( =7,analogous to 2).
The generalization exempliﬁed by 1,2,4–9is that a nominal phrase can be
formed corresponding to a base structure but not to a surface structure. Thus
we have 4corresponding to 1/prime/primeand8corresponding to 6(more properly, to the
deep structure underlying 6as1/primeunderlies 1,but no nominalized expression
such as 5and9,corresponding to the surface structures 2and7.This general
observation can be illustrated by many other examples. It is interesting becauseof the support it lends to the assumption that abstract deep structures of the sortillustrated play a role in the mental representation of sentences. We ﬁnd thatwhen we study English grammar on the basis of this and related assumptions,we are able to characterize quite readily the class of sentences to which therecorrespond nominal phrases of the sort under discussion. There is no naturalwaytocharacterize this class in terms of surface structure, since, as we have
seen, sentences that are very similar in surface structure behave quite differentlywith respect to the formal processes involved in the construction of nominalexpressions. We might go on to try to explain these facts at a deeper level by
formulating a principle of universal grammar from which it would follow thatthe nominal phrases in question will correspond only to deep structures.
Tosummarize, the examples in question are important for the study of lan-
guage because of the evidence that they provide in support of a particulartheory of linguistic structure, not because of the fact that the various conceptsof certainty are of interest in their own right. The philosopher concerned withcertainty would learn very little from a collection of data that is of great interestfor linguistic research.
Apart from accident or matters of personal history, linguistics will be of
relevance to philosophy only insofar as its conclusions about the nature of lan-guage bear on questions that concern the philosopher. One cannot predict towhat extent this will be true in the future; it might turn out, for example, that

--- Page 168 ---

Linguistics and philosophy 149
linguistic study of semantic and syntactic structure in the future will provide
aﬁrm basis for certain kinds of philosophical investigation – one thinks, for
example, of the potential relevance of a systematic classiﬁcation of verbs that
would have cross-language validity. For the moment, this is more a hope for
the future than a present reality, however. Still, I think that a case can be madethat certain well-founded conclusions about the nature of language do bear ontraditional philosophical questions, but in ways rather different from those justmentioned. Speciﬁcally, I think that these conclusions are relevant to the prob-lem of how knowledge is acquired and how the character of human knowledgeis determined by certain general properties of the mind. What I would like todo, for the remainder of this paper, is to restate certain proposals about thismatter that have been developed elsewhere,
1and then to consider a variety of
problems and objections that have been raised by several philosophers withrespect to these proposals.
2
One might adopt the following research strategy for the study of cognitive
processes in humans. A person is presented with a physical stimulus that heinterprets in a certain way. Let us say that he constructs a certain “percept”that represents certain of his conclusions (unconscious, in general) about thesource of stimulation. To the extent that we can characterize this percept, wecan proceed to investigate the process of interpretation. We can, in other words,proceed to develop a model of perception that takes stimuli as inputs and assignspercepts as “outputs,” a model that will meet certain given empirical conditionson the actual pairing of stimuli with interpretations of these stimuli. For example,the person who understands sentences 1and2knows (whether he is aware of
it or not) that in the case of 2it is a proposition that is certain and in the case
of1it is a person who is certain of something, in a very different sense of
“certain.” If we are interested in studying perception of language – speciﬁcally,the processes by which sentences are understood – we can begin by describingthe percepts in such a way as to bring out this difference, as we did in proposingthat1
/prime/primeand2/prime/prime,interpreted in the suggested manner, are essential components
of the percept. We can then ask how these percepts are constructed by the hearer,given the input stimuli 1and2.
Aperceptual model that relates stimulus and percept might incorporate a cer-
tain system of beliefs, certain strategies that are used in interpreting stimuli, andother factors – for example, organization of memory. In the case of language,
1See, for example, my contribution to the symposium on innate ideas published in Synthese ,
Vol. 17, No. 1, March 1967, pp. 2–11, and the references cited there on p. 11.
2Speciﬁcally, the contributions by Nelson Goodman and Hilary Putnam to the symposium in
Synthese ,Vol. 17, No. 1, March 1967, pp. 12–28, and the review articles by Henry Hi˙ zand
Gilbert Harman in the issue of the Journal of Philosophy devoted to “Some Recent Issues in
Linguistics,” V ol. 64, No. 2, February 2, 1967, pp. 67–87. The latter two are largely devoted tocritical analysis of Chapter 1 of my Aspects of the Theory of Syntax (Cambridge, Mass.: MIT
Press, 1965).

--- Page 169 ---

150 Language and Mind
the technical term for the underlying system of beliefs is “grammar,” or “gen-
erative grammar.” A grammar is a system of rules that generates an inﬁniteclass of “potential percepts,” each with its phonetic, semantic, and syntacticaspects, the class of structures that constitute the language in question. Thepercepts themselves are ﬁrst-order constructs; we determine their propertiesby experiment and observation. The grammar that underlies the formation ofpercepts is a second-order construct. To study it, we must abstract away fromthe other factors that are involved in the use and understanding of language,and concentrate on the knowledge of language
3that has been internalized in
some manner by the language user.
Concentrating on this system, we can then inquire into the means by which it
wasacquired and the basis for its acquisition. We can, in other words, attempt
to construct a second model, a learning model, which takes certain data as inputand gives, as “output,” the system of beliefs that is one part of the internalstructure of the perceptual model. The “output,” in this case, is represented inthe “ﬁnal state” of the organism that has acquired this system of beliefs; we areasking, then, how this ﬁnal state was achieved, through the interplay of innatefactors, maturational processes, and organism–environment interaction.
In short, we can begin by asking “what is perceived” and move from there
to a study of perception. Focusing on the role of belief (in our case, knowledgeof language) in perception, we can try to characterize “what is learned” andmove from there to the study of learning. One might, of course, decide to studysome other topic, or to proceed in some different manner. Thus much of modernpsychology has decided, for reasons that do not impress me, to limit itself tothe study of behavior and control of behavior. I do not want to pursue the matterhere, but I will merely state my own opinion: that this approach has provenquite barren, and that it is irrational to limit one’s objectives in this way. Onecannot hope to study learning or perception in any useful way by adhering tomethodological strictures that limit the conceptual apparatus so narrowly as todisallow the concept “what is perceived” and the concept “what is learned.”
Ithink that interesting conclusions can be reached when one studies human
language along the lines just outlined. In the areas of syntax and phonetics atleast, a plausible general account can be given of the system of representationfor percepts in any human language. Furthermore, there has been substantialprogress in constructing generative grammars that express the knowledge oflanguage that is the “output” of a learning model and a fundamental componentof a perceptual model. There is, I believe, good evidence that a generativegrammar for a human language contains a system of base rules of a highlyrestricted sort, a set of grammatical transformations that map the deep structures
3Since the language has no objective existence apart from its mental representation, we need not
distinguish between “system of beliefs” and “knowledge,” in this case.

--- Page 170 ---

Linguistics and philosophy 151
formed in accordance with base rules onto surface structures, and a set of
phonological rules that assign phonetic interpretations, in a universal phoneticalphabet, to surface structures. Furthermore, there is also good evidence thatcertain highly restrictive principles determine the functioning of these rules,conditions of ordering and organization of a complex and intricate sort. Thereis a considerable literature dealing with these matters, and I will not try to reviewit here. I only wish to emphasize that there is no a priori necessity for a languageto be organized in the highly speciﬁc manner proposed in these investigations.Hence if this theory of linguistic structure is correct, or near correct, somenontrivial problems arise for the theory of human learning. Speciﬁcally, wemust ask how, on the basis of the limited data available to him, the child is ableto construct a grammar of the sort that we are led to ascribe to him, with itsparticular choice and arrangement of rules and with the restrictive principles ofapplication of such rules. What, in other words, must be the internal structureof a learning model that can duplicate this achievement? Evidently, we must tryto characterize innate structure in such a way as to meet two kinds of empiricalconditions. First, we must attribute to the organism, as an innate property, astructure rich enough to account for the fact that the postulated grammar isacquired on the basis of the given conditions of access to data; second, we mustnot attribute to the organism a structure so rich as to be incompatible with theknown diversity of languages. We cannot attribute knowledge of English to thechild as an innate property, because we know that he can learn Japanese as wellas English. We cannot attribute to him merely the ability to form associations,or to apply the analytic procedures of structural linguistics, because (as is easyto show when these proposals are made precise) the structures they yield arenot those that we must postulate as generative grammars. Within the empiricalbounds just stated, we are free to construct theories of innate structure and totest them in terms of their empirical consequences. To say this is merely todeﬁne the problem. Substantive questions arise only when a speciﬁc theory isproposed.
By investigating sentences and their structural descriptions, speech signals
and the percepts to which they give rise, we can arrive at detailed conclusionsregarding the generative grammar that is one fundamental element in linguisticperformance, in speech and understanding of speech. Turning then to the nexthigher level of abstraction, we raise the question of how this generative grammaris acquired. From a formal point of view, the grammar that is internalized byevery normal human can be described as a theory of his language, a theory of
ahighly intricate and abstract form that determines, ultimately, a connection
between sound and meaning by generating structural descriptions of sentences(“potential percepts”), each with its phonetic, semantic, and syntactic aspects.From this point of view, one can describe the child’s acquisition of knowledgeof language as a kind of theory construction. Presented with highly restricted

--- Page 171 ---

152 Language and Mind
data, he constructs a theory of the language of which this data is a sample (and,
in fact, a highly degenerate sample, in the sense that much of it must be excludedas irrelevant and incorrect – thus the child learns rules of grammar that identifymuch of what he has heard as ill-formed, inaccurate, and inappropriate). Thechild’s ultimate knowledge of language obviously extends far beyond the datapresented to him. In other words, the theory he has in some way developed hasapredictive scope of which the data on which it is based constitute a negligible
part. The normal use of language characteristically involves new sentences,sentences that bear no point-by-point resemblance or analogy to those in thechild’s experience. Furthermore, the task of constructing this system is carriedout in a remarkably similar way by all normal language learners, despite widedifferences in experience and ability. The theory of human learning must facethese facts.
Ithink that these facts suggest a theory of human intelligence that has a
distinctly rationalist ﬂavor. Using terms suggested by Peirce, in his lectureson “the logic of abduction,” the problem of the theory of learning is to statethe condition that “gives a rule to abduction and so puts a limit on admissi-ble hypotheses.” If “man’s mind has a natural adaptation to imagining correcttheories of some kinds,” then acquisition of knowledge of a sort that we areconsidering is possible. The problem for the psychologist (or linguist) is toformulate the principles that set a limit to admissible hypotheses. I have madedetailed suggestions in this regard elsewhere, and will not repeat them here.Roughly, I think it reasonable to postulate that the principles of general linguis-tics regarding the nature of rules, their organization, the principles by whichthey function, the kinds of representations to which they apply and which theyform, all constitute part of the innate condition that “puts a limit on admissiblehypotheses.” If this suggestion is correct, then there is no more point askinghow these principles are learned than there is in asking how a child learns tobreathe, or, for that matter, to have two arms. Rather, the theory of learningshould try to characterize the particular strategies that a child uses to determinethat the language he is facing is one, rather than another, of the “admissible lan-guages.” When the principles just alluded to are made precise, they constitute anempirical assumption about the innate basis for the acquisition of knowledge,an assumption that can be tested in a variety of ways. In particular, we can askwhether it falls between the bounds described earlier: that is, does it ascribe arich enough innate structure to account for the acquisition of knowledge, but astructure not so rich as to be falsiﬁed by the diversity of languages? We mightalso ask many other questions, for example, how the schema that is proposedas a basis for acquisition of knowledge of language relates to the principles that“give a rule to abduction” in other domains of human (or animal) intelligence.
What I am suggesting is that if we wish to determine the relevance of linguis-
tics to philosophy, we must investigate the conclusions that can be established

--- Page 172 ---

Linguistics and philosophy 153
concerning the nature of language, the ways in which language is used and
understood, the basis for its acquisition. I think that these conclusions haveinteresting consequences for psychological theory – in particular, that theystrongly support an account of mental processes that is, in part, familiar, fromrationalist speculation about these matters. They support the conclusion thatthe role of intrinsic organization is very great in perception, and that a highlyrestrictive initial schema determines what counts as “linguistic experience” andwhat knowledge arises on the basis of this experience. I also think, and haveargued elsewhere, that the empiricist doctrines that have been prevalent in lin-guistics, philosophy, and psychology in recent years, if formulated in a fairlyprecise way, can be refuted by careful study of language. If philosophy is whatphilosophers do, then these conclusions are relevant to philosophy, both in itsclassical and modern varieties.
At this point, I would like to turn to some of the critical analysis of this point
of view that has appeared in the recent philosophical literature, speciﬁcally, tothe items referred to in note 2.
Goodman’s treatment of these questions seems to me to suffer, ﬁrst, from a
historical misunderstanding; second, from a failure to formulate correctly theexact nature of the problem of acquisition of knowledge and third, from a lack
of familiarity with the work that has led to the conclusions that he criticizes,those that are outlined above.
His historical misunderstanding has to do with the issue between Locke and
whoever it was that Locke thought he was criticizing in his discussion of innateideas. Goodman believes that “Locke mad e... acutely clear” that the doctrine
of innate ideas is “false or meaningless.” I will not dwell on this matter, since itis a commonplace of historical scholarship that Locke’s critique of the doctrineof innate ideas “assails it in its crudest form, in which it is countenanced byno eminent advocate.”
4Even Lord Herbert makes it clear that the common
notions “remain latent” in the absence of appropriate stimulation, that theyare the “principles without which we should have no experience at all” butthat they will obviously not be constantly in consciousness, even to “normalmen,” and certainly not to those who are “headstrong, foolish, weak-mindedand imprudent,” to “madmen, drunkards, and infants,” and so on. And as theseideas are elaborated by Descartes and others, it is repeatedly emphasized thatwhile innate ideas and principles determine the nature of experience and theknowledge that can arise from it, they will ordinarily not be in consciousness.Since Locke’s arguments fail to come to grips with the “dispositional” natureof innate structure that is insistently maintained by the leading proponents ofrationalist doctrine, they also invariably miss the mark; it seems that he must
4A. C. Fraser (ed.), in his edition of Locke’s Essay Concerning Human Understanding ,1894
(reprinted by Dover, 1959), p. 38 of the Dover edition.

--- Page 173 ---

154 Language and Mind
have mistaken the actual views of Herbert, Descartes, the minor Cartesians,
Cudworth, and others.
It is surprising that Goodman accuses those who “identify the innate ideas
with capacities” of “sophistry.” Goodman is free, if he wishes, to use the terms“idea” and “innate idea” in accordance with Locke’s misunderstanding of ratio-nalist doctrine, but hardly to accuse others of “sophistry” when they examineand develop this doctrine in the form in which it was actually presented. It isparticularly surprising to hear Goodman speak of the necessity of applying theterm “idea,” in “its normal use.” One would hardly expect Goodman to proposethis sort of “ordinary language argument” against the use of a technical term.Furthermore, as Thomas Reid pointed out, if we use “idea” in the nontechnicalway,then not only the position of Descartes, but also that of Locke and Hume
reduces to absurdity – an observation that is correct, but that shows nothingmore than the absurdity of insisting that a technical term must be understood in“the normal use” of the homonymous nontechnical term of ordinary discourse.
Let me turn, however, to the substantive problem of acquisition of knowledge,
as Goodman formulates it in the speciﬁc case of language acquisition. Quiteproperly, he distinguishes two cases: initial language, and second-languageacquisition. But his analysis of the two cases leaves much to be desired.
Consider ﬁrst the problem of second-language acquisition. In what I under-
stand to be Goodman’s view,
5second-language acquisition poses no problem,
since “once one language is available and can be used for giving explanationand instruction, the limitations [determined by an innate schematism] are tran-scended.” This way of putting the matter misconstrues the situation in two basicrespects. First, it is misleading to speak of the innate schematism that has beenproposed as merely providing “limitations” for acquisition of language. Rather,what has been proposed is that this schematism makes possible the acquisition
of a rich and highly speciﬁc system on the basis of limited data. To take oneexample, the problem is to explain how the data available to a language learner
(ﬁrst or second) sufﬁces to establish that the phonological rules (the rules thatassign phonetic representations to surface structures) apply cyclically, ﬁrst toinnermost phrases of the surface structure, then to larger phrases, etc., until themaximal domain of phonological processes – in simple cases, the full sentence –is reached. There is in fact good evidence that the rules do apply cyclically, butthis evidence is not of a sort that can be used as the basis for induction from pho-netic data to the principle of cyclic application, by any procedure of inductionthat has general validity. In particular, much of this evidence is derived from ananalysis of percepts, that is, from investigation of the way in which someone
5Cf. his article in the symposium in Synthese ,Vol. 17, No. 1, March 1967, p. 24. Given the
dialogue form of his article, it is difﬁcult to be certain that one is not misrepresenting his position.
However, I see no other way to interpret these remarks.

--- Page 174 ---

Linguistics and philosophy 155
who has already mastered the language interprets speech signals. It seems that
this interpretation imposes a certain structure that is not indicated directly inthe speech signal, for example, in the determination of stress contours.
6Obvi-
ously the child cannot acquire the knowledge that phonological rules applycyclically from data that are available to him only after he knows and makes
use of this principle. This is an extreme example, but it nevertheless illustratesquite well the basic problem: to explain how a rich and highly speciﬁc grammaris developed on the basis of limited data that is consistent with a vast numberof other conﬂicting grammars. An innate schematism is proposed, correctly orincorrectly, as an empirical hypothesis to explain the uniformity, speciﬁcity, andrichness of detail and structure of the grammars that are, in fact, constructedand used by the person who has mastered the language. Therefore the word“limitation” in Goodman’s formulation is quite inappropriate.
More serious, it must be recognized that one does not learn the grammatical
structure of a second language through “explanation and instruction,” beyondthe most elementary rudiments, for the simple reason that no one has enoughexplicit knowledge about this structure to provide explanation and instruction.
Forexample, consider the property of nominalization in English noted earlier,
namely, that a certain class of nominal expressions corresponds only to deepand not surface structures. The person who has learned English as a secondlanguage well enough to make the judgments illustrated by examples 1–10has
not acquired this knowledge through “explanation and instruction.” Until quiterecently, no one, to my knowledge, was aware of this phenomenon; the second-language learner, like the ﬁrst-language learner, has somehow established thefacts for himself, without explanation or instruction. Again, the example is quite
typical. Only a trivial part of the knowledge that the second-language learneracquires is presented to him by direct instruction. Even the most cursory atten-tion to the facts of second-language acquisition is sufﬁcient to establish this.Hence, although second-language acquisition is, indeed, to be distinguishedfrom ﬁrst-language acquisition, the distinction is not of the sort that Goodmansuggests. While it may be true that “once some language is available, acquisi-tion of others is relatively easy,” it nevertheless remains a very serious prob-lem – not signiﬁcantly different from the problem of explaining ﬁrst-languageacquisition – to account for this fact.
Consider now the more important matter of ﬁrst-language acquisition, the
problem to which the empirical hypotheses regarding innate schematism have
6Forsome discussion, see my paper “Explanatory Models in Linguistics,” in E. Nagel, P. Suppes,
and A. Tarski, eds., Logic, Methodology, and Philosophy of Science (Stanford, Calif.: Stanford
University Press, 1962). For some recent and much more extensive discussion, see N. Chomsky
and M. Halle, Sound Patterns of English (New York: Harper & Row, 1968), and the references
cited there, and my paper “Some General Properties of Phonological Rules,” Language ,Vol. 43,
March 1967, pp. 102–28.

--- Page 175 ---

156 Language and Mind
been directed. Goodman argues that there is no problem in explaining ﬁrst-
language acquisition, because “acquisition of an initial language is acquisitionof a secondary symbolic system”: the fundamental step has already been taken,and details can be elaborated within an already existing framework. This argu-ment might have some force if it were possible to show that some of the speciﬁcproperties of grammar – say the distinction of deep and surface structure, thespeciﬁc properties of grammatical transformations and phonological rules, theprinciples of rule ordering, and so on – were present in these already acquiredprelinguistic “symbolic systems.” But there is not the slightest reason to believethat this is so. Goodman’s argument is based on a metaphorical use of the term“symbolic system,” and collapses as soon as we try to give this term a pre-cise meaning. If it were possible to show that “prelinguistic symbolic systems”share certain signiﬁcant properties with natural language, we could then arguethat these properties of natural language are somehow acquired by “analogy,”though we would now face the problem of explaining how the “prelinguis-tic symbolic systems” developed these properties and how the analogies areestablished. But the issue is academic, since, for the moment, there is no rea-son to suppose the assumption to be true. Goodman’s argument is a bit like a“demonstration” that there is no problem in accounting for the development ofcomplex organs, because everyone knows that mitosis takes place. This seemsto me to be obscurantism, which can be maintained only so long as one fails tocome to grips with the actual facts.
There is, furthermore, a non sequitur in Goodman’s discussion of ﬁrst- and
second-language acquisition. Recall that he explains the presumed ease ofsecond-language acquisition on the grounds that it is possible to use the ﬁrst lan-guage for explanation and instruction. He then goes on to argue that “acquisitionof an initial language is acquisition of a secondary symbolic system,” and ishence quite on a par with second-language acquisition. The primary symbolicsystems he has in mind are “rudimentary prelinguistic symbolic systems inwhich gestures and sensory and perceptual occurrences of all sorts function assigns.” But evidently these systems, whatever they may be, cannot “be used forgiving explanation and instruction” in the way in which a ﬁrst language can be
used in second-language acquisition. Consequently, even on his own grounds,
Goodman’s argument is incoherent.
Goodman maintains that “the claim we are discussing cannot be experimen-
tally tested even when we have an acknowledged example of a ‘bad’ language,and . . . that the claim has not even been formulated to the extent of citation ofasingle general property of ‘bad’ languages.” The ﬁrst of these conclusions is
correct, in his sense of “experimental test,” namely, a test in which we “take aninfant at birth, isolate it from all the inﬂuences of our language-bound culture,and attempt to inculcate it with one of the ‘bad’ artiﬁcial languages.” Obviously,this is not feasible, exactly as comparable experimental tests are not feasible in

--- Page 176 ---

Linguistics and philosophy 157
any other area of human psychology. But there is no reason for dismay at the
impracticality of such direct tests as these. There are many other ways – thosediscussed earlier, and extensively in the literature – in which evidence can beobtained regarding the properties of grammars and in which hypotheses regard-ing the general properties of such grammars can be put to empirical test. Anysuch hypothesis immediately speciﬁes, correctly or incorrectly, certain prop-erties of “bad” languages. It therefore makes an empirical claim that can befalsiﬁed by ﬁnding counterinstances in some human language, or by showing
that under the actual conditions of language acquisition, the properties in ques-tion do not appear in the system that is developed by the language learner. Inlinguistics, as in any other ﬁeld, it is only in such indirect ways as these that onecan hope to ﬁnd evidence bearing on nontrivial hypotheses. Direct experimentaltests of the sort that Goodman, for some reason, regards as necessary, are rarelyfeasible, a fact that may be unfortunate but that is nevertheless characteristic ofmost research.
Goodman’s further claim, that not “a single general property of ‘bad’ lan-
guages has been formulated,” is quite unfair. There are dozens of books andpapers concerned with formulating properties of universal grammar and exam-ining their empirical consequences, and each such property speciﬁes “bad”languages, as just noted. One is free to argue that these attempts are misguided,inadequate, unconvincing, refuted by facts, etc., but not to deny blandly thatthey exist. I do not see how to avoid the conclusion that when Goodman speaksof “the unimpressive evidence adduced with respect to languages,” he simplyspeaks out of ignorance, rather than from a considered analysis of the work thathas been done in the ﬁeld.
In discussing properties of “bad” languages, Goodman refers only to one case,
namely, the case of the concocted language Gruebleen ,which “differs from
ordinary English only in that it contains the predicates ‘grue’ (for “examinedbefore tand green or not so examined and blue”) and ‘bleen’ (for “examined
before tand blue or not so examined and green”) in place of the predicates
‘green’ and ‘blue.’” He argues that even in this case, one must be “painfullyaware of the difﬁculties of answering” the question of what in general is “the
difference between Gruebleen-like and English-like languages.” I think thatthis is a rather marginal issue, since much more deep-seated properties of“English-like languages” have been formulated and investigated, but, sincehe brings up this example, it is well to point out that the difﬁculties to which healludes are in large measure a consequence of the vagueness of the question heasks. Thus there is no difﬁculty in ﬁnding some property of Gruebleen that isnot a property of “English-like languages,” even a property of some general-ity. For example, consider the predicate “match” understood as in Goodman’sStructure of Appearance ,but applying now to objects rather than qualia. Thus
two objects match “if and only if they are not noticeably different on direct

--- Page 177 ---

158 Language and Mind
comparison.”7Gruebleen has the curious property that if an object A is exam-
ined before tand an object B is examined after t,and both are found to be
grue (or both bleen), then we know that they will not match. But there is no
tsuch that given two objects, one examined before tand one after t,and both
found to be green (or blue), we can predict that they will not match. Theymay not match, but then they also may match, if both are green (or blue). Infact, it is undoubtedly a general property of natural languages that they are
“English-like” rather than “Gruebleen-like,” in this sense, in the domain ofcolor terms. Thus there is no difﬁculty in establishing a fairly general distinc-tion between Gruebleen-like and English-like languages, in this speciﬁc respect.Of course, this would not satisfy Goodman’s requirements, for his special pur-poses, because one can construct other problems of the gruebleen type that arenot taken account of by this property. As long as Goodman’s vague notions“English-like” and “Gruebleen-like” are left unspeciﬁed, there is of course nowaytomeet his demand that a general property be stated distinguishing the
two kinds of language, and any speciﬁc distinction that is proposed will alwaysgive rise to new riddles of induction. This is an interesting comment about thelimitations of inductive methods, but has no more relevance to the problem ofspecifying the characteristics of universal grammar than to any other enterpriseof science, say, the problem of specifying the genetic conditions that determinethat a human embyro will develop legs rather than wings, under a given range ofconditions.
Iamnot, incidentally, proposing that the property just cited serves to explain
why every language-learner (in fact, every mouse, chimpanzee, etc.) uses greenrather than grue as the basis for generalization. No doubt this is a simple con-sequence of certain properties of the sensory system, a conclusion that is quiteuninteresting from Goodman’s point of view, but not, for that reason, incorrect.
Returning to the main point, it is interesting that at one stage of his argument
Goodman remarks, quite correctly, that even if “for certain remarkable facts Ihave no alternative explanation,” “that alone does not dictate acceptance of . . . anintrinsically repugnant and incomprehensible theory.” But now let us considerthe theory of innate ideas that arouses Goodman’s indignation, and ask whetherit is “incomprehensible” and “repugnant.”
Consider ﬁrst the matter of comprehensibility. It does not seem to me incom-
prehensible that some aspect of the “ﬁnal state”of an organism or automatonshould also be an aspect of its “initial state,” prior to any interaction with theenvironment, just as it is not incomprehensible that this aspect of the ﬁnalstate should have developed through internal processes, perhaps set in motion
7N. Goodman, Structure of Appearance ,2nd edn. (Indianapolis: Bobbs-Merrill, 1966), p. 272.
The distinction between Gruebleen and English that I am now discussing is not to be confused
with a pseudodistinction, correctly rejected by J. Ullian on the basis of a different usage of thenotion “match.” See Philosophical Review ,July 1961.

--- Page 178 ---

Linguistics and philosophy 159
by organism–environment interaction of some sort. But consider the actual
doctrines developed in the speculative psychology of rationalism, rather thanLocke’s caricature. Descartes, for example, argued that the idea of a triangleis innate in that “the idea of a true triangle . . . can be more easily conceivedby our mind than the more complex ﬁgure of the triangle drawn on paper,” sothat when a child ﬁrst sees the more complex ﬁgure, he will “apprehend notit itself, but rather the authentic triangle.” As Cudworth elaborates this view,“every irregular and imperfect triangle [is] as perfectly that which it is, as themost perfect triangle,” but we interpret sensory images in terms of a notion of“regular ﬁgure” that has its source in the “rule, pattern and exemplar” generatedby the mind as an “anticipation,” just as we interpret all sensory data in termsof certain concepts of object and relations among objects, certain notions ofcause and effect, gestalt properties, functions in a “space” of possible humanactions, and so on. Neither this view, nor its elaboration in modern psychol-ogy, is incomprehensible, though it may of course be misguided or incorrect.Similarly, there is no difﬁculty in comprehending the proposal that there arecertain innate conditions on the form of grammar that determine what con-stitutes linguistic experience and what knowledge will arise on the basis ofthis experience. Again, one can easily design an automaton that will func-tion in this manner, so that although the proposal may be wrong, it is notincomprehensible.
Whatever Goodman’s attitudes might be to these formulations, it is interesting
that he appears quite willing, at least in this paper, to accept the view that in somesense the mature mind contains ideas; it is obviously not incomprehensible, then,that some of these ideas are “implanted in the mind as original equipment,” touse his terminology. His argument is directed not against the notion that “ideasare in the mind,” but rather against the assumption that they are “in the mind”prior to experience, and surely if one assumption is comprehensible, then theother is as well (though neither, as noted, does justice to the classical rationalistview or to its modern variants). On the other hand, this approach to the problemof acquisition of knowledge will, no doubt, be “repugnant” to one who considersempiricist doctrine immune to doubt or challenge. But this is to treat empiricistdoctrines as articles of religious faith. Surely it is not reasonable to be so boundto a tradition as to refuse to examine conﬂicting views about acquisition ofknowledge on their merits.
Let me turn next to Hilary Putnam’s contribution to the same symposium.
Although his paper deals more directly with the points that are actually at issue,still it seems to me that his arguments are inconclusive, primarily becauseof certain erroneous assumptions about the nature of the acquired grammars.Speciﬁcally, he enormously underestimates, and in part misdescribes, the rich-ness of structure, the particular and detailed properties of grammatical form andorganization that must be accounted for by a “language acquisition model,” that

--- Page 179 ---

160 Language and Mind
are acquired by the normal speaker–hearer and that appear to be uniform among
speakers and also across languages.
Tobegin with, Putnam assumes that at the level of sound structure, the
only property that can be proposed in universal grammar is that a languagehas “a short list of phonemes.” This uniformity among languages, he argues,requires no elaborate explanatory hypothesis. It can be explained simply interms of “such parameters as memory span and memory capacity,” and no“rank Behaviorists” would have denied that these are innate properties. In fact,however, very strong empirical hypotheses have been proposed regarding thechoice of universal distinctive features, the form of phonological rules, theordering and organization of these rules, the relation of syntactic structureto phonetic representation, none of which can conceivably be accounted foron grounds of memory limitations. Putnam bases his account largely on my“Explanatory Models in Linguistics” (see note 6), which examines in some
detail the principle of cyclic application of phonological rules, a principle that,if correct, raises some rather serious problems. We must ask how the childacquires knowledge of this principle, a feat that is particularly remarkable since,as already noted, much of the evidence that leads the linguist to posit thisprinciple is drawn from the study of percepts and is thus not even available tothe child. Similar questions arise with respect to many other aspects of universalphonology. In any event, if the proposals that have been elaborated regardingsound structure are correct or near correct, then the similarities among languagesat this level, and the richness of the knowledge acquired by the child, are indeedremarkable facts, and demand an explanation.
Above the level of sound structure, Putnam assumes that the only signiﬁ-
cant properties of language are that they have proper names, that the grammarcontains a phrase-structure component, and that there are rules “abbreviating”sentences generated by the phrase-structure component. He argues that the spe-ciﬁc character of the phrase-structure component is determined by the existenceof proper names; that the existence of a phrase-structure component is explainedby the fact that “all the natural measures of complexity of an algorithm . . . leadto the . . . result” that phrase-structure systems provide the “algorithms whichare ‘simplest’ for virtually any computing system,” hence also “for naturallyevolved ‘computing systems’”; that there is nothing surprising in the fact that
languages contain rules of abbreviations. Hence, he concludes, the only innateconditions that must be postulated are those that apply to all reasonable “com-puting systems,” and no Behaviorist should feel any surprise at this.
Each of the three conclusions, however, is vitiated by a false assumption.
First, it is obvious that there are many different phrase-structure grammarsconsistent with the assumption that one of the categories is that of proper names.In fact, there is much dispute at the moment about the general properties of theunderlying base system for natural languages; the dispute is not in the least

--- Page 180 ---

Linguistics and philosophy 161
resolved by the existence of proper names as a primitive category in many
languages.8
As to the second point, it is simply untrue that all measures of complexity and
speed of computation lead to phrase-structure rules as the “simplest possiblealgorithm.” The only existing results that have even an indirect relevance tothis matter are those dealing with context-free phrase-structure grammars andtheir automata-theoretic interpretation. Context-free grammars are a reasonablemodel for the rules generating deep structures, when we exclude the lexicalitems and the distributional conditions they meet. But even apart from thisfundamental discrepancy, the only existing results relate context-free grammarsto a class of automata called “nondeterministic pushdown storage automata,”and these have no particularly striking properties insofar as speed or complexityof computation are concerned, and are certainly not “natural” from this pointof view. In terms of time and space conditions on computation, the somewhatsimilar but not formally related concept of real-time deterministic automatonwould seem to be far more natural. In short, there are no results demonstrating
that phrase-structure grammars are optimal in any computational sense (nor,certainly, are there any results dealing with the much more complex notion ofbase structure with a context-free phrase-structure grammar and a lexicon, withmuch richer properties, as components).
But there is no point in pursuing this matter, since what is at stake, in any
event, is not the “simplicity” of phrase-structure grammars but rather of trans-
formational grammars that contain a phrase-structure component, the latterplaying a role in the generation of deep structures. And there is absolutely nomathematical concept of “ease of computation” or “simplicity of algorithm”that even suggests that such systems have some advantage over the variouskinds of automata that have been investigated from this point of view. In fact,these systems have never really been considered in a strictly mathematical con-text, though there are interesting initial attempts to study some of their formalproperties.
9The source of the confusion is a misconception on Putnam’s part
as to the nature of grammatical transformations. These are not, as he supposes,rules that “abbreviate” sentences generated by phrase-structure rules. Rather,they are operations that form surface structures from underlying deep struc-tures, which are generated, in part, by phrase-structure rules. Although therehas been considerable evolution of theory since the notions of transformational
8Not, incidentally, in all. Although this is hardly important here, it seems that many languages
do not have proper names as a primitive category, but rather form proper names by recursiveprocesses of an elaborate sort. See, for example, G. H. Matthews, Hidatsa Syntax (The Hague:
Mouton, 1965), pp. 191 f.
9See, for example, S. Peters and R. Ritchie, “On the Generative Capacity of TransformationalGrammars,” Information Sciences (to be published); and J. P. Kimball, “Predicates Deﬁnable
overTransformational Derivations by Intersection with Regular Languages,” Information and
Control ,Vol. 2, 1967, pp. 177–95.

--- Page 181 ---

162 Language and Mind
generative grammar were ﬁrst proposed, one assumption that has remained con-
stant is that the phrase-structure rules generate only abstract structures, whichare then mapped into surface structures by grammatical transformations – thelatter being structure-dependent operations of a peculiar sort that have neverbeen studied outside of linguistics, in particular, nor in any branch of mathe-matics with which I am familiar. To show that transformational grammars arethe “simplest possible” one would have to demonstrate that an optimal com-puting system would take a string of symbols as input and determine its surfacestructure, the underlying deep structure, and the sequence of transformationaloperations that relate these two labeled bracketings. Nothing known about easeor simplicity of computation gives any reason to suppose that this is true; infact, the question has never been raised. One can think of certain kinds of orga-
nization of memory that might be well adapted to transformational grammars,butthis is a different matter entirely.
10Iwould, naturally, assume that there is
some more general basis in human mental structure for the fact (if it is a fact)that languages have transformational grammars; one of the primary scientiﬁcreasons for studying language is that this study may provide some insight intogeneral properties of mind. Given those speciﬁc properties, we may then be ableto show that transformational grammars are “natural.” This would constitutereal progress, since it would now enable us to raise the problem of innate condi-tions on acquisition of knowledge and belief in a more general framework. Butit must be emphasized that, contrary to what Putnam asserts, there is no basisfor assuming that “reasonable computing systems” will naturally be organizedin the speciﬁc manner suggested by transformational grammar.
Ibelieve that this disposes of Putnam’s main argument, namely, that there is
“nothing surprising,” even to a Behaviorist, in the linguistic universals that arenow being proposed and investigated. Let me then turn to his second argument,that even if there were surprising linguistic universals, they could be accountedfor by a simpler hypothesis than that of an innate universal grammar, namely,the hypothesis of common origin of languages. This proposal misrepresentsthe problem at issue. As noted earlier, the empirical problem we face is todevise a hypothesis about initial structure rich enough to account for the factthat a speciﬁc grammar is acquired, under given conditions of access to data. Tothis problem, the matter of common origin of language is quite irrelevant. Thegrammar has to be discovered by the child on the basis of the data available tohim, through the use of the innate capacities with which he is endowed. To beconcrete, consider again the two examples discussed above: the association ofnominal phrases to base structures and the cyclic application of phonological
10Forsome speculations on this matter, see G. A. Miller and N. Chomsky, “Finitary Models
of Language Users,” Part II, in R. D. Luce, R. Bush, and E. Galanter, eds., Handbook of
Mathematical Psychology (New York: Wiley, 1963), V ol. II.

--- Page 182 ---

Linguistics and philosophy 163
rules. The child masters these principles (if we are correct in our conclusions
about grammar) on the basis of certain linguistic data; he knows nothing aboutthe origin of language and could not make use of such information if he hadit. Questions of common origin are relevant to the empirical problems we arediscussing only in that the existing languages might not be a “fair sample” ofthe “possible languages,” in which case we might be led mistakenly to proposetoo narrow a schema for universal grammar. This possibility must be kept inmind, of course, but it seems to me a rather remote consideration, given theproblem that is actually at hand, namely, the problem of ﬁnding a schema richenough to account for the development of the grammars that seem empiricallyjustiﬁed. The discovery of such a schema may provide an explanation for theempirically determined universal properties of language. The existence of theseproperties, however, does not explain how a speciﬁc grammar is acquired bythe child.
Putnam’s discussion of the ease of language-learning seems to me beside the
point. The question whether there is a critical period for language-learning isinteresting,
11butithas little relevance to the problem under discussion. Suppose
that Putnam were correct in believing that “certainl y...6 0 0 hours [of direct
method instruction] will enable any adult to speak and read a foreign languagewith ease.” We would then face the problem of explaining how, on the basisof this restricted data, the learner has succeeded in acquiring the speciﬁc anddetailed knowledge that enables him to use the language with ease, and toproduce and understand a range of structures of which the data presented tohim constitute a minute sample.
Finally, consider the alternative approach that Putnam suggests to the prob-
lem of language acquisition. He argues that instead of postulating an innateschematism one should attempt to account for this achievement in terms of“general multipurpose learning strategies.” It is these that must be innate, notgeneral conditions on the form of the knowledge that is acquired. Evidently,this is an empirical issue. It would be sheer dogmatism to assert of either ofthese proposals (or of some particular combination of them) that it must be
correct. Putnam is convinced, on what grounds he does not say, that the innatebasis for the acquisition of language must be identical with that for acquiringany other form of knowledge, that there is nothing “special” about the acqui-sition of language. A nondogmatic approach to this problem can be pursued,through the investigation of speciﬁc areas of human competence, such as lan-guage, followed by the attempt to devise a hypothesis that will account forthe development of such competence. If we discover that the same “learningstrategies” are involved in a variety of cases, and that these sufﬁce to account
11See E. H. Lenneberg, Biological Foundations of Language (New York: Wiley, 1967), for evi-
dence bearing on this issue.

--- Page 183 ---

164 Language and Mind
for the acquired competence, then we will have good reason to believe that
Putnam’s empirical hypothesis is correct. If, on the other hand, we discoverthat different innate systems (whether involving schemata or heuristics) haveto be postulated, then we will have good reason to believe that an adequate the-ory of mind will incorporate separate “faculties,” each with unique or partiallyunique properties. I cannot see how one can resolutely insist on one or the otherconclusion in the light of the evidence now available to us. But one thing isquite clear: Putnam has no justiﬁcation for his ﬁnal conclusion, that “invoking‘Innateness’ only postpones the problem of learning; it does not solve it.”
12
Invoking an innate representation of universal grammar does solve the problemof learning (at least partially), in this case, if in fact it is true that this is the basis(or part of the basis) for language acquisition, as it well may be. If, on the otherhand, there exist general learning strategies that account for the acquisition ofgrammatical knowledge, then postulation of an innate representation of univer-sal grammar will not “postpone” the problem of learning, but will rather offeran incorrect solution to this problem. The issue is an empirical one of truth orfalsity, not a methodological one of stages of investigation. At the moment, the
only concrete proposal that is at all plausible, in my opinion, is the one sketchedabove. When some “general learning strategy” is suggested, we can look intothe relative adequacy of these alternatives, on empirical grounds.
Henry Hi˙ z’s review article deals mainly with the distinction between com-
petence and performance. One can attempt to explain technical concepts suchas these in two different ways. At a presystematic level, one can try to indicate,
12Or for his assumption that the “weighting functions” proposed in universal grammar constitute
the “sort of fact ...[that] ...learning theory tries to account for; notthe explanation being
sought.” No one would say that the genetic basis for the development of arms rather than wingsin a human embryo is “the kind of fact that learning theory tries to account for,” rather than thebasis for explanation of other facts about human behavior. The question whether the weightingfunction is learned, or whether it is the basis for learning, is an empirical one. There is not theslightest reason to assume, a priori, that it is to be accounted for by learning rather than geneticendowment, or some combination of the two.
There are other minor points in Putnam’s discussion that call for some comment. For example,
he asserts that since certain ambiguities “require coaching to detect,” it follows that “the claimthat grammar ‘explains the ability to recognize ambiguities’ . . . lacks the impressiveness thatChomsky believes it to have.” But he misconstrues the claim, which relates to competence, notperformance. What the grammar explains is why “the shooting of the hunters” (the examplehe cites) can be understood with hunters as subject or object but that in “the growth of corn”we can understand “corn” only as subject (the explanation, in this case, turns on the relation ofnominalizations to deep structures, noted earlier). The matter of coaching is beside the point.What is at issue is the inherent sound–meaning correlation that is involved in performance,butonly as one of many factors. Putnam also misstates the argument for assuming the active–
passive relation to be transformational. It is not merely that the speaker knows them to be related.Obviously that would be absurd; the speaker also knows that “John will leave tomorrow” and“John will leave three days after the day before yesterday” are related, but this does not implythat there is a transformational relation between the two. Syntactic arguments are given in manyplaces in the literature. See, for example, my Syntactic Structures (The Hague: Mouton, 1957);
Aspects of the Theory of Syntax.

--- Page 184 ---

Linguistics and philosophy 165
necessarily in a loose and somewhat vague and only suggestive way, just what
role the concept is intended to play in a more general framework, and why itseems to be a useful idea to try to develop. Discussion at this level is entirelylegitimate, but there will generally be much room for misunderstanding. At asecond level, one can develop the concept in as precise a way as the state of theﬁeld permits, with no consideration for motivation or general implications. Atthis level, the problem is to determine not what the concept in question is, butwhy there is any point in developing it.
At the presystematic level, I have tried to explain what I mean by “linguis-
tic competence” in terms of models of use and acquisition of language, in themanner outlined earlier. At the systematic level, competence is expressed byagenerative grammar that recursively enumerates structural descriptions of
sentences, each with its phonetic, syntactic, and semantic aspects. It is hardlynecessary to emphasize that any such grammar that we can actually presenttoday is incomplete, not only because our knowledge of particular languagesis deﬁcient, but also because our understanding of phonetic and semantic rep-resentation and the kinds of structures and rules that mediate between them islimited and unsatisfactory in many respects.
Turning to Hi˙ z’s paper, there is, not surprisingly, a certain degree of misun-
derstanding between us at the presystematic level. Hi˙ zsuggests that my use of
the notion “competence” “is to be understood as saying that introspection is asource of linguistic knowledge.” I do agree that introspection is an excellentsource of data for the study of language, but this conclusion does not followfrom the decision to study linguistic competence. One might (irrationally, inmy opinion) refuse to use such evidence, and still try to discover the genera-tive grammar that represents “what is learned” and that plays a fundamentalrole in language use. This decision would be pointless, rather on a par with anastronomer’s refusal, at one stage of the science, to use what he sees throughatelescope as data, but the decision has nothing to do with the distinction
between competence and performance. I have no doubt that it would be pos-sible to devise operational and experimental procedures that could replace thereliance on introspection with little loss, but it seems to me that in the presentstate of the ﬁeld, this would simply be a waste of time and energy. Obviously,any such procedure would ﬁrst have to be tested against the introspective evi-dence. If one were to propose a test for, say, grammaticalness, that fails to makethe distinctions noted earlier in the proper way, one would have little faith in theprocedure as a test for grammaticalness. To me it seems that current researchis not hampered signiﬁcantly by lack of accurate data, but rather by our inabil-ity to explain in a satisfactory way data that are hardly in question. One whofeels differently can support his point of view by demonstrating the gains ininsight and understanding that can be achieved by reﬁnements in techniquesof data collection and analysis, say, by operational techniques for establishing

--- Page 185 ---

166 Language and Mind
grammaticalness, techniques that have been judged by the prior test of intuition
and shown to be sufﬁciently sound so that one can rely on them in difﬁcultor obscure cases. In any event, the whole matter has nothing to do with thedecision to study linguistic competence.
Hi˙zregards it as “paradoxical” to assert, as I have, that linguistics “attempts
to specify what the speaker actually knows, not what he may report abouthis knowledge.” This he regards as “a peculiar sense of ‘knowledge.’” To meit seems a rather ordinary sense, and a nonparadoxical usage. A person whoknows English may give all sorts of incorrect reports about the knowledgethat he actually possesses and makes use of constantly, without awareness. Asnoted earlier, when we study competence – the speaker–hearer’s knowledge ofhis language – we may make use of his reports and his behavior as evidence, butwe must be careful not to confuse “evidence” with the abstract constructs thatwe develop on the basis of evidence and try to justify in terms of evidence. ThusIwould deﬁnitely reject three of the ﬁve conditions that, Hi˙ zsuggests, rules
must satisfy if they are to constitute an account of competence in my sense,namely, that the native speaker feels that the sentences generated by the rules arein his language, that they have the assigned structures, and that what the speakerfeels is true. Since performance – in particular, judgments about sentences –obviously involves many factors apart from competence, one cannot accept asan absolute principle that the speaker’s judgments will give an accurate accountof his knowledge. I am surprised that Hi˙ zshould offer this interpretation of my
views immediately after having quoted my statement that the speaker’s reportsabout his competence may be in error.
At least for the purposes of discussion, Hi˙ ziswilling to accept the view that
agenerative grammar, a system of rules assigning structures to sentences, can
serve to characterize competence. He then points out, correctly, that the linguistis guided in his choice of a grammar by certain “general principles about lan-guage as such,” and that this general theory – universal grammar – will haveexplanatory value if it selects particular grammars correctly. He then attributes
to me, incorrectly, the view that universal grammar is to be identiﬁed with “atheory of language acquisition.” My view, rather, is that universal grammar isone element of such a theory, much as competence is one element of a theory ofperformance. There are surely many other factors involved in language acqui-sition beyond the schematism and weighting function that – if my suggestion iscorrect – play a part in determining the nature of the acquired competence. Thismisinterpretation of my proposal regarding the relation of universal grammar tolanguage acquisition parallels the misinterpretation of my proposal regardingthe relation of competence to performance; in both cases, what is omitted is thereference to other factors that must be involved. In the case of language acqui-sition, furthermore, it must be emphasized that the model I am suggesting canat best only be regarded as a ﬁrst approximation to a theory of learning, sinceit is an instantaneous model and does not try to capture the interplay between

--- Page 186 ---

Linguistics and philosophy 167
tentative hypotheses that the child may construct, new data interpreted in terms
of these hypotheses, new hypotheses based on these interpretations, and so on,until some relatively ﬁxed system of competence is established. I think that aninstantaneous model is a reasonable ﬁrst approximation, but this, as any otheraspect of research strategy, must ultimately be evaluated in terms of its successin providing explanations and insight.
Hi˙zregards the reference to classical formulations of problems of language
and mind as “confusing and misleading historical baggage.” I disagree with thisjudgment, but have nothing to add here beyond what I have written elsewhere.
13
My feeling is that the contributions of rationalist psychology and linguistics areinteresting in themselves, and are quite relevant to present concerns, more so, infact, than much of the work of the past century. One who ﬁnds these forays into
intellectual history “confusing and misleading” can perfectly well disregardthem. I see no issue here.
Before leaving this matter, I should mention that Hi˙ zisinaccurate in stating
that Herbert of Cherbury restricted himself to “religious knowledge.” Nor canThomas Reid be described as one of those concerned to develop a doctrine ofinnate universals. Furthermore, it is surely misleading to say that I “call upon”Descartes and others “to support” my “stand on innate universals.” Their advo-cacy of a similar position does not constitute “support.” Rather, I am suggestingthat their contributions have been inadequately appreciated, and that we can stilllearn a good deal from a careful study of them.
Hi˙zobjects to the fact that my proposals concerning universal grammar are
based on detailed examination of a few languages rather than “examinationof many cases.” I certainly agree that one should study as many languages aspossible. Still, a caveat should be entered. It would be quite easy to present
enormous masses of data from varied languages that are compatible with allconceptions of universal grammar that have so far been formulated. Thereis no point in doing so. If one is concerned with the principles of universalgrammar, he will try to discover those properties of particular grammars thatbear on these principles, putting aside large amounts of material that, so faras he can determine, do not. It is only through intensive studies of particularlanguages that one can hope to ﬁnd crucial evidence for the study of universalgrammar. One study such as that of Matthews on Hidatsa (see note 8)isworth
one thousand superﬁcial studies of varied languages from this point of view. Ifsomeone feels that the base of data is too narrow, what he should do is show thatsome of the material omitted refutes the principles that have been formulated.Otherwise, his criticism has no more force than a criticism of modern geneticsfor basing its theoretical formulations on the detailed investigation of only a feworganisms.
13In my Current Issues in Linguistic Theory (The Hague: Mouton, 1964), Section 1; Aspects of
the Theory of Syntax ,Chapter 1, Section 8; Cartesian Linguistics (New York: Harper & Row,
1966).

--- Page 187 ---

168 Language and Mind
Hi˙zalso argues that the principles of universal grammar, even if true, may
indicate only “the common historical origin of languages.” I have already
pointed out why this hypothesis is without explanatory force.
Hi˙zmaintains that decisions about particular parts of grammar (by the lin-
guist) are “determined not by a general theory but by internal usefulness withinthe particular grammar,” and objects that I do not make this clear. Since I haveno idea what is meant by “internal usefulness,” I have nothing to say aboutthis point. The issue is confused by his misinterpretation of my use of thenotion “simplicity.” When I speak of “simplicity of grammar,” I am referringto a “weighting function,” empirically determined, that selects a grammar ofthe form permitted by the universal schematism over others that are also of theproper form and are compatible with the empirical data. I am not using theterm “simplicity” to refer to that poorly understood property of theories thatleads the scientist to select one rather than another. The evaluation measurethat deﬁnes “simplicity of grammars” is part of linguistic theory. We must tryto discover this measure on empirical grounds, by considering the actual rela-tions between input data and acquired grammars. Thus the notion “simplicityof grammar” plays a role analogous to that of a physical constant; we mustestablish it on empirical grounds, and there is no a priori insight on which wecan rely. The problems of deﬁning “simplicity of theories” in a general contextof epistemology and philosophy of science are entirely irrelevant to the issueof determining, on empirical grounds, the properties of grammars that lead tothe selection of one rather than another in language acquisition. This aspect hasbeen emphasized repeatedly. See, for example, Aspects ,Chapter 1, Section 7.
One ﬁnal comment. Hi˙ zsuggests that “it should be easier to explain why
we assign such-and-such a structure to a sentence by pointing out how thissentence changes the readings of neighboring sentences than by referring toinnate universal ideas and mental reality.” Here he is confusing two entirelydifferent kinds of explanations. If I want to explain why, yesterday afternoon atthree o’clock, John Smith understood “the shooting of the hunters” as referringto the act of shooting the hunters, rather than the hunters’ act of shooting, I willof course bring into consideration the situational context (not limiting myselfto the “readings of neighboring sentences”). If I am interested in explainingwhy this phrase is susceptible to these two interpretations, but the phrase “thegrowth of corn” is susceptible to only one (namely, the corn’s growing and notthe act or process of growing corn), then I will appeal ﬁrst to the particulargrammar of English, and more deeply, to the linguistic universals that led to theconstruction of this grammar by a child exposed to certain data. Since entirelydifferent things are being explained, it is senseless to claim that one manner ofexplanation is “easier” than the other.
Harman’s critique is also concerned with the matter of competence and per-
formance. He begins by ascribing to me a view that I have never held, and have

--- Page 188 ---

Linguistics and philosophy 169
explicitly rejected on numerous occasions, namely, that “competence [is] the
knowledge that the language is described by the rules of the grammar,” and that
agrammar describes this “competence.” Obviously, it is absurd to suppose that
the speaker of the language knows the rules in the sense of being able to statethem. Having attributed to me this absurd view, Harman goes on to strugglewith all sorts of purported confusions and difﬁculties of interpretation. But hecites nothing that could possibly be regarded as a basis for attributing to me thisview, though he does quote remarks in which I explicitly reject it. Therefore, Iwill not discuss this part of his argument at all.
In Harman’s framework, there are two kinds of knowledge: knowing that and
knowing how. Obviously knowledge of a language is not a matter of “know-ing that.” Therefore, for him, it must be a matter of “knowing how.” A typicalspeaker “knows how to understand other speakers”; his competence is his abil-ity “to speak and understand the language described by [the] grammar” thatdescribes the language. I do not know what Harman means by the locution“knows how to understand,” but clearly he is using the term “competence” inadifferent way from what I proposed in the work he is reviewing. In my sense
of “competence,” the ability to speak and understand the language involves notonly “competence” (that is, mastery of the generative grammar of the language,tacit knowledge of the language), but also many other factors. In my usage,the grammar is a formal representation of what I have called “competence.” Ihave no objection to Harman’s using the term in a different way, but when heinsists on supposing that his usage is mine, naturally, only confusion will result.Again, I see no point in tracing in detail the various difﬁculties into which thismisinterpretation leads him.
According to Harman, the “competence to speak and understand the lan-
guage” is a skill, analogous to the skill of a bicycle rider. Given his insistencethat knowledge of language is a matter of “knowing how” (since it is obviouslynot “knowing that”), this is not an unexpected conclusion. But he suggests norespect in which ability to use a language (let alone the competence, in mysense, that constitutes an element of this ability) is like the ability to ride abicycle, nor do I see any. The proper conclusion, then, would be that there isno reason to suppose that knowledge of language can be characterized in termsof “knowing how.” I therefore see no point in the analogy that he suggests.Knowledge of language is not a skill, a set of habits, or anything of the sort.Isee nothing surprising in the conclusion that knowledge of language cannot
be discussed in any useful or informative way in this impoverished framework.In general, it does not seem to me true that the concepts “knowing how” and“knowing that” constitute exhaustive categories for the analysis of knowledge.Nor is it surprising that Harman ﬁnds it difﬁcult to understand my remarks, orthose of anyone else who is concerned with knowledge of language, given thathe insists on restricting himself to this framework.

--- Page 189 ---

170 Language and Mind
Harman tries to show that there is a fundamental incoherence in my pro-
posal that in acquiring or using knowledge of a language (in developing “an
internal representation of a generative system” or making use of it in speakingor understanding speech), the child makes use of an innate schematism thatrestricts the choice of grammars (in the case of acquisition) or an internalizedgrammar (in the case of language use). His argument seems to me unclear. AsIunderstand it, it seems to proceed as follows. He argues that this internalized
system must be presented in “another more basic language,” which the childmust come to understand before he can make use of this schematism to learnthis language, or before he can make use of the grammar to understand speech.But this, he argues, leads to a vicious circle or an inﬁnite regress. Thus if wewere to say that the child knows the “more basic language” directly, withoutlearning, then why not say also that he knows “directly the language he speaks,”without learning; a vicious circle. Or, if we say that he must learn the more basiclanguage, then this raises the question how the more basic language is learned,and leads to an inﬁnite regress. This argument is totally invalid. Consider thecase of acquisition of language. Even if we assume that the innate schematismmust be represented in an “innate language,” neither conclusion follows. Thechild must know this “innate language,” in Harman’s terms, but it does notfollow that he must “speak and understand it” (whatever this might mean) orthat he must learn it. All that we need assume is that he can make use of thisschematism when he approaches the task of language learning. So much for theinﬁnite regress. As to the vicious circle, there is a very simple reason why wecannot assume that the child knows the language he speaks directly, withoutlearning, namely, that the assumption is false. We cannot claim that every childis born with a perfect knowledge of English. On the other hand, there is noreason why we should not suppose that the child is born with a perfect knowl-edge of universal grammar, that is, with a ﬁxed schematism that he uses, in theways described earlier, in acquiring language. This assumption may be false,
butitisquite intelligible. If one insists on describing this knowledge as “direct
knowledge of a more basic language,” I see no reason to object, so long as weare clear about what we mean, but would merely point out that there is no reasonat all to doubt that the child has this direct knowledge. Hence there is no viciouscircle, and no inﬁnite regress. Similarly, if we consider the case of languageuse, there is neither incoherence nor implausibility. There is surely no inﬁniteregress and no vicious circle in the assumption that in language use (speakingor understanding) the user employs an internally represented grammar. We caneasily construct a model (say, a computer program) that functions in this way. Itherefore fail to see any basis for Harman’s belief that there is an inﬁnite regressor vicious circle inherent in, or even suggested by this formulation.
In the second part of his paper, Harman turns to my argument that current
work in linguistics supports a view of language and mind that has a distinctly

--- Page 190 ---

Linguistics and philosophy 171
rationalist ﬂavor, and is in conﬂict with the empiricist views that have dominated
the study of language and mind in recent years. He asserts that to infer a grammarfrom data, a model of language learning must already have detailed informationabout the theory of performance. This is an interesting proposal, and it deservesto be developed. But I cannot go along with his rather dogmatic claim, hardlyargued in the paper, that this approach must necessarily be correct, and that anyother approach must fail to provide any insight into the problem of acquisitionof knowledge. I think that the work of the past few years on universal grammardoes, in fact, suggest and in part support an interesting, rather classical approachto the problem of how knowledge is acquired. In the absence of any argumentas to why this approach must fail to be illuminating, I see no reason not tocontinue with the investigation of how principles of universal grammar mightselect a particular grammar on the basis of the data available.
Let us turn now to the issue of rationalist and empiricist approaches to prob-
lems of language and mind. As Harman points out, if we describe an innateschematism biased toward (or restricted to) a speciﬁc form of grammar as partof the “principles of induction used,” and deﬁne “resourceful empiricism” asadoctrine that makes use of such “principles of induction” as this, then surely
“resourceful empiricism” cannot be refuted, “no matter what the facts aboutlanguage [or anything else] turned out to be.” Of course, this new doctrine of“resourceful empiricism” would now incorporate “principles of induction” thatare, so it seems, quite speciﬁc to the task of language acquisition and of nogeneral validity.
The concept “resourceful empiricism” so deﬁned seems to me of little inter-
est. The issue that concerns me is whether there are “ideas and principles ofvarious kinds that determine the form of the acquired knowledge in what may
be a rather restricted and highly organized way,” or alternatively, whether “thestructure of the acquisition device is limited to certain elementary peripheralprocessing mechanism s...a n d certain analytical data-processing mechanisms
or inductive principles” ( Aspects ,pp. 47 f). I have argued that “it is historically
accurate as well as heuristically valuable to distinguish these two very differentapproaches to the problem of acquisition of knowledge,” even though they ofcourse “cannot always be sharply distinguished” in the work of a particularperson ( ibid.,p.52). In particular, I have tried to show that it is possible to
formulate these approaches so that the former incorporates the leading ideasof classical rationalism as well as the modern variant I have been describing,and that the latter includes classical empiricist doctrine as well as the theoriesof acquisition of knowledge (or belief, or habit) developed in a wide range ofmodern work (Quine’s notions of quality space and formation of knowledge byassociation and conditioning; Hull’s approach in terms of primitive uncondi-tioned reﬂexes, conditioning, and habit structures; taxonomic linguistics, withits analytic procedures of segmentation and classiﬁcation and its conception

--- Page 191 ---

172 Language and Mind
of language as a “habit system,” and so on).14Needless to say, there is no
necessity to view the various attempts to study language acquisition within this
framework; I can only say that I think it is both useful and accurate. These alter-natives can be made fairly precise and investigated in terms of their empiricalconsequences. Harman’s proposal to deﬁne “resourceful empiricism” in suchaway as to include both approaches, and to be, as he notes, immune to any
factual discovery, is merely a pointless terminological suggestion and cannot
obscure the difference between the approaches mentioned or the importance ofpursuing and evaluating them.
15
Tosummarize, I doubt that linguistics can provide “a new technique” for
analytic philosophy that will be of much signiﬁcance, at least in its presentstate of development. Nevertheless, it seems to me that the study of languagecan clarify and in part substantiate certain conclusions about human knowledgethat relate directly to classical issues in the philosophy of mind. It is in thisdomain, I suspect, that one can look forward to a really fruitful collaborationbetween linguistics and philosophy in coming years.
14Harman observes correctly that I ignore the “enormous philosophical literature on induction,”
and limit myself solely to an investigation of the procedures of taxonomic linguistics as “theonly proposals that are explicit enough to support serious study.” He does not, however, showhow anything in the literature on induction bears on the problems I am considering. The reasonis that there is nothing. The literature on induction is quite interesting, but it happens to dealwith entirely different questions. It does not even hint at procedures of analysis or acquisition ofbelief or conﬁrmation that would overcome the problems that I have been discussing. There is,for example, nothing in the literature on induction that gives any insight into how the principlescited above as examples (the cycle of phonological rules or the rule of nominalization) mightbe reached “by induction” from the data available. But it is such questions as these that mustbe faced in the study of language acquisition.
15Twominor points in this connection. Harman sees only a “tenuous historical connection”
between procedures of segmentation and classiﬁcation and phrase-structure grammar. The con-nection is actually much closer. Zellig Harris, in his Methods in Structural Linguistics (Chicago:
University of Chicago Press, 1951), tried to show how a systematic use of such procedures,ampliﬁed by a simple inductive step, would lead to a set of rules that might be regardedas generating an inﬁnite set of sentences. A set of Harris’ “morpheme to utterance” formulas,though not quite the same as phrase-structure grammar, is quite similar. The concept of “phrase-structure grammar” was explicitly designed to express the richest system that could reasonablybe expected to result from the application of Harris-type procedures to a corpus. Harris, andother methodologists of the 1940s, were developing an approach to linguistic analysis that onecan trace at least to Saussure.
Secondly, Harman is quite correct in pointing out that in my reference to “the only [empiricist]
proposals that are explicit enough to support serious study,” I omitted mention of Harris’ andHi˙z’s method of studying co-occurrence relationships. He feels that this method is “similar in
spirit to the taxonomic procedures.” I don’t see the point in arguing this, one way or another. Inany event, I know of no reason to suppose that such procedures can lead to or can even provideevidence for or against the postulation of a generative grammar.

--- Page 192 ---

7 Biolinguistics and the human capacity
Iwould like to say a few words about what has come to be called “the biolin-
guistic perspective,” which began to take shape half a century ago in discussions
among a few graduate students who were much inﬂuenced by developments inbiology and mathematics in the early postwar years, including work in ethol-ogy that was just coming to be known in the United States. One of themwasEric Lenneberg, whose seminal 1967 study Biological Foundations of
Language remains a basic document of the ﬁeld. By then considerable inter-
change was proceeding, including interdisciplinary seminars and internationalconferences. The most far-reaching one, in 1974, was called, for the ﬁrst time,“Biolinguistics.” Many of the leading questions discussed there remain verymuch alive today.
One of these questions, repeatedly brought up as “one of the basic questions
to be asked from the biological point of view,” is the extent to which apparentprinciples of language, including some that had only recently come to light,are unique to this cognitive system. An even more basic question from thebiological point of view is how much of language can be given a principledexplanation, whether or not homologous elements can be found in other domains
or organisms. The effort to sharpen these questions and to investigate them forlanguage has come to be called “the minimalist program” in recent years, butthe questions arise for any biological system, and are independent of theoreticalpersuasion, in linguistics and other domains. Answers to these questions arenot only fundamental to understanding the nature and functioning of organismsand their subsystems, but also to investigation of their growth and evolution.
The biolinguistic perspective views a person’s language in all its aspects –
sound, meaning, structure – as a state of some component of the mind, under-standing “mind” in the sense of eighteenth-century scientists who recognizedthat after Newton’s demolition of the “mechanical philosophy,” based on theintuitive concept of a material world, no coherent mind–body problem remains,and we can only regard aspects of the world “termed mental,” as the resultof “such an organical structure as that of the brain,” as chemist–philosopherJoseph Priestley observed. Thought is a “little agitation of the brain,” DavidHume remarked; and, as Darwin commented a century later, there is no
173

--- Page 193 ---

174 Language and Mind
reason why “thought, being a secretion of the brain,” should be considered
“more wonderful than gravity, a property of matter.” By then, the more tem-pered view of the goals of science that Newton introduced had become scientiﬁccommon sense: Newton’s reluctant conclusion that we must be satisﬁed withthe fact that universal gravity exists, even if we cannot explain it in terms of theself-evident “mechanical philosophy.” As many commentators have observed,this intellectual move “set forth a new view of science” in which the goal is“not to seek ultimate explanations” but to ﬁnd the best theoretical account wecan of the phenomena of experience and experiment (I. Bernard Cohen).
The central issues in the domain of study of mind still arise, in much the same
form. They were raised prominently at the end of the “Decade of the Brain,”which brought the last millennium to a close. The American Academy of Artsand Sciences published a volume to mark the occasion, summarizing the currentstate of the art. The guiding theme was formulated by neuroscientist VernonMountcastle in his introduction to the volume: it is the thesis that “Things men-tal, indeed minds, are emergent properties of brains, [though] these emergencesare not regarded as irreducible but are produced by principles . . . we do not yetunderstand.” The same thesis, which closely paraphrases Priestley, has beenput forth in recent years as an “astonishing hypothesis” of the new biology, a“radically new idea” in the philosophy of mind, “the bold assertion that mentalphenomena are entirely natural and caused by the neurophysiological activitiesof the brain,” and so on. But this is a misunderstanding. The thesis follows fromthe collapse of any coherent concept of “body” or “material” in the seventeethcentury, as was soon recognized. Terminology aside, the fundamental thesisremains what has been called “Locke’s suggestion”: that God might have cho-sen to “superadd to matter a faculty of thinking” just as he “annexed effects tomotion, which we can in no way conceive motion able to produce.”
Mountcastle’s reference to reductive principles that we “do not yet under-
stand” also begs some interesting questions, as a look at the history of scienceillustrates, even quite recent science. It is reminiscent of Bertrand Russell’sobservation in 1929, also reﬂecting standard beliefs, that “chemical laws cannotat present be reduced to physical laws.” The phrase “at present,” like Mount-castle’s word “yet,” expresses the expectation that the reduction should takeplace in the normal course of scientiﬁc progress, perhaps soon. In the case ofphysics and chemistry, it never did: what happened was uniﬁcation of a virtuallyunchanged chemistry with a radically revised physics. It’s hardly necessary toadd that the state of understanding and achievement in those areas eighty yearsago was far beyond anything that can be claimed for the brain and cognitivesciences today. Hence conﬁdence in “reduction” to the little that is understoodis not necessarily appropriate.
From the array of phenomena that one might loosely consider language-
related, the biolinguistic approach focuses attention on a component of human

--- Page 194 ---

Biolinguistics and the human capacity 175
biology that enters into the use and acquisition of language, however one inter-
prets the term “language.” Call it the “faculty of language,” adapting a tradi-tional term to a new usage. This component is more or less on a par with thesystem of mammalian vision, insect navigation, or others. In many of thesecases, the best available explanatory theories attribute to the organism com-putational systems and what is called “rule-following” in informal usage – forexample, when a recent text on vision presents the so-called “rigidity principle”
as it was formulated ﬁfty years ago: “if possible, and other rules permit, inter-pret image motions as projections of rigid motions in three dimensions.” Inthis case, later work provided substantial insight into the mental computationsthat seem to be involved when the visual system follows these rules, but evenfor very simple organisms, that is typically no slight task, and relating mentalcomputations to analysis at the cellular level is commonly a distant goal. Somephilosophers have objected to the notion “rule-following” – for language, rarelyvision. But I think that is another misunderstanding, one of many in my opin-ion. It is of some interest to compare qualms expressed today about theoriesof language, and aspects of the world “termed mental” more generally, withdebates among leading scientists well into the 1920s as to whether chemistrywasamere calculating device predicting the results of experiments, or whether
it merits the honoriﬁc status of an account of “physical reality,” debates laterunderstood to be completely pointless. The similarities, which I have discussedelsewhere, are striking and I think instructive.
Putting these interesting topics aside, if we adopt the biolinguistic perspec-
tive, a language is a state of the faculty of language – an I-language in technicalusage, where “I” underscores the fact that the conception is internalist, individ-ual, and intensional (with an “s,” not a “t”) – that is, the actual formulation ofthe generative principles, not the set it enumerates; the latter we can think of asamore abstract property of the I-language, rather as we can think of the set of
possible trajectories of a comet through the solar system as an abstract propertyof that system.
The decision to study language as part of the world in this sense was regarded
as highly controversial at the time, and still is, by many linguists as well. It seemsto me that the arguments advanced against the legitimacy of the approach havelittle forc e–a weak thesis; and that its basic assumptions are tacitly adopted
even by those who strenuously reject the m–a much stronger thesis. I will not
enter into this chapter of contemporary intellectual history here, but will simplyassume that crucial aspects of language can be studied as part of the naturalworld in the sense of the biolinguistic approach that took shape half a century
ago, and has been intensively pursued since, along various different paths.
The language faculty is one component of what the co-founder of mod-
ern evolutionary theory, Alfred Russel Wallace, called “man’s intellectual andmoral nature” : the human capacities for creative imagination, language and

--- Page 195 ---

176 Language and Mind
other modes of symbolism, mathematics, interpretation and recording of natu-
ral phenomena, intricate social practices and the like, a complex of capacitiesthat seem to have crystallized fairly recently, perhaps a little over 50,000 yearsago, among a small breeding group of which we are all descendants – a com-plex that sets humans apart rather sharply from other animals, including otherhominids, judging by the archaeological record. The nature of the “humancapacity,” as some researchers now call it, remains a considerable mystery. Itwasone element of a famous disagreement between the two founders of the
theory of evolution, with Wallace holding, contrary to Darwin, that evolution ofthese faculties cannot be accounted for in terms of variation and natural selec-tion alone, but requires “some other inﬂuence, law, or agency,” some principleof nature alongside gravitation, cohesion, and other forces without which thematerial universe could not exist. Although the issues are differently framedtoday, they have not disappeared.
It is commonly assumed that whatever the human intellectual capacity is, the
faculty of language is essential to it. Many scientists agree with paleoanthropol-
ogist Ian Tattersall, who writes that he is “almost sure that it was the inventionof language” that was the “sudden and emergent” event that was the “releasingstimulus” for the appearance of the human capacity in the evolutionary record –the “great leap forward” as Jared Diamond called it, the result of some geneticevent that rewired the brain, allowing for the origin of human language with
the rich syntax that provides a multitude of modes of expression of thought, aprerequisite for social development and the sharp changes of behavior that arerevealed in the archaeological record, also generally assumed to be the triggerfor the rapid trek from Africa, where otherwise modern humans had apparentlybeen present for hundreds of thousands of years. The view is similar to thatof the Cartesians, but stronger: they regarded normal use of language as theclearest empirical evidence that another creature has a mind like ours, but notthe criterial evidence for mind and the origin of the human capacity.
If this general picture has some validity, then the evolution of language may
be a very brief affair, even though it is a very recent product of evolution.Of course, there are innumerable precursors, and they doubtless had a longevolutionary history. For example, the bones of the middle ear are a marvellous
sound-amplifying system, wonderfully designed for interpreting speech, butthey appear to have migrated from the reptilian jaw as a mechanical effect ofgrowth of the neocortex in mammals that began 160 million years ago, so it isreported. We know far too little about conceptual systems to say much, but it’sreasonable to suppose that they too had a long history after the separation ofhominids, yielding results with no close similarity elsewhere. But the question ofevolution of language itself has to do with how these various precursors were
organized into the faculty of language, perhaps through some slight geneticevent that brought a crucial innovation. If that is so, then the evolution of

--- Page 196 ---

Biolinguistics and the human capacity 177
language itself is brief, speculations that have some bearing on the kind of
inquiry into language that is likely to be productive.
Tattersall takes language to be “virtually synonymous with symbolic
thought.” Elaborating, one of the initiators of the 1974 symposium, NobelLaureate Fran¸ cois Jacob, observed that “the role of language as a communi-
cation system between individuals would have come about only secondarily,”perhaps referring to discussions at the 1974 conference, where his fellow NobelLaureate Salvador Luria was one of the more forceful advocates of the view thatcommunicative needs would not have provided “any great selective pressure toproduce a system such as language,” with its crucial relation to “development ofabstract or productive thinking.” “The quality of language that makes it uniquedoes not seem to be so much its role in communicating directives for action” orother common features of animal communication, Jacob continues, but rather“its role in symbolizing, in evoking cognitive images,” in “molding” our notionof reality and yielding our capacity for thought and planning, through its uniqueproperty of allowing “inﬁnite combinations of symbols” and therefore “mentalcreation of possible worlds,” ideas that trace back to the seventeeth-centurycognitive revolution.
Jacob also stressed the common understanding that answers to questions
about evolution “in most instances . . . can hardly be more than more or lessreasonable guesses.” And in most cases, hardly even that. An example that isperhaps of interest here is the study of evolution of the bee communicationsystem, unusual in that in principle it permits transmission of information overan inﬁnite (continuous) range. There are hundreds of species of honey and stin-gless bees, some having variants of communication systems, some not, thoughthey all seem to survive well enough. So there is plenty of opportunity forcomparative work. Bees are incomparably easier to study than humans, alongevery dimension. But little is understood. Even the literature is sparse. The most
recent extensive review I have seen, by entomologist Fred Dyer, notes that eventhe basic computational problems of coding spatial information to motor com-mands, and the reverse for follower bees, remains “puzzling,” and “What sortsof neural events might underlie these various mapping processes is unknown,”while evolutionary origins scarcely go beyond speculation. There is nothinglike the huge literature and conﬁdent pronouncements about the evolution ofhuman language – something that one might also ﬁnd a bit “puzzling.”
Wecan add another insight of seventeeth-and eighteenth-century philosophy,
with roots as far back as Aristotle’s analysis of what were later interpreted asmental entities: that even the most elementary concepts of human language donot relate to mind-independent objects by means of some reference-like rela-tion between symbols and identiﬁable physical features of the external world,as seems to be universal in animal communication systems. Rather, they arecreations of the “cognoscitive powers” that provide us with rich means to refer

--- Page 197 ---

178 Language and Mind
to the outside world from certain perspectives, but are individuated by men-
tal operations that cannot be reduced to a “peculiar nature belonging” to thething we are talking about, as Hume summarized a century of inquiry. JuliusMoravcsik’s “aitiational theory of semantics” is a recent development of someof these ideas, from their Aristotelian origins and with rich implications fornatural language semantics.
These are critical observations about the elementary semantics of natural
language, suggesting that its most primitive elements are related to the mind-independent world much as the internal elements of phonology are, not by areference-like relation but as part of a considerably more intricate species ofconception and action. I cannot try to elaborate here, but I think such consid-erations, if seriously pursued, reveal that it is idle to try to base the semanticsof natural language on any kind of “word–object” relation, however intricatethe constructed notion of “object,” just as it would be idle to base the phoneticsof natural language on a “symbol–sound” relation, where sounds are taken tobe constructed physical events – perhaps indescribable four-dimensional con-structs based on motions of molecules, with further questions dispatched to thephysics department, or if one wants to make the problem still more hopeless,to the sociology department as well. It is universally agreed that these movesare the wrong ones for the study of the sound side of language, and I thinkthe conclusions are just as reasonable on the meaning side. For each utterance,there is a physical event, but that does not imply that we have to seek somemythical relation between such an internal object as the syllable [ta] and anidentiﬁable mind-independent event; and for each act of referring there is somecomplex aspect of the experienced or imagined world on which attention isfocused by that act, but that is not to say that a relation of reference exists fornatural language. I think it does not, even at the most primitive level.
If this much is generally on the right track, then, at least two basic problems
arise when we consider the origins of the faculty of language and its role in thesudden emergence of the human intellectual capacity: ﬁrst, the core semantics ofminimal meaning-bearing elements, including the simplest of them; and second,the principles that allow unbounded combinations of symbols, hierarchicallyorganized, which provide the means for use of language in its many aspects. Bythe same token, the core theory of language – universal grammar, UG – mustprovide, ﬁrst, a structured inventory of possible lexical items that are related toor perhaps identical with the concepts that are the elements of the “cognoscitivepowers” and second, means to construct from these lexical items the inﬁnitevariety of internal structures that enter into thought, interpretation, planning,
and other human mental acts, and are sometimes externalized, a secondaryprocess if the speculations just reviewed turn out to be correct. On the ﬁrstproblem, the apparently human-speciﬁc conceptual–lexical apparatus, thereis insightful work on relational notions linked to syntactic structures and on

--- Page 198 ---

Biolinguistics and the human capacity 179
the partially mind-internal objects that appear to play a critical role (events,
propositions, etc.). But there is little beyond descriptive remarks on the corereferential apparatus that is used to talk about the world. The second problemhas been central to linguistic research for a half-century, with a long historybefore in different terms.
The biolinguistic approach adopted from the outset the point of view that
cognitive neuroscientist R. G. Gallistel calls “the norm in neuroscience” today,the “modular view of learning”: the conclusion that in all animals, learning isbased on specialized mechanisms, “instincts to learn” in speciﬁc ways. Hesuggests that we think of these mechanisms as “organs within the brain,”achieving states in which they perform speciﬁc kinds of computation. Apartfrom “extremely hostile environments,” they change states under the triggeringand shaping effect of external factors, more or less reﬂexively, and in accor-dance with internal design. That is the “process of learning,” though “growth”might be a more appropriate term, avoiding misleading connotations of the term“learning.” One might relate these ideas to Gallistel’s encyclopedic work onorganization of motion, based on “structural constraints” that set “limits on thekinds of solutions an animal will come up with in a learning situation.”
The modular view of learning of course does not entail that the components
of the module are unique to it: at some level, everyone assumes that they arenot – the cell, for example. The question of the level of organization at whichunique properties emerge remains a basic question from a biological point ofview, as it was at the 1974 conference. Gallistel’s observations recall the conceptof “canalization” introduced into evolutionary and developmental biology byC. H. Waddington sixty years ago, referring to processes “adjusted so as tobring about one deﬁnite end result regardless of minor variations in conditionsduring the course of the reaction,” thus ensuring “the production of the normal,that is optimal type in the face of the unavoidable hazards of existence.” Thatseems to be a fair description of the growth of language in the individual. A coreproblem of the study of the faculty of language is to discover the mechanismsthat limit outcomes to “optimal types.”
It has been recognized since the origins of modern biology that organism-
external developmental constraints and architectural-structural principles enter
not only into the growth of organisms but also their evolution. In a classiccontemporary paper, Maynard Smith and associates trace the post-Darwinianversion back to Thomas Huxley, who was struck by the fact that there appear to
be “predetermined lines of modiﬁcation” that lead natural selection to “producevarieties of a limited number and kind” for every species. They review a vari-
ety of such constraints in the organic world and describe how “limitations onphenotypic variability” are “caused by the structure, character, composition, ordynamics of the developmental system.” They also point out that such “devel-opmental constraints undoubtedly play a signiﬁcant role in evolution” though

--- Page 199 ---

180 Language and Mind
there is yet “little agreement on their importance as compared with selection,
drift, and other such factors in shaping evolutionary history.” At about thesame time, Jacob wrote that “the rules controlling embryonic development,”almost entirely unknown, interact with other physical factors to “restrict possi-ble changes of structures and functions” in evolutionary development, providing“architectural constraints” that “limit adaptive scope and channel evolutionarypatterns,” to quote a recent review. The best-known of the ﬁgures who devotedmuch of their work to these topics are D’Arcy Thompson and Alan Turing, whotook a very strong view on the central role of such factors in biology. In recentyears, such considerations have been adduced for a wide range of problems ofdevelopment and evolution, from cell division in bacteria to optimization ofstructure and function of cortical networks, even to proposals that organismshave “the best of all possible brains,” as argued by computational neuroscientistChris Cherniak. The problems are the border of inquiry, but their signiﬁcanceis not controversial.
Assuming that the faculty of language has the general properties of other
biological systems, we should, therefore, be seeking three factors that enterinto the growth of language in the individual:(1)Genetic factors, apparently near uniform for the species, the topic of UG.The genetic endowment interprets part of the environment as linguisticexperience, a nontrivial task that the infant carries out reﬂexively, and
determines the general course of the development of the language facultyto the languages attained.
(2)Experience, which leads to variation, within a fairly narrow range, as in thecase of other subsystems of the human capacity and the organism generally.
(3)Principles not speciﬁc to the faculty of language.
The third factor includes principles of structural architecture that restrict out-
comes, including principles of efﬁcient computation, which would be expectedto be of particular signiﬁcance for computational systems such as language,determining the general character of attainable languages.
One can trace interest in this third factor back to the Galilean intuition that
“nature is perfect,” from the tides to the ﬂight of birds, and that it is the taskof the scientist to discover in just what sense this is true. Newton’s conﬁdencethat Nature must be “very simple” reﬂects the same intuition. However obscureit may be, that intuition about what Ernst Haeckel called nature’s “drive forthe beautiful” (“Sinn fuer das Schoene”) has been a guiding theme of modernscience ever since its modern origins.
Biologists have tended to think differently about the objects of their inquiry,
adopting Jacob’s image of nature as a tinkerer, which does the best it canwith materials at hand – often a pretty poor job, as human intelligence seemsto be intent on demonstrating about itself. British geneticist Gabriel Dovercaptures the prevailing view when he concludes that “biology is a strange and

--- Page 200 ---

Biolinguistics and the human capacity 181
messy business and ‘perfection’ is the last word one would use to describe how
organisms work, particularly for anything produced by natural selection” –though produced only in part by natural selection, as he emphasizes, and asevery biologist knows, and to an extent that cannot be quantiﬁed by available
tools. These expectations make good sense for systems with a long and complexevolutionary history, with plenty of accidents, lingering effects of evolutionary
history that lead to nonoptimal solutions of problems, and so on. But the logicdoes not apply to relatively sudden emergence, which might very well lead tosystems that are unlike the complex outcomes of millions of years of Jacobian“bricolage,” perhaps more like snowﬂakes, or phyllotaxis, or cell division intospheres rather than cubes, or polyhedra as construction materials, or much elsethat is found in the natural world. The minimalist program is motivated by thesuspicion that something like that may indeed be true for human language, andIthink recent work has given some reason to believe that language is in many
respects an optimal solution to conditions it must satisfy, far more so than couldhave been anticipated a few years ago.
Returning to the early days, within the structuralist/behaviorist frameworks
of the 1950s, the closest analogues to UG were the procedural approaches devel-oped by Trubetzkoy, Harris, and others, devised to determine linguistic unitsand their patterns from a corpus of linguistic data. At best, these cannot reachvery far, no matter how vast the corpus and futuristic the computational devices
used. Even the elementary formal and meaning-bearing elements, morphemes,do not have the “beads on a string” character that is required for proceduralapproaches, but relate much more indirectly to phonetic form. Their natureand properties are ﬁxed within the more abstract computational system thatdetermines the unbounded range of expressions. The earliest approaches togenerative grammar therefore assumed that the genetic endowment provides aformat for rule systems and a method for selecting the optimal instantiation ofit, given data of experience. Speciﬁc proposals were made then and in the yearsthat followed. In principle, they provided a possible solution to the problem oflanguage acquisition, but involved astronomical calculation, and therefore didnot seriously address the issues.
The main concerns in those years were quite different, as they still are. It may
be hard to believe today, but it was commonly assumed ﬁfty years ago that thebasic technology of linguistic description was available, and that language vari-ation was so free that nothing of much generality was likely to be discovered. Assoon as efforts were made to provide fairly explicit accounts of the properties oflanguages, it immediately became obvious how little was known, in any domain.Every speciﬁc proposal yielded a treasure trove of counter-evidence, requiringcomplex and varied rule systems even to achieve a very limited approximationto descriptive adequacy. That was highly stimulating for inquiry into language,butalso left a serious quandary, since the most elementary considerations led to

--- Page 201 ---

182 Language and Mind
the conclusion that UG must impose narrow constraints on possible outcomes in
order to account for the acquisition of language, the task of achieving “explana-tory adequacy,” so called. Sometimes these are called “poverty of stimulus”problems in the study of language, though the term is misleading because thisis just a special case of basic issues that arise universally for organic growth,including cognitive growth, a variant of problems recognized as far back asPlato.
Anumber of paths were pursued to try to resolve the tension. The most
successful turned out to be efforts to formulate general principles, attributedto UG – that is, the genetic endowment – leaving a somewhat reduced residueof phenomena that would result, somehow, from experience. These approacheshad some success, but the basic tensions remained unresolved at the time of the1974 conference.
Within a few years, the landscape changed considerably. In part this was the
result of a vast array of new materials from studies of much greater depth thanpreviously, in part from opening new topics to investigation. About twenty-ﬁveyears ago, much of this work crystallized in a radically different approach toUG, the “Principles and Parameters” (P&P) framework, which for the ﬁrst timeoffered the hope of overcoming the tension between descriptive and explanatoryadequacy. This approach sought to eliminate the format framework entirely, andwith it, the traditional conception of rules and constructions that had been prettymuch taken over into generative grammar. In these respects, it was a much moreradical departure from the rich tradition of 2,500 years than early generativegrammar. The new P&P framework led to an explosion of inquiry into lan-guages of the most varied typology, leading to new problems previously notenvisioned, sometimes answers, and the reinvigoration of neighboring disci-plines concerned with acquisition and processing, their guiding questions nowreframed in terms of parameter-setting within a ﬁxed system of principles ofUG. No one familiar with the ﬁeld has any illusion today that the horizons ofinquiry are even visible, let alone at hand.
Abandonment of the format framework also had a signiﬁcant impact on
the biolinguistic program. If, as had been assumed, acquisition is a matter ofselection among options made available by the format provided by UG, thenthe format must be rich and highly articulated, allowing relatively few options;otherwise, explanatory adequacy is out of reach. The best theory of languagemust be a very unsatisfactory one from other points of view, with a complex arrayof conditions speciﬁc to human language, restricting possible instantiations.The fundamental biological issue of principled explanation could barely becontemplated, and correspondingly, the prospects for some serious inquiry intoevolution of language were dim; evidently, the more varied and intricate the
conditions speciﬁc to language, the less hope there is for a reasonable accountof the evolutionary origins of UG. These are among the questions that were

--- Page 202 ---

Biolinguistics and the human capacity 183
raised at the 1974 symposium and others of the period, but they were left as
apparently irresoluble problems.
The P&P framework offered prospects for resolution of these tensions as
well. Insofar as this framework proves valid, acquisition is a matter of param-eter setting, and is therefore divorced entirely from the remaining format forgrammar: the principles of UG. There is no longer a conceptual barrier to thehope that the UG might be reduced to a much simpler form, and that basicproperties of the computational systems of language might have a principledexplanation instead of being stipulated in terms of a highly restrictive language-
speciﬁc format for grammars. Returning to the three factors of language design,adoption of a P&P framework overcomes a difﬁcult conceptual barrier to shift-ing the burden of explanation from factor ( 1), the genetic endowment, to factor
(3), language-independent principles of structural architecture and computa-
tional efﬁciency, thereby providing some answers to the fundamental questionsof biology of language, its nature and use, and perhaps its evolution.
With the conceptual barriers imposed by the format framework overcome, we
can try more realistically to sharpen the question of what constitutes a principledexplanation for properties of language, and turn to one of the most fundamental
questions of the biology of language: to what extent does language approximatean optimal solution to conditions that it must satisfy to be usable at all, givenextra-linguistic structural architecture? These conditions take us back to the
traditional characterization of language since Aristotle as a system that linkssound and meaning. In our terms, the expressions generated by a language mustsatisfy two interface conditions: those imposed by the sensorimotor system andby the conceptual–intentional system that enters into the human intellectualcapacity and the variety of speech acts.
Wecan regard an explanation of properties of language as principled insofar
as it can be reduced to properties of the interface systems and general consid-erations of computational efﬁciency and the like. Independently, the interfacesystems can be studied on their own, including comparative study that has beenproductively underway. And the same is true of principles of efﬁcient computa-tion, applied to language in recent work by many investigators with importantresults, and perhaps also amenable to comparative inquiry. In a variety of ways,then, it is possible both to clarify and to address some of the basic problems ofthe biology of language.
At this point we have to move on to more technical discussion than is possible
here, but a few informal remarks may help sketch the general landscape, at least.
An elementary fact about the language faculty is that it is a system of dis-
crete inﬁnity, rare in the organic world. Any such system is based on a primitiveoperation that takes objects already constructed, and constructs from them anew object: in the simplest case, the set containing them. Call that operationMerge. Either Merge or some equivalent is a minimal requirement. With Merge

--- Page 203 ---

184 Language and Mind
available, we instantly have an unbounded system of hierarchically structured
expressions. The simplest account of the “Great Leap Forward” in the evolution
of humans would be that the brain was rewired, perhaps by some slight muta-
tion, to provide the operation Merge, at once laying a core part of the basis forwhat is found at that dramatic moment of human evolution: at least in principle;to connect the dots is far from a trivial problem. There are speculations aboutthe evolution of language that postulate a far more complex process: ﬁrst somemutation that permits two-unit expressions, perhaps yielding selectional advan-tage by reducing memory load for lexical items; then further mutations to permitlarger ones; and ﬁnally the Great Leap that yields Merge. Perhaps the earliersteps really took place, though there is no empirical or serious conceptual argu-ment for the belief. A more parsimonious speculation is that they did not, andthat the Great Leap was effectively instantaneous, in a single individual, whowasinstantly endowed with intellectual capacities far superior to those of oth-
ers, transmitted to offspring and coming to predominate. At best a reasonableguess, as are all speculations about such matters, but about the simplest oneimaginable, and not inconsistent with anything known or plausibly surmised. Itis hard to see what account of human evolution would not assume at least thismuch, in one or another form.
Similar questions arise about growth of language in the individual. It is
commonly assumed that there is a two-word stage, a three-word stage, andso on, with an ultimate Great Leap Forward to unbounded generation. That isobserved in performance, but it is also observed that at the early stage the childunderstands much more complex expressions, and that random modiﬁcationof longer ones – even such simple changes as placement of function wordsin a manner inconsistent with UG or the adult language – leads to confusionand misinterpretation. It could be that unbounded Merge, and whatever elseis involved in UG, is present at once, but only manifested in limited waysfor extraneous reasons, memory and attention limitation and the like; mattersdiscussed at the 1974 symposium, and now possible to investigate much moresystematically and productively.
The most restrictive case of Merge applies to a single object, forming a sin-
gleton set. Restriction to this case yields the successor function, from which therest of the theory of natural numbers can be developed in familiar ways. Thatsuggests a possible answer to a problem that troubled Wallace in the late nine-teenth century: in his words, that the “gigantic development of the mathematicalcapacity is wholly unexplained by the theory of natural selection, and must bedue to some altogether distinct cause,” if only because it remained unused. Onepossibility is that the natural numbers result from a simple constraint on thelanguage faculty, hence not given by God, in accord with Kronecker’s famousaphorism, though the rest is created by man, as he continued. Speculationsabout the origin of the mathematical capacity as an abstraction from linguistic

--- Page 204 ---

Biolinguistics and the human capacity 185
operations are not unfamiliar. There are apparent problems, including disso-
ciation with lesions and diversity of localization, but the signiﬁcance of suchphenomena is unclear for many reasons (including the issue of possession vs.use of the capacity). There may be something to these speculations, perhapsalong the lines just indicated.
Elementary considerations of computational efﬁciency impose other condi-
tions on the optimal solution to the task of linking sound and meaning. Thereis by now extensive literature exploring problems of this kind, and I think it isfair to say that there has been considerable progress in moving towards princi-
pled explanation. It is even more clear that these efforts have met one primaryrequirement for a sensible research program: stimulating inquiry that has beenable to overcome some old problems while even more rapidly bringing to lightnew ones, previously unrecognized and scarcely even formulable, and enrichinggreatly the empirical challenges of descriptive and explanatory adequacy thathave to be faced; and for the ﬁrst time, opening a realistic prospect of movingsigniﬁcantly beyond explanatory adequacy to principled explanation along thelines indicated.
The quest for principled explanation faces daunting tasks. We can formulate
the goals with reasonable clarity. We cannot, of course, know in advance howwell they can be attained – that is, to what extent the states of the languagefaculty are attributable to general principles, possibly even holding for organ-
isms generally. With each step toward this goal, we gain a clearer grasp of thecore properties that are speciﬁc to the language faculty, still leaving quite unre-solved problems that have been raised for hundreds of years. Among these arethe question how properties “termed mental” relate to “the organical structureof the brain,” problems far from resolution even for insects, and with uniqueand deeply mysterious aspects when we consider the human capacity and itsevolutionary origins.

--- Page 205 ---

Index
A-over-A principle 45–48,49,50
abduction xvi–xvii,80,81,84,152
active and passive transformations 136
Akmajian, Adrian 96
Alsop, Joseph 84
ambiguity 28–32,134
American Academy of Arts and Sciences 174
animal communication 9–10,60,61,72,177
and human language 10,59–60,88–91
Ardrey, Robert 84
Aristotle 177,183
Austin, J. L. 143
automata 3,4,5,161
Barlow, H.B. 83
behavioral sciences x,57
behavioralist approaches to the study of
language 22,181
and linguistics vii,146,162
psychology and 21,63
belief 150
Bever, T. G. 82–83
biolinguistics 173–85
Bloomﬁeld, Leonard 2,13
Bower, Thomas 83
canalization 179
Cartesian philosophers 5–6,22
animal behavior 11
contribution of 6
mind, problems of 7,11–12,176
categorial features of universal grammar 114,
124,125–27,128,131,137,138
phonetic 114
syntactic 124,125–27,128,130,131,137,
138
Cherniak, Chris 180
cognitive sciences xiv–xvi,xvii,149
Cohen, I. Bernard 174
communication
animal 9–10,59–60,61,72,177
mathematical theory of 3,4conﬁrmation, problem of 79
conservation (of volume) 82–83
Cordemoy, G´ eraud de 5–6
creative aspect of language use 6,8,9,87,
88–91,175
Descartes on 10,89
mechanisms enabling 90
Cudworth, Ralph 159
culture 69
cyclic application, principle of 39–42,49,66,
116–19,141,154,160
Darwin, Charles xv,173,176
deep structure x,15,25,93,94,97,140
base phrase-markers and 131
character of 32
functions of 26
and labeled bracketing 122
and meaning 94,97,146
and pronominalization 97
relation to surface structure 15,27,62,135,
137,138,144
rules generating 124
and semantic interpretation 111,120,123,
131,145
and syntax 123
theory of 15–16,17
and transformations 134,135,137,138,
144,145
and universal syntax 137
variation from language to language
66–67,140
Delbr¨ uk, Berthold 18
deletion transformations 31,49,50–52
derivations 127,128,129
Descartes, Rene 7
innate ideas 71,73–74,153,154,159,167
language xv,9–12,89
mind 5,6
Diamond, Jared 176
dispositions 31,33
Dougherty, Ray 96
186

--- Page 206 ---

Index 187
Dover, Gabriel 180
Du Marsais, C´ esar Chesneau 16
Dyer, Fred 177
Eden, Murray 86
ellipsis 16,17
emergence 62,178,181
Emonds, Joseph 53
empiricism, resourceful 171,172
English 118,140
stress assignment 116–19,141
vowel shifts 36,42
wh- questions 43–45
erasure principle 50–52,53
ethology, comparative 83,85
evolution 59,177
‘Great Leap Forward’ account 184
of language 59,176,184
explanation 14,22,23,27,182
principled 182,183,185
rational 13–14
explanatory theory 23,24–25,146,166,175
extraposition 45
Fant, G. 108
Ferster, C. B. 58
formatives 113,114,115
Fraser, A. C. 71,153
Galanter, E. 105
Gallistel, R. G. 179
generative grammar x,62,77,78,82,91,112,
113
acquisition of 151
construction of 150
elements of 97–98
form of 142
and language acquisition 165
and linguistic competence 64,68,86,165,
166
principles 175
semantic component 123
theory of 63,91,140,145
transformational 86,93
Ginsburg, S. 75
Goodman, Nelson 70,71–74,77,149,
153–58,159
grammar 23,91,111,138
see also universal grammar
components of 62,107–10,111,113–38,
139
deﬁnition 150
and innate ideas 170
and linguistic competence 103
perception of 38,39philosophical 13,14,16,18,19,57,66–67
phrase structure 160
properties of 27,104
requirements of 15
rules of 28,92,126,127,137
simplicity and 168
structure of 113–38
transformational-generative 75,91,93
universal: seeuniversal grammar
grammatical relations 122
grammatical transformations 25–26,93,97,
133,138,144
A-over-A principle 45–48,49,50
active–passive 136
conditions on 49,52,53,62
cyclic application of 41,49
deletion 31,49,50–52
erasure principle 50–52,53
extraposition 45
Merge 183,184
nature of 52,161
and phrase-markers 133,139
rules of 132
sequence of 134
simplicity and 162
structure-dependent 51,54,55
and syntactic rules 124,132,137
gravity 7,174
habits 31,33,105
Haeckel, Ernst 180
Halle, M. 34,66,108,114,116,142,155
Harman, Gilbert 149,167,172
Harris, Zellig 29,172,181
Herbert, Lord 71,153,167
Hiz, Henry 29,149,164–68
Hockett, C. F. 134
Huarte, Juan xiv,xvi,8–9,17
Hubel, David 83
Hull, Clark 171
human action, theory of 64
human capacity 99–100,173–85
Humboldt, Wilhelm von xv,15,18,62,67,
113
Hume, David xv,154,173,178
Huxley, Thomas 179
induction 80
information, concept of 3
innate ideas 70,77,78,80,83,85,158
comprehensibility of the notion 158
Goodman 73,154
Locke 71,153
Putnam 77,160
instinct 85

--- Page 207 ---

188 Index
intelligence xi,8–9,69,99,152
and language 9,61,176,178
intonation 95–96
see also stress assignment in English
Jackendoff, Ray 96
Jacob, Fran¸ cois 177,180
Jacobs, R. 27,41
Jakobson, R. 65,108,109
Joos, Martin xiii,67,68
junctures 113,114,115
Katz, J. 27,52,109,110,142
Kimball, J. P. 161
knowledge 69,166
acquisition of 152,153,154,159,
171
and experience 68,70
knowing how and knowing that 169
problems of 147,149,153,159
K¨ohler, Wolfgang 21,23–26
Kronecker, Leopold 184
La Forge, L. de 5,12
labeled bracketing 127,129,136,144
Lange, Friedrich xv
language x,24,61,87,88–101,167
and animal communication 10,59–60,72,
88–91
characteristics of 60,183
common origin of 76,162–63,168
discreteness of 108,183
evolution of 59,176,184
faculty of 24,175,178,180,183
general properties of 102
knowledge of 22,23,25,26,33,62,98,
102
see also linguistic competence
and mental organization 9,61,100
nature of 5,102–42,148
organization of 90,91
properties of 74,92,107,108,118,181
rationalist theory of 9,12,25,67,68,70,
77,99,171–72
structuralist approaches to the study of 22
study of 1,19,22,58,98,100
theory of 182
unboundedness of 105,108
written system for 138
language acquisition xi,58,141,151–52,
154,163,181
explanations for 78,79,99–100,142
factors involved 180,183
ﬁrst language 71,154,155–56
innate basis for 87,119,162,170model of 106,120,159,165,171
Peirce on 79–82
poverty of stimulus xvii,11
second languages 72,154–55,156
stages of 184
theory of 166
and universal grammar 33,77,120
language structure
and generative grammar 112,142
study of x,110,111
theory of 12,103,139,148,151
Lashley, Karl 60
Le Grand, Antoine 90
learning xi,64,164
and generative grammar 63,69,151
model of 150,151
modular view of 179
strategies 76,163,164
theory of 79,81,152,166
Lees, R. B. 30,142
Leibnitz, Gottfried Wilhelm 7,71
Lenneberg, Eric ix,82,163,173
Lettvin, J. 83
L´evi-Strauss, C. 65,66
lexicon 34–35,124–25,128,130,131
linguistic competence 4,20,62,102–04,165,
169
concept of 63,69
and grammar 16,27,54,55,56,64,136,
166
and knowing how 169
model for 104
and performance 102–04,139,164–68
study of 66,76,98,104–06,166
linguistics vii,xi
achievements of 146
anthropological 68,140
appropriateness 11
coherence 11
comparative studies 18
deep structure 140
explanation 14,24
mathematical 62–63
methodology 3,17,20,146,165
object of xiv
and philosophy 1,9,143–72
Principles and Parameters approach xviii,
182,183
and psychology 1,67,78
rationalist theory of 167,170
structural 3,4,17,19–20,22,57,65,106
and universal grammar xvii
Locke, John xv,71,73,153,154,174
Lorenz, Konrad 84–85
Luria, Salvador 177

--- Page 208 ---

Index 189
mathematics 176,184,185
Matthews, G. H. 161,167
meaning 88–01,133, 143
see also semantic interpretation
and deep structure 94,97
and surface structure 96,97
Mehler, J. 82–83
mental processes xv,21,153,173,177,185
Merge operation 183,184
Miller, G. A. 55,81,105,162
mind 5,85,90,164,174
evolution of 86
linguistic contributions to the study of
1–20,21–56,57–87
other minds 9
philosophy of 172
problems of 25,57,167
rationalist theory of 99,171–72
Minsky, M. 82
Moracvsik, Julius 178
Mountcastle, Vernon 174
Nelson, R. J. 75
Newton, Isaac 7,174,180
nominalization 93,94,155
numbers, natural 184
other minds, problem of 9
Papert, S. 82
Peirce, Charles Sanders xvi–xvii,79–82,84,
85,152
perception, role of belief in 150
perceptual model 103–04,106,118,119,
149–50
performance 102,139,151,166,171
Perlmutter, David 51
Peters, S. 161
philosophical grammar 13,14,16,18,19,57,
66–67
philosophy
analytic 145
Cartesian 5–6,7,22
and linguistics 1,143
of language 57–58
of mind 22
phonetic representation 37,38,97,103
categorial features 114
formatives 113,114,115
junctures 113,114,115
and surface structure 94,113,116
phonetics, universal 107–09
phonological rules 35–38,65,66,114,115
cyclic application of 40
ordering of 116phrase-markers 130,137,139
base phrase-markers 127,131,137
phrase structure 14,74,75,115,125,160
rules 75,127,161,162
physics 7
Piaget, J. 82
poetry 90
Polignac, Melchior de 10
Popper, Karl 59
Port-Royal Grammar and Logic 13,14–15,
16,17
Postal, P. 27,34,142
predicate-of relation 122
presupposition 95,96
Pribram, K. H. 105
Priestley, Joseph 173,174
Principles and Parameters approach to
linguistics xviii,182,183
pronominalization 96,97
proper names 160
psychic distance 23,55,69
psychology 6,14,21,63,68,79,104
and intelligence 98
and linguistics 1,64,67,78,82,100
rationalist 25,167
stimulus–response 2,3,4
Putnam, Hilary 70,74–77,149,159–64
Quine, W. V . O. xiii,63,171
Racine, Louis 10
reductive principles 174
redundancy rules 125
refernential opacity 145,146
Reid, Thomas 154,167
rigidty principle 175
Ritchie, R. 161
romanticism 9,67,99
Rosenbaum, Peter S. 27,41,50–51,53,142
Rosenﬁeld, Leonora Cohen 10
Ross, John 29,30,43,49
Rousseau, J.-J. 67
rule-following 175
Russell, Bertrand 2,174
Ryle, Gilbert 12
Salzinger, K. 81
Sanctius 16,17,18
Saussure, Ferdinand de 17–18,20
Schlegel, A. W. 90
Sch¨utzenberger, M. P. 86
science, seventeenth century developments in
5,7,8,14,71,177
semantic interpretation 97,111,123,137,
145,1781

--- Page 209 ---

190 Index
semantic interpretation ( cont.)
and deep structure 136
and phonetic representation 103
rules of 52,53
and surface structure 94
universal 53
semantic representation 110,120,121
Shklovsky, Viktor 21
simplicity 160,161,162,168,180
skills 31
Skinner, B.F. 2,81
Smith, J. Maynard 179
sound–meaning relation 91,106
sound structure 33–34,74
stress assignment in English 95–96,116–19,
141
structural linguistics 3,4,17,19–20,22,57,
65,106
structure-dependent rules 54,55
surface structure 15,25,92–93,97,114
character of 32,33
generation of 34,124,143,144,145,
161
and grammatical transformations 67,162
and labeled bracketing 115
and meaning 96,136,146
and phonetic representation 94,116
relation to deep structure 15,27,62,131,
135,137,145
syntax and 123
theory of 15–16,17
variation in 140
symbolism 176,177
syntactic rules 34,132,138
syntax, universal 113,123–38
Tattersal, Ian 176,177
teaching 89
terminal symbols 126,127
Thompson, D’Arcy 180
Thorpe, W. H. 59–60
Tolman, Edward 79
transparency 47,48
Troubetzky, N. 65,181
Turing, Alan 180universal grammar x,13,24,106–13,178,
181
A-over-A principle 46
characteristics of 158
components of 138
conditions on 68
deﬁnition 112
discovery of xvii
evidence for 99,167
evolution of 182
and experience 182
hypotheses of 140
and innate ideas 74,77,141,170
innate representation of 76,164
and language acquisition 100,182,184
and learning 77
and particular grammar 24,38,42,56
principles of 55,62,120,140,168
problems of 28
properties of 157
seventeenth century developments in 99
simplicity 183
theory of 142,166
universal phonetics 107–09,112,113–20,
123,160
distinctive features 108,109
universal semantics 53,87,107,109–10,112,
120–23,145
universal syntax 113,123–38
Vaugelas, rule of 13
Vendler, Zeno 51,143,146
vonNeumann, John 3
vowel shifts 36,42
Waddington, C. H. 179
Wallace, Alfred Russell 175,176,184
Weinreich, U. 52,109
Whitehead, A. N. 20
Whitney, William Dwight 18,66,67
Wiest, W. M. 63
Wilkins, Bishop 107
Wittgenstein, Ludwig 22,63
Yngve, V . 139